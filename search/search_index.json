{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tip If you are looking for the full documentation of the OMNeT++ Simulator, please visit: https://omnetpp.org/documentation OMNeT++ Tutorials and Technical Articles \u00b6 This site hosts additional documentation for OMNeT++, including the TicToc tutorial and various technical articles. Tutorials \u00b6 TicToc . An introductory tutorial that guides you through building and working with an example simulation model. Result Analysis with Python and Pandas . This tutorial walks you through the initial steps of analyzing simulation results using Python, Pandas and Matplotlib, and helps you with some of the most common tasks. Selected Technical Articles \u00b6 Running Simulation Campaigns in the Cloud . Concepts, ideas and a solution draft for harnessing the power of computing clouds for running simulation campaigns. Using AWS for Running INET Simulation Campaigns . Presents a minimal but powerful toolset for running INET simulations on Amazon's cloud platform. Use the sidebar to access the full list of available articles.","title":"Overview"},{"location":"#omnet-tutorials-and-technical-articles","text":"This site hosts additional documentation for OMNeT++, including the TicToc tutorial and various technical articles.","title":"OMNeT++ Tutorials and Technical Articles"},{"location":"#tutorials","text":"TicToc . An introductory tutorial that guides you through building and working with an example simulation model. Result Analysis with Python and Pandas . This tutorial walks you through the initial steps of analyzing simulation results using Python, Pandas and Matplotlib, and helps you with some of the most common tasks.","title":"Tutorials"},{"location":"#selected-technical-articles","text":"Running Simulation Campaigns in the Cloud . Concepts, ideas and a solution draft for harnessing the power of computing clouds for running simulation campaigns. Using AWS for Running INET Simulation Campaigns . Presents a minimal but powerful toolset for running INET simulations on Amazon's cloud platform. Use the sidebar to access the full list of available articles.","title":"Selected Technical Articles"},{"location":"404/","text":"The page you requested doesn't exist. Better luck next time! Go back to the front page.","title":"404"},{"location":"showfile/","text":"","title":"_Source Browser_"},{"location":"articles/building-omnetpp-in-docker/","text":"Recently we started to use Docker to build OMNeT++ releases. This approach gave us reproducible results and independence from the local development environment. While previously we needed a carefully configured Jenkins build server, from now on anyone can re-create the release archives by simply checking out the OMNeT++ sources, going into the releng directory and executing the build-omnetpp-in-docker script (as long as git and docker are already installed on the system). The docker images required by the build process are available on DockerHub. Go and give it a try if you need to build an OMNeT++ distro for whatever reason.","title":"Building an OMNeT++ Distro"},{"location":"articles/neuralnet-errormodels/","text":"Note This is a Proposed Research Topic . Proposed research topics are ideas that we find both very promising as a research topic, and practically very useful . We have already spent some time trying out the idea and proven (at least to ourselves) that it is feasible and the approach outlined here can be made to work, but we don't have the resources (mostly, time) to elaborate it in-house. If you are a researcher (e.g. PhD student) looking for an exciting and rewarding topic to work on, we'd love to hear from you! Abstract \u00b6 In wireless network simulations, an error model describes how the signal to noise ratio (SNR) affects the amount of errors at the receiver. Given that SNR is a multi-dimensional function over time and frequency, and given the diversity of coding and modulation schemes used in wireless networks, writing a good and efficient error model is a very difficult problem. Existing error models are often closed formulas derived from empirical observations, and in addition to being very limited in scope, they sometimes fall down even within their supposed limitations. In contrast, we propose using neural networks and deep learning techniques to produce an error model that, in addition to being universally applicable, can produce reasonable answers even for cases that it was not explicitly trained for. Background \u00b6 With the recent INET 4.2 release comes a new and versatile radio signal analog domain representation. The new representation is capable of handling all kinds of radio signals such as OFDM, FHSS, UWB, chirp, and so on. It also allows mixing wireless technologies arbitrarily. The API is flexible in terms of representation composition, and it also allows arbitrary extensions to be combined with existing representations. The next step would be to come up with better wireless error models, because the current ones don't take advantage of the capabilities of the new representation. Error models are responsible for determining whether a packet has been received correctly, from the complete description of the physical signal and interference characteristics. All current wireless error models solely rely on the minimum (or average) SNIR in order to determine the packet error rate. In both cases, the SNIR is calculated over the whole received signal both in time and frequency. Using the minimum SNIR as input often yields completely incorrect results (e.g. a short strong interfering spike ruins the minimum SNIR regardless of its duration). Moreover, the current WiFi error models are not even up-to-date for the latest versions of the IEEE 802.11 standard, so the highest datarates are not supported. The new signal representation allows the error models to look at the SNIR for each physical symbol in every subcarrier and modulation. Doing so would not only provide more accurate error models for a particular wireless model, but it would also allow more accurate crosstalk and coexistence simulations. Proposal \u00b6 INET could have wireless error models that use neural networks and deep learning to determine the packet error rate based on hundreds or even thousands of signal parameters for every reception quite efficiently. For example, the error model could calculate the packet error rate from the minimum/average SNIR parameters for each and every symbol for every subcarrier of the received signal. The deep learning method would be used as a curve fitting technique for the packet error rate function to capture the essence of the utilized modulation, scrambling, interleaving, forward error correction, etc. of the particular wireless technology. The data to teach the neural network could be generated by simulating the reception of many signals doing symbol/bit level simulation. That is, actually determining how every symbol is affected and decoding the signal. Of course, this is a very time consuming process, but it can be done in parallel (e.g. using cloud services) and it only has to be done once for a particular wireless technology. This data could also be generated by Matlab or other 3 rd party tools, but luckily INET already contains many algorithms for symbol/bit level simulation. The generated data would contain the thousands of signal parameters and the reception outcome. This data could then be fed into another cloud service which teaches a neural network using deep learning and produces the parameters for the neural network to be used in the actual wireless simulations. We expect runtime performance to be good, because running a neural network is several magnitudes faster than teaching the network. Progress \u00b6 Early experiments show that this is a very promising direction. After some training, our neural network performed better than some well-known error models already in INET. It is also clear from the early experiments that a researcher with more experience in neural networks than ourselves could achieve much more than we could, and the end result would be practical to be used in INET simulations. Our code can be found in the topic/neuralnetworkerrormodel branch of the INET repository.","title":"Better Wireless Error Models Using Neural Networks"},{"location":"articles/neuralnet-errormodels/#abstract","text":"In wireless network simulations, an error model describes how the signal to noise ratio (SNR) affects the amount of errors at the receiver. Given that SNR is a multi-dimensional function over time and frequency, and given the diversity of coding and modulation schemes used in wireless networks, writing a good and efficient error model is a very difficult problem. Existing error models are often closed formulas derived from empirical observations, and in addition to being very limited in scope, they sometimes fall down even within their supposed limitations. In contrast, we propose using neural networks and deep learning techniques to produce an error model that, in addition to being universally applicable, can produce reasonable answers even for cases that it was not explicitly trained for.","title":"Abstract"},{"location":"articles/neuralnet-errormodels/#background","text":"With the recent INET 4.2 release comes a new and versatile radio signal analog domain representation. The new representation is capable of handling all kinds of radio signals such as OFDM, FHSS, UWB, chirp, and so on. It also allows mixing wireless technologies arbitrarily. The API is flexible in terms of representation composition, and it also allows arbitrary extensions to be combined with existing representations. The next step would be to come up with better wireless error models, because the current ones don't take advantage of the capabilities of the new representation. Error models are responsible for determining whether a packet has been received correctly, from the complete description of the physical signal and interference characteristics. All current wireless error models solely rely on the minimum (or average) SNIR in order to determine the packet error rate. In both cases, the SNIR is calculated over the whole received signal both in time and frequency. Using the minimum SNIR as input often yields completely incorrect results (e.g. a short strong interfering spike ruins the minimum SNIR regardless of its duration). Moreover, the current WiFi error models are not even up-to-date for the latest versions of the IEEE 802.11 standard, so the highest datarates are not supported. The new signal representation allows the error models to look at the SNIR for each physical symbol in every subcarrier and modulation. Doing so would not only provide more accurate error models for a particular wireless model, but it would also allow more accurate crosstalk and coexistence simulations.","title":"Background"},{"location":"articles/neuralnet-errormodels/#proposal","text":"INET could have wireless error models that use neural networks and deep learning to determine the packet error rate based on hundreds or even thousands of signal parameters for every reception quite efficiently. For example, the error model could calculate the packet error rate from the minimum/average SNIR parameters for each and every symbol for every subcarrier of the received signal. The deep learning method would be used as a curve fitting technique for the packet error rate function to capture the essence of the utilized modulation, scrambling, interleaving, forward error correction, etc. of the particular wireless technology. The data to teach the neural network could be generated by simulating the reception of many signals doing symbol/bit level simulation. That is, actually determining how every symbol is affected and decoding the signal. Of course, this is a very time consuming process, but it can be done in parallel (e.g. using cloud services) and it only has to be done once for a particular wireless technology. This data could also be generated by Matlab or other 3 rd party tools, but luckily INET already contains many algorithms for symbol/bit level simulation. The generated data would contain the thousands of signal parameters and the reception outcome. This data could then be fed into another cloud service which teaches a neural network using deep learning and produces the parameters for the neural network to be used in the actual wireless simulations. We expect runtime performance to be good, because running a neural network is several magnitudes faster than teaching the network.","title":"Proposal"},{"location":"articles/neuralnet-errormodels/#progress","text":"Early experiments show that this is a very promising direction. After some training, our neural network performed better than some well-known error models already in INET. It is also clear from the early experiments that a researcher with more experience in neural networks than ourselves could achieve much more than we could, and the end result would be practical to be used in INET simulations. Our code can be found in the topic/neuralnetworkerrormodel branch of the INET repository.","title":"Progress"},{"location":"articles/omnetpp-debugging-tips/","text":"You would think that in 2020, C++ debugging on Linux is a solved problem. C++ is a mature language used by a huge programmer audience and a multitude of large projects. It has excellent open-source tool support, and which OS would run those tools run better than Linux, right? Well, yes and no. Members of our team have gone through considerable pain trying to debug during the development of the INET Framework. While eventually we have managed to overcome the difficulties (mostly), it was far from being trivial, and took us a lot of research to find the combination of tools, compile options and other tricks that are able to provide an improved C++ debugging experience. In this blog post we share our experiences, in an effort to help our fellow developers. (It is important to note that content in this post might have a limited \"best before\" date, as the software landscape is changing over time in C++ land, too.) The Issues \u00b6 Here are a few issues we have experienced at one time or another: std::string (and other standard types) displayed as <incomplete type> in the debugger seeing the internals of the class (instead of the data you are interested in) when inspecting an std::string , std::vector or other standard containers seeing the <optimised out> message when inspecting a variable in your debug build (this happens even with -O0 , which is supposed to disable all optimizations) expression evaluation is incomplete (not all C++ syntax is supported) and unreliable (often reports errors, etc.) need for step filtering: as users, we are usually not interested in debugging into the internals of the standard C++ classes linking INET with full debug info taking too long debugger taking a very long time to start up, due to the amount of debug info it needs to load debugger not stopping at breakpoints response times being too slow when debugging with gdb not all debugger frontends being able to use lldb as backend choice of standalone debugger (there are many, all of them with their own issues) But before we delve into the details, let's come clear with the basics. Debugging on Various Platforms \u00b6 A debugger usually consists of two parts: a frontend (UI), and a debugger backend. The backend is responsible for the actual low-level debugging tasks like starting a process, setting breakpoints, stepping in the code, querying variables etc. A frontend, on the other hand, is responsible for displaying the debugging information to the user and handling user input. The OMNeT++ IDE, which is based on Eclipse, contains a debugger frontend provided by the CDT project. The CDT debugger uses gdb as its backend. gdb must be installed separately from the IDE. On Linux, gdb is normally installed by the system's package manager. On Windows, gdb is included in the bundled MinGW tool-chain. Finally on macOS, you must get gdb from a 3 rd party package manager (like homebrew), because the gdb instance that comes with macOS is no longer maintained and is quite outdated. (This is because macOS uses lldb as the default debugger, and they no longer need/care about gdb .) Installing gdb on macOS can be a headache, because the 3 rd party gdb executable must be digitally signed locally to be able to debug other processes. Because of the above limitations, sometimes it makes sense to debug OMNeT++ models outside of the IDE. Note On macOS, OMNeT++ 6 uses lldb (through the bundled lldb-mi2 driver) so the above issues are no longer present and debugging is working out of box. Debugger Backends - Pros and Cons \u00b6 We recommend three possible debugger backends for OMNeT++. Each of them have their own strengths and weaknesses: gdb PRO: very mature debugger, with a few unique features like, reverse debugging, continuous debugging or pretty printing. A lot of debugger frontends support gdb . CON: it can get quite slow with complicated C++ code, variable evaluation and stepping is not always reliable and sometimes rather slow. On macOS, setting up gdb is quite complicated and it still has limited usability. lldb PRO: a modern debugger with great pretty printing support, fast stepping and evaluation. Works out of the box on macOS. CON: There is only a handful of frontends that support LLDB. Notably, Eclipse CDT does not support lldb at the moment. rr PRO: This is an interesting special-purpose debugger. It supports execution recording and replaying (hence the name) with forward and reverse execution of the application. It is a gdb drop-in replacement, with a few additional commands for reverse execution. CON: Few frontends support it properly, and rr itself works only on Intel-made CPUs made after cca. 2010. Debugger Frontends \u00b6 Here are a few debugger frontends we would like to highlight: Eclipse CDT This is the built-in debugger in the OMNeT++ IDE. It integrates only with gdb (at the moment), so it inherits all the advantages and disadvantages of the gdb backend. Visual Studio Code VS Code is an extensible cross platform code editor/IDE that has several debugger extensions on its Marketplace. Some of them supports gdb , lldb , or both. VS Code is practically the only viable cross-platform debugger frontend. The primary mode of launching debugging sessions in VS Code is by creating debug configurations in its own editor. Launching it from the command-line as a standalone debugger is also possible, but it requires crafting some nifty command-line arguments (which could be hidden behind a shell script, for user sanity). Xcode This is the official development environment on macOS (and only available on that platform), and uses lldb as its backend. Setting up a debugging session for an OMNeT++ model can be quite complicated, as you have to set up a workspace and a project in Xcode before debugging. gdbgui This is a Python-based frontend (for gdb and rr ) so it can naturally run on all platforms. While it is somewhat limited, its unique point is that the user interface is rendered in a browser, so you can easily debug a remotely running process. It has also first class integration with the rr reverse debugger, so you can easily navigate an execution recording. This is extremely useful to catch rarely occurring crashes and bugs. nemiver This is a GTK-based front-end for gdb . It provides a reasonably complete and comfortable debugging experience on Linux systems, albeit the user interface has some annoying quirks. As the project has not received much attention from its developers for years, it is unclear when/whether those issues are going to be resolved. KDbg KDbg is roughly the KDE equivalent of Nemiver: a Qt-based frontend to gdb . Configuring OMNeT++ for Optimal Debugging Experience \u00b6 To achieve the best possible experience during debugging, you should fine tune some OMNeT++ compiler and linker flags before building OMNeT++ and your code in debug mode. Compiler flags can be configured in the configure.user file by adding the necessary options to the CFLAGS_DEBUG variable and then re-running ./configure and re-building OMNeT++ and your model. You can temporarily add these options also to Makefile.inc . In this case it is enough to just clean and re-build OMNeT++ and your model without re-configuration: make cleanall && make MODE=debug Depending on whether you intend to use gdb or lldb for debugging, you should add -ggdb3 or -glldb to the CFLAGS_DEBUG variable. Note that by default OMNeT++ assumes you intend to use gdb. You may need to configure gdb to improve load times if you work with big projects. In ~/.gdbinit specify main set worker-threads unlimited to allow parallel loading of debug information. If you use clang as your compiler and use 3 rd -party libraries in you project that do not have debugging symbols, you may have a hard time inspecting some types defined in those libraries. This is especially painful with the standard C++ library, where you cannot inspect std::string or other standard containers. This usually surfaces as a debugger error complaining about 'incomplete types'. To remedy this situation, you should install the corresponding debug symbols on your operating system (i.e. on Ubuntu: sudo apt-get install libstdc++6-8-dbg ), however note that the actual name of the debug symbol package varies widely. A more generic solution is to force the clang compiler to emit debug information for all types it encounters during compilation, but this greatly increases the size of the debug info (and the linking time as a consequence), because debug info for the standard C++ library will be included for each compilation unit. To force the compiler to generate full debug info, add -fstandalone-debug to the CFLAGS_DEBUG variable. However, this is recommended only if you cannot install the debugging symbols for the given library. !!! note This is not a problem with gcc and clang on macOS, where the default behavior is to generate full debug info anyway. If you had to apply the above workaround to get the standard C++ types working in the debugger, you may experience increased linking times, especially with big projects. You can specify -gsplit-dwarf in the CFLAGS_DEBUG variable to force the debug info into a separate (*.dwo) file for each compilation unit. This will speed up the linking process, but it will impact the startup time of the debugger. In turn, debugger startup time can be improved by supplying the -Wl,--gdb-index option as well, which enables pre-indexing of the .dwo files, thereby allowing faster loading. It is also useful to force some additional runtime-checking to avoid hard to detect bugs like stack overflows. Add -fstack-protector (or -fsanitize=safe-stack if you use clang ) to detect these issues. Debugging with VS Code \u00b6 VS Code has several debugger extensions, some of them supporting either gdb or lldb . We found CodeLLDB the most usable for debugging OMNeT++ models. As its name suggest, this extension uses lldb as a backend. To install, just open VS Code's extensions view, type CodeLLDB in the search field and click the 'install' button. As this debugger uses lldb , we recommend adding the -glldb option to the CFLAGS_DEBUG variable in configure.user . Launching Simulations with the CodeLLDB debugger \u00b6 To launch as a standalone debugger, create an executable shell script named codelldb in your path, containing: 1 2 3 4 #!/bin/sh PROG = $( realpath $1 ) shift code --open-url \"vscode://vadimcn.vscode-lldb/launch/command? $PROG $* \" After that, you can start debugging with: codelldb ./aloha_dbg -u Cmdenv -c PureAloha2 Attaching CodeLLDB to the Simulation \u00b6 It is also possible to configure OMNeT++ to allow invoking the debugger from inside the simulation by choosing the Simulate|Debug Now or Simulate|Debug Next Event menu items. In this case the simulation will execute a pre-configured command to launch the debugger and attach the current process to it. You have two options to do this: You can configure it globally by setting the following environment variable in your shell's startup file (i.e. .bashrc ): export OMNETPP_DEBUGGER_COMMAND=\"code --open-url \\\"vscode://vadimcn.vscode-lldb/launch/config?{request: 'attach', pid: '%u'}\\\"\" You can set it only for the current simulation by setting the following configuration key in your omnetpp.ini : debugger-attach-command=\"code --open-url \\\"vscode://vadimcn.vscode-lldb/launch/config?{request: 'attach', pid: '%u'}\\\"\" Both approaches will allow you to drop into the debugger interactively from Qtenv. Pretty Printing OMNeT++ Data Structures \u00b6 Both gdb and lldb allows you to define pretty printers in Python that allow you to better display some complicated data structures. OMNeT++ defines such printers for certain data structures (i.e. simtime_t etc.). To activate the pretty printers import them in the VS Code debug console . command script import < OMNETPP_ROOT >/ python / omnetpp / lldb / formatters / omnetpp . py Note Formatters for gdb and lldb are not compatible with each other! Debugging with gdbgui \u00b6 Gdbgui is a browser-based graphical frontend for gdb and rr , written in Python. To install just type: $ pip3 install gdbgui Start the debugger using: $ gdbgui --args ./aloha_dbg -u Cmdenv -c PureAloha1 which will launch also a browser window containing the debugger UI. Reverse Debugging Tips \u00b6 Reverse debugging allows you to step/run in forward and reverse direction. It is very easy to run until an exception happened, then set a breakpoint and run backwards until that breakpoint is reached. This will stop at the point when last time the execution passed that point before the exception happened. It is very easy to uncover the actual cause of the bug this way. Both gdbgui and VS Code support reverse debugging using mozilla's rr as a backend. Note rr works only on recent Intel processors. To record a simulation, use: $ rr record ./aloha_dbg -u Cmdenv -c PureAloha1 Note Graphical programs cannot be recorded properly so we recommend running your simulation in Cmdenv mode. If you want to send a recorded simulation to an other machine, you can pack it, so all dependencies of the executable will be copied into the directory, and then that directory can be compressed and sent to a different user. $ rr pack <RECORD_DIRECTORY> Replaying with gdbgui \u00b6 Start the debugger in replay mode, which will replay the last recording: $ gdbgui --rr or an earlier recording stored in a directory: $ gdbgui DIRECTORY --rr Replaying with VS Code \u00b6 First, start a replay $ rr replay -s 50001 then the VS Code editor using the same port number $ code --open-url \"vscode://vadimcn.vscode-lldb/launch/config?{targetCreateCommands: ['target create full/path/to/omnetpp/samples/aloha/aloha_dbg'], processCreateCommands: ['gdb-remote 127.0.0.1:50001'], request: 'custom', reverseDebugging: true }\" You will see some extra buttons on the debugger toolbar allowing reverse stepping and execution.","title":"Improving the C++ Debugging Experience on Linux and other OSes"},{"location":"articles/omnetpp-debugging-tips/#the-issues","text":"Here are a few issues we have experienced at one time or another: std::string (and other standard types) displayed as <incomplete type> in the debugger seeing the internals of the class (instead of the data you are interested in) when inspecting an std::string , std::vector or other standard containers seeing the <optimised out> message when inspecting a variable in your debug build (this happens even with -O0 , which is supposed to disable all optimizations) expression evaluation is incomplete (not all C++ syntax is supported) and unreliable (often reports errors, etc.) need for step filtering: as users, we are usually not interested in debugging into the internals of the standard C++ classes linking INET with full debug info taking too long debugger taking a very long time to start up, due to the amount of debug info it needs to load debugger not stopping at breakpoints response times being too slow when debugging with gdb not all debugger frontends being able to use lldb as backend choice of standalone debugger (there are many, all of them with their own issues) But before we delve into the details, let's come clear with the basics.","title":"The Issues"},{"location":"articles/omnetpp-debugging-tips/#debugging-on-various-platforms","text":"A debugger usually consists of two parts: a frontend (UI), and a debugger backend. The backend is responsible for the actual low-level debugging tasks like starting a process, setting breakpoints, stepping in the code, querying variables etc. A frontend, on the other hand, is responsible for displaying the debugging information to the user and handling user input. The OMNeT++ IDE, which is based on Eclipse, contains a debugger frontend provided by the CDT project. The CDT debugger uses gdb as its backend. gdb must be installed separately from the IDE. On Linux, gdb is normally installed by the system's package manager. On Windows, gdb is included in the bundled MinGW tool-chain. Finally on macOS, you must get gdb from a 3 rd party package manager (like homebrew), because the gdb instance that comes with macOS is no longer maintained and is quite outdated. (This is because macOS uses lldb as the default debugger, and they no longer need/care about gdb .) Installing gdb on macOS can be a headache, because the 3 rd party gdb executable must be digitally signed locally to be able to debug other processes. Because of the above limitations, sometimes it makes sense to debug OMNeT++ models outside of the IDE. Note On macOS, OMNeT++ 6 uses lldb (through the bundled lldb-mi2 driver) so the above issues are no longer present and debugging is working out of box.","title":"Debugging on Various Platforms"},{"location":"articles/omnetpp-debugging-tips/#debugger-backends-pros-and-cons","text":"We recommend three possible debugger backends for OMNeT++. Each of them have their own strengths and weaknesses: gdb PRO: very mature debugger, with a few unique features like, reverse debugging, continuous debugging or pretty printing. A lot of debugger frontends support gdb . CON: it can get quite slow with complicated C++ code, variable evaluation and stepping is not always reliable and sometimes rather slow. On macOS, setting up gdb is quite complicated and it still has limited usability. lldb PRO: a modern debugger with great pretty printing support, fast stepping and evaluation. Works out of the box on macOS. CON: There is only a handful of frontends that support LLDB. Notably, Eclipse CDT does not support lldb at the moment. rr PRO: This is an interesting special-purpose debugger. It supports execution recording and replaying (hence the name) with forward and reverse execution of the application. It is a gdb drop-in replacement, with a few additional commands for reverse execution. CON: Few frontends support it properly, and rr itself works only on Intel-made CPUs made after cca. 2010.","title":"Debugger Backends - Pros and Cons"},{"location":"articles/omnetpp-debugging-tips/#debugger-frontends","text":"Here are a few debugger frontends we would like to highlight: Eclipse CDT This is the built-in debugger in the OMNeT++ IDE. It integrates only with gdb (at the moment), so it inherits all the advantages and disadvantages of the gdb backend. Visual Studio Code VS Code is an extensible cross platform code editor/IDE that has several debugger extensions on its Marketplace. Some of them supports gdb , lldb , or both. VS Code is practically the only viable cross-platform debugger frontend. The primary mode of launching debugging sessions in VS Code is by creating debug configurations in its own editor. Launching it from the command-line as a standalone debugger is also possible, but it requires crafting some nifty command-line arguments (which could be hidden behind a shell script, for user sanity). Xcode This is the official development environment on macOS (and only available on that platform), and uses lldb as its backend. Setting up a debugging session for an OMNeT++ model can be quite complicated, as you have to set up a workspace and a project in Xcode before debugging. gdbgui This is a Python-based frontend (for gdb and rr ) so it can naturally run on all platforms. While it is somewhat limited, its unique point is that the user interface is rendered in a browser, so you can easily debug a remotely running process. It has also first class integration with the rr reverse debugger, so you can easily navigate an execution recording. This is extremely useful to catch rarely occurring crashes and bugs. nemiver This is a GTK-based front-end for gdb . It provides a reasonably complete and comfortable debugging experience on Linux systems, albeit the user interface has some annoying quirks. As the project has not received much attention from its developers for years, it is unclear when/whether those issues are going to be resolved. KDbg KDbg is roughly the KDE equivalent of Nemiver: a Qt-based frontend to gdb .","title":"Debugger Frontends"},{"location":"articles/omnetpp-debugging-tips/#configuring-omnet-for-optimal-debugging-experience","text":"To achieve the best possible experience during debugging, you should fine tune some OMNeT++ compiler and linker flags before building OMNeT++ and your code in debug mode. Compiler flags can be configured in the configure.user file by adding the necessary options to the CFLAGS_DEBUG variable and then re-running ./configure and re-building OMNeT++ and your model. You can temporarily add these options also to Makefile.inc . In this case it is enough to just clean and re-build OMNeT++ and your model without re-configuration: make cleanall && make MODE=debug Depending on whether you intend to use gdb or lldb for debugging, you should add -ggdb3 or -glldb to the CFLAGS_DEBUG variable. Note that by default OMNeT++ assumes you intend to use gdb. You may need to configure gdb to improve load times if you work with big projects. In ~/.gdbinit specify main set worker-threads unlimited to allow parallel loading of debug information. If you use clang as your compiler and use 3 rd -party libraries in you project that do not have debugging symbols, you may have a hard time inspecting some types defined in those libraries. This is especially painful with the standard C++ library, where you cannot inspect std::string or other standard containers. This usually surfaces as a debugger error complaining about 'incomplete types'. To remedy this situation, you should install the corresponding debug symbols on your operating system (i.e. on Ubuntu: sudo apt-get install libstdc++6-8-dbg ), however note that the actual name of the debug symbol package varies widely. A more generic solution is to force the clang compiler to emit debug information for all types it encounters during compilation, but this greatly increases the size of the debug info (and the linking time as a consequence), because debug info for the standard C++ library will be included for each compilation unit. To force the compiler to generate full debug info, add -fstandalone-debug to the CFLAGS_DEBUG variable. However, this is recommended only if you cannot install the debugging symbols for the given library. !!! note This is not a problem with gcc and clang on macOS, where the default behavior is to generate full debug info anyway. If you had to apply the above workaround to get the standard C++ types working in the debugger, you may experience increased linking times, especially with big projects. You can specify -gsplit-dwarf in the CFLAGS_DEBUG variable to force the debug info into a separate (*.dwo) file for each compilation unit. This will speed up the linking process, but it will impact the startup time of the debugger. In turn, debugger startup time can be improved by supplying the -Wl,--gdb-index option as well, which enables pre-indexing of the .dwo files, thereby allowing faster loading. It is also useful to force some additional runtime-checking to avoid hard to detect bugs like stack overflows. Add -fstack-protector (or -fsanitize=safe-stack if you use clang ) to detect these issues.","title":"Configuring OMNeT++ for Optimal Debugging Experience"},{"location":"articles/omnetpp-debugging-tips/#debugging-with-vs-code","text":"VS Code has several debugger extensions, some of them supporting either gdb or lldb . We found CodeLLDB the most usable for debugging OMNeT++ models. As its name suggest, this extension uses lldb as a backend. To install, just open VS Code's extensions view, type CodeLLDB in the search field and click the 'install' button. As this debugger uses lldb , we recommend adding the -glldb option to the CFLAGS_DEBUG variable in configure.user .","title":"Debugging with VS Code"},{"location":"articles/omnetpp-debugging-tips/#launching-simulations-with-the-codelldb-debugger","text":"To launch as a standalone debugger, create an executable shell script named codelldb in your path, containing: 1 2 3 4 #!/bin/sh PROG = $( realpath $1 ) shift code --open-url \"vscode://vadimcn.vscode-lldb/launch/command? $PROG $* \" After that, you can start debugging with: codelldb ./aloha_dbg -u Cmdenv -c PureAloha2","title":"Launching Simulations with the CodeLLDB debugger"},{"location":"articles/omnetpp-debugging-tips/#attaching-codelldb-to-the-simulation","text":"It is also possible to configure OMNeT++ to allow invoking the debugger from inside the simulation by choosing the Simulate|Debug Now or Simulate|Debug Next Event menu items. In this case the simulation will execute a pre-configured command to launch the debugger and attach the current process to it. You have two options to do this: You can configure it globally by setting the following environment variable in your shell's startup file (i.e. .bashrc ): export OMNETPP_DEBUGGER_COMMAND=\"code --open-url \\\"vscode://vadimcn.vscode-lldb/launch/config?{request: 'attach', pid: '%u'}\\\"\" You can set it only for the current simulation by setting the following configuration key in your omnetpp.ini : debugger-attach-command=\"code --open-url \\\"vscode://vadimcn.vscode-lldb/launch/config?{request: 'attach', pid: '%u'}\\\"\" Both approaches will allow you to drop into the debugger interactively from Qtenv.","title":"Attaching CodeLLDB to the Simulation"},{"location":"articles/omnetpp-debugging-tips/#pretty-printing-omnet-data-structures","text":"Both gdb and lldb allows you to define pretty printers in Python that allow you to better display some complicated data structures. OMNeT++ defines such printers for certain data structures (i.e. simtime_t etc.). To activate the pretty printers import them in the VS Code debug console . command script import < OMNETPP_ROOT >/ python / omnetpp / lldb / formatters / omnetpp . py Note Formatters for gdb and lldb are not compatible with each other!","title":"Pretty Printing OMNeT++ Data Structures"},{"location":"articles/omnetpp-debugging-tips/#debugging-with-gdbgui","text":"Gdbgui is a browser-based graphical frontend for gdb and rr , written in Python. To install just type: $ pip3 install gdbgui Start the debugger using: $ gdbgui --args ./aloha_dbg -u Cmdenv -c PureAloha1 which will launch also a browser window containing the debugger UI.","title":"Debugging with gdbgui"},{"location":"articles/omnetpp-debugging-tips/#reverse-debugging-tips","text":"Reverse debugging allows you to step/run in forward and reverse direction. It is very easy to run until an exception happened, then set a breakpoint and run backwards until that breakpoint is reached. This will stop at the point when last time the execution passed that point before the exception happened. It is very easy to uncover the actual cause of the bug this way. Both gdbgui and VS Code support reverse debugging using mozilla's rr as a backend. Note rr works only on recent Intel processors. To record a simulation, use: $ rr record ./aloha_dbg -u Cmdenv -c PureAloha1 Note Graphical programs cannot be recorded properly so we recommend running your simulation in Cmdenv mode. If you want to send a recorded simulation to an other machine, you can pack it, so all dependencies of the executable will be copied into the directory, and then that directory can be compressed and sent to a different user. $ rr pack <RECORD_DIRECTORY>","title":"Reverse Debugging Tips"},{"location":"articles/omnetpp-debugging-tips/#replaying-with-gdbgui","text":"Start the debugger in replay mode, which will replay the last recording: $ gdbgui --rr or an earlier recording stored in a directory: $ gdbgui DIRECTORY --rr","title":"Replaying with gdbgui"},{"location":"articles/omnetpp-debugging-tips/#replaying-with-vs-code","text":"First, start a replay $ rr replay -s 50001 then the VS Code editor using the same port number $ code --open-url \"vscode://vadimcn.vscode-lldb/launch/config?{targetCreateCommands: ['target create full/path/to/omnetpp/samples/aloha/aloha_dbg'], processCreateCommands: ['gdb-remote 127.0.0.1:50001'], request: 'custom', reverseDebugging: true }\" You will see some extra buttons on the debugger toolbar allowing reverse stepping and execution.","title":"Replaying with VS Code"},{"location":"articles/porting-code-into-omnetpp/","text":"Network protocols and stacks are becoming increasingly more complex nowadays, to the point that writing a simulation model for them is often not a really viable option due to resource constraints. An alternative to writing a model from scratch is to reuse an existing open source real-world implementation, if one exists. Reuse has clear advantages: beyond taking much less effort to implement, one can also a spare significant amount of testing and validation effort. The drawback of course is less integration with the simulation framework (statistics, logging, etc.), difficulties with maintenance (keeping up to date with the upstream codebase), and having to live with a Frankenstein piece of software. An early example of reusing real-life network code is Sam Jensen's Network Simulation Cradle (NSC), which wraps various real-life TCP implementations (the Linux, FreeBSD, OpenBSD, and lwIP ones) in order to make them accessible in simulators such as ns-3 and OMNeT++. In this blog post, we explore some of the workable approaches of bringing an existing protocol implementation into an OMNeT++ simulation. Setting out \u00b6 Let's assume we have the external library that we want to use in a simulation. Picture lwIP for example, a TCP/IP stack which includes many other protocols. To make it usable in an OMNeT++ simulation, we'd wrap the lwIP code into a simple module which would serve as \"glue\" code. There are a few issues that obviously need to be solved, in the glue code or by modifying the lwIP code, to make it work: input/output, timers, configuring, statistics, etc. That is, the packets lwIP sends and receives need to be converted to/from OMNeT++ messages; socket API calls made in the simulation need to find their way to the appropriate functions of lwIP; timers in the lwIP code need to map to OMNeT++ self messages; configuration possibilities of lwIP need to be mapped to OMNeT++ module parameters; statistics collected inside lwIP (if there are any -- possibly they need to be added) need to be exposed as OMNeT++ signals. This is all fairly straightforward, but not always easy. It is hard to formulate generic advice on those topics, because the solution tends to depend very much on the internals of the library to be wrapped. Instead, in the rest of this post we deal with a less obvious problem that looks easy on the surface but tends to cause the most headache in practice: the issue of global variables. Most real-life software assumes it runs as a sole instance in its process, and therefore uses global variables to access its state such as configuration, connection tables, etc. However, in the simulation we typically want to create multiple instances: we want to simulate multiple routers, switches, hosts, etc. in a network. Each instance should have (sticking to the lwIP example) its own routing table, list of open sockets, etc. Thus, we need separate sets of global variables for each lwIP instance, and make sure each instance uses its own set. There are a few distinct approaches to solve this problem, such as: Packaging the software as a shared library, and loading it at runtime as many times as the number of instances you need. The dynamic loader of your system would take care that each loaded instance is a self-contained one, having no data common with the other instances. Exploiting object-orientation. If the library is implemented in C++, odds are that creating multiple instances is only as much as instantiating the corresponding classes multiple times. Or, if the library is programmed in C, it could be feasible to wrap the code into C++ classes. Introducing indirection, i.e. collecting all global variables into a struct and modifying the library to access them via a pointer, a single global variable that we always set to point to the appropriate set of variables before calling into the library. Copying, that is, we maintain a \"backup\" (or \"shadow\", if you like) set of global variables of each instance, and we copy its contents in/out of the actual global variables before and after each call into the library. In the following sections we explore these options. Loading the library multiple times \u00b6 If the library is (or can be) compiled to a shared library, the first idea is to load it multiple times. Then, for example, if you have 10 routers in the simulation and each one needs one instance of the protocol the library implements, you would load the library 10 times, in such a way that the dynamic linker of your system ensures that each loaded instance is a self-contained one, with no data common with the other instances. Sounds easy. The first hurdle is that simple dlopen() won't work: when the library is already loaded, further dlopen() calls just return the handle of the already loaded instance. (This is also true if the first instance of the library was loaded as part of the program and not via dlopen() .) It is possible to fool dlopen() by making several copies of the shared library on the disk under different names, but this solution is neither elegant nor very efficient, even if you try soft or hard links to spare disk space instead of copying. (Still, I've seen projects follow this approach; see https://github.com/simgrid/simgrid/issues/137 .) A better alternative is to use dlmopen() . The dlmopen() function differs from dlopen() primarily in that it accepts an additional argument that specifies the link-map list (also referred to as a namespace) in which the shared object should be loaded. When LM_ID_NEWLM is specified in this argument, the library will be loaded into a new empty namespace, allowing multiple instances of the library to be loaded. void * handle = dlmopen ( LM_ID_NEWLM , \"libfoo.so\" , RTLD_NOW ); An inconvenience of using libraries via dlopen() / dlmopen() is that symbols (such as function names) in the library are not directly available in the parent program, but dlsym() needs to be used to resolve them. A more serious limitation is that on the system we tested, dlmopen() only allowed 15 instances of the library to be loaded. This limit can be raised (up to 256?), but loading the library multiple times certainly doesn't scale to hundreds or thousands of instances. Exploiting object orientation \u00b6 Porting C++ libraries \u00b6 If the library was written in C++ (as opposed to pure C), chances are that functionality is nicely encapsulated into a collection of classes. If the library is well-designed, one can just create as many instances of the network stack (or the component is question) as needed. Except of course, if the library makes use of the dreaded Singleton pattern, or otherwise uses global variables. However, singletons are usually easy to identify in the code, and the source can be modified to allow several instances to coexist in memory. In the rest of this post, we assume that the library was written in C. Wrapping into C++ \u00b6 That fact that library was written in C doesn't necessarily mean in cannot be turned into (non-idiomatic) C++ with some effort. After all, most C code compiles as C++ just fine. If you manage to wrap the code into a C++ class (so that C global variables become data members of the C++ class, and C functions become member functions of the same class), it will be easy to instantiate it in multiple instances. Notice that function bodies won't need to be modified much: since this-> is implicit for member access, C variable accesses and function calls will transparently compile as member accesses and calls in C++. What needs to be changed? Assuming that all important definitions in the library are in a single header file, adding class Foo { (replace Foo with a name of your choice) near the top and }; near the bottom of the file will turn types into nested types, and variable declarations and functions into class members. (Variable declarations need a little massaging: extern needs to be removed, and the initial value brought over from the implementation ( .c ) file.) The corresponding .c file should be renamed to .cc so that it is compiled as C++, functions definitions (and possibly some more elements) in it need to be decorated with Foo:: , and global variable definitions commented out (ensuring they are declared as class members now). Some constructs such as static functions and static variables inside functions (= global variables only visible inside the function) need extra care. The resulting code should largely compile without any further changes, notably without having to change function bodies. In practice, subtle incompatibilities between C and C++ often cause compilation errors, but they are usually not hard to fix. An example of this approach is how lwIP was ported into INET Framework. Introducing indirection \u00b6 An extra level of indirection solves everything, right? Maybe not, but here it really helps. In this solution, we create a separate set of global variables for each instance, and modify the library to access the variables via a pointer (which we define as a global variable so it doesn't need to be passed around). Before each call into the library, we set the pointer to the set of globals of the correct instance. There are several ways to achieve this effect. Let's see them! Indirection by modifying the source \u00b6 The most obvious solution is to modify the source code: collect all global variables into a struct type as fields, and prefix all accesses of the individual global variables with ptr-> (better names for the pointer can certainly be invented). Now the library will access its \"global\" variables via the ptr pointer (it needs to be defined as a global variable of course). Now, one can create and use an arbitrary number of instances from the library by allocating a globals struct for each, and setting the pointer to point to right one before each call. Note that private global variables, such as those declared static in C files and inside function bodies, so they'll also need a place in the struct. The difficulty with this solution is of course the need to modify each and every global variable access in the code. The task can be pulled off by hand (tedious), or with tools like Sam Jensen's globalizer tool. The number of textual changes could be reduced by the use of macros ( #define foo ptr->m_foo ), but that often causes more problems than it solves (the name of a global variable may also occur as the name of a parameter, local variable or even a type, causing weird compile errors). Another drawback is that the above code changes are usually quite intrusive, in the sense that they are not local, but occur all over the codebase. Chances are that when you want to upgrade to the next version of the library, the original \"globalization\" patch won't apply, and you'll end up having to do the replacements all over again. Indirection by code instrumentation \u00b6 A definitely more hardcore solution is to intervene during the compilation of the library, and add the indirection then. It may sound like science fiction, but quite surprisingly, it is actually feasible. They key is to add a plugin to the Clang compiler to perform a new compilation pass. The pass will operate on the LLVM intermediate representation (IR) of the compilation unit. It can iterate over all global variable accesses in the code, and modify each one to introduce the indirection by a pointer global variable. The struct of global variables can also be created during the compiler pass. This may sounds straightforward, but there are a number of subtle problems to solve along the way. For example, since the C++ compiler processes each file in isolation, it is not easy to decide in which file to place the new global variables so as not to cause multiple definitions. Attila T\u00f6r\u00f6k in our team has experimented with this approach, and while not solving it completely, he has gotten quite far with it. If you are interested in picking up where he left off, drop us an email. The drawback of this (otherwise very elegant) solution is the deployment. If you want your users to be able to compile the library, they'll need the compiler plugin. And if you distribute the compiler plugin in source form (it is written in C++), they'll need to install the clang-devel package to be able to build it. Indirection by hacking the dynamic linker \u00b6 There might be a third solution as well, involving the dynamic linker. When accessing functions and global variables loaded from a shared library, there is already an indirection in place: all such accesses go through the Global Offset Table (GOT). In theory, it could be possible to update the corresponding entries in the GOT to point to the desired set of global variables before each call into the library. For that matter, it could also be possible to have separate instances of the GOT table for each library instance, and just activate the right GOT before calls into the library. So far we haven't figured out enough about the GOT data structure and the dynamic loader in general to judge whether the above idea is feasible or not. It is unclear whether there is enough information available at runtime to identify the relevant entries in the GOT, or even, whether GOT is writable at all. One thing is certain: such as solution, even if it worked, would be extremely platform dependent. Copying \u00b6 The alternative to indirection is copying. In this approach, we maintain a \"shadow\" set of global variables for each library instance. Before before each call into the library, we populate the global variables from the instance's \"shadow\" set, and after the call, we update the \"shadow\" set by copying the global variables into it. Whenever a new library instance is needed, we populate the new \"shadow\" set from a pristine backup copy that we saved early, before the first call into the library. This is possible to do with C libraries because, unlike C++ libraries, they normally don't contain initialization code that automatically runs on loading the library. (In C++, constructors of global objects are run automatically on library load, which makes things tricky. In C, global variables are only allowed to have constant initializers. While it is also possible in C to create initialization functions run automatically on library load, e.g. via __attribute__ ((constructor)) , that is not very common.) The cost of copying before/after calls may or may not be significant, depending on how the library was written. If the library mostly uses dynamically allocated data structures, the cost is small. Static allocations are the problem. Static buffers, for example, can add a lot to the amount of memory that needs to be copied. In such cases, modifying the library to allocate the buffers dynamically can make a lot of difference in performance. While the copying approach is certainly less efficient than the indirection approach (where all it takes to switch instance is to change the value of a single pointer), it is significantly easier to implement, and requires fewer (and less intrusive) modification to the library source code. There are several ways this approach can be implemented. Copying variables one by one \u00b6 A straightforward approach is to collect the global variables of the library into a struct type that represents the \"shadow\" set (while leaving the original source intact), and write two functions that contain a series of assignments: one would copy the global variables into a struct, and the other would copy in the reverse direction. If library contains global variables that are not accessible to the copying functions (because they are marked static and are local to a file or are inside functions), such places need to be modified in the library source. The drawback of this solution is that it requires you to collect the list of global variables (which requires attention when you upgrade to a newer version of the library), and may also require source code changes to the library. The next approach is also not without problems, but it eliminates both these issues. Copying the library's data segment \u00b6 It is possible to automate the above approach. Global variables of a shared library form a contiguous region in the memory, so it we know the range of addressses it occupies, we can copy all of them in one go, using a simple memcpy() call. The difficult problem is how to determine the address range. Unfortunately there is no \"official\" solution. Our first idea was this: if the compiler preserves the order of variables within a compilation unit, we can place guard variables like startVars and endVars at the top and bottom of each source file, and the address range between them will include all global variables. Unfortunately, the assumption was wrong: variables seemed to appear in the memory in an arbitrary order. Later we learned gcc has a -fno-toplevel-reorder option for exactly this purpose. Similarly, if the linker preserves the order the object files appear on its command line, it should be possible to surround the list objects files with a startvars.o and an endvars.o file that contain similarly named guard variables. Unfortunately, it also turned out that the actual order of variables in the shared library does not necessarily reflect their original order on the command line. And, the linker does not appear to have an analogous do-not-reorder option. The next option we explored was to get the address range from the dynamic linker. Indeed, there is a dl_iterate_phdr() API function that enumerates the loaded shared libraries and the segments in each. We figured out that global variables are in the segment that has type 1 (PT_LOAD) and flags 6 (R+W). However, getting the address range of that segment was not much use to us. First, it points to readonly memory within the area where the shared object file was loaded by the dynamic linker, and not to the region where the library's actual variables are. Second, it includes important internal data structures (a few hundred bytes at the start) in addition to the global variables, and apparently those data structures should not be messed with. These hurdles, especially the second one, could not be overcome. The third idea that finally worked was to use a linker script to insert the guard variables. The main purpose of the linker script is to describe for the linker how the sections in the input files (object files, libraries, etc) should be mapped into the output file, and to control the memory layout of the output file. Linker scripts are normally considered an internal matter of the linking process, but it's possible to export one, modify it, and let the linker use the modified version. For example, by editing the .data block, and adding startmarker.o(.data); at the top and endmarker.o(.data); at the bottom, one can achieve that the data of the two named object files ( startmarker.o and endmarker.o ) are placed at the top and bottom of the data segment. If those files contain guard variables, their address range at runtime denotes the data area of the library. Since we stopped experimenting before reaching a conclusion, it is still unclear whether this approach can be made to work. Conclusion \u00b6 With so many options, the question is which approach you should choose in your project. Here are a few guidelines. Multiple loading is relatively straightforward, and the required effort does not depend of on the size of the codebase (huge libraries are just as easy to load as small ones), but is limited in the number of instances you can create (usually a few dozen). Therefore, it is practical for libraries which have a large codebase or whose source is not available -- provided that it doesn't need to be instantiated on a massive scale. Copying is not very efficient (usually there too many variables, static tables, etc). Simulations using that approach may be slow. Wrapping into C++ might not be practical with C code that contains a lot of constructs that don't compile as C++ (too much patching required). Choose your own weapon.","title":"Porting Real-World Protocol Implementations into OMNeT++"},{"location":"articles/porting-code-into-omnetpp/#setting-out","text":"Let's assume we have the external library that we want to use in a simulation. Picture lwIP for example, a TCP/IP stack which includes many other protocols. To make it usable in an OMNeT++ simulation, we'd wrap the lwIP code into a simple module which would serve as \"glue\" code. There are a few issues that obviously need to be solved, in the glue code or by modifying the lwIP code, to make it work: input/output, timers, configuring, statistics, etc. That is, the packets lwIP sends and receives need to be converted to/from OMNeT++ messages; socket API calls made in the simulation need to find their way to the appropriate functions of lwIP; timers in the lwIP code need to map to OMNeT++ self messages; configuration possibilities of lwIP need to be mapped to OMNeT++ module parameters; statistics collected inside lwIP (if there are any -- possibly they need to be added) need to be exposed as OMNeT++ signals. This is all fairly straightforward, but not always easy. It is hard to formulate generic advice on those topics, because the solution tends to depend very much on the internals of the library to be wrapped. Instead, in the rest of this post we deal with a less obvious problem that looks easy on the surface but tends to cause the most headache in practice: the issue of global variables. Most real-life software assumes it runs as a sole instance in its process, and therefore uses global variables to access its state such as configuration, connection tables, etc. However, in the simulation we typically want to create multiple instances: we want to simulate multiple routers, switches, hosts, etc. in a network. Each instance should have (sticking to the lwIP example) its own routing table, list of open sockets, etc. Thus, we need separate sets of global variables for each lwIP instance, and make sure each instance uses its own set. There are a few distinct approaches to solve this problem, such as: Packaging the software as a shared library, and loading it at runtime as many times as the number of instances you need. The dynamic loader of your system would take care that each loaded instance is a self-contained one, having no data common with the other instances. Exploiting object-orientation. If the library is implemented in C++, odds are that creating multiple instances is only as much as instantiating the corresponding classes multiple times. Or, if the library is programmed in C, it could be feasible to wrap the code into C++ classes. Introducing indirection, i.e. collecting all global variables into a struct and modifying the library to access them via a pointer, a single global variable that we always set to point to the appropriate set of variables before calling into the library. Copying, that is, we maintain a \"backup\" (or \"shadow\", if you like) set of global variables of each instance, and we copy its contents in/out of the actual global variables before and after each call into the library. In the following sections we explore these options.","title":"Setting out"},{"location":"articles/porting-code-into-omnetpp/#loading-the-library-multiple-times","text":"If the library is (or can be) compiled to a shared library, the first idea is to load it multiple times. Then, for example, if you have 10 routers in the simulation and each one needs one instance of the protocol the library implements, you would load the library 10 times, in such a way that the dynamic linker of your system ensures that each loaded instance is a self-contained one, with no data common with the other instances. Sounds easy. The first hurdle is that simple dlopen() won't work: when the library is already loaded, further dlopen() calls just return the handle of the already loaded instance. (This is also true if the first instance of the library was loaded as part of the program and not via dlopen() .) It is possible to fool dlopen() by making several copies of the shared library on the disk under different names, but this solution is neither elegant nor very efficient, even if you try soft or hard links to spare disk space instead of copying. (Still, I've seen projects follow this approach; see https://github.com/simgrid/simgrid/issues/137 .) A better alternative is to use dlmopen() . The dlmopen() function differs from dlopen() primarily in that it accepts an additional argument that specifies the link-map list (also referred to as a namespace) in which the shared object should be loaded. When LM_ID_NEWLM is specified in this argument, the library will be loaded into a new empty namespace, allowing multiple instances of the library to be loaded. void * handle = dlmopen ( LM_ID_NEWLM , \"libfoo.so\" , RTLD_NOW ); An inconvenience of using libraries via dlopen() / dlmopen() is that symbols (such as function names) in the library are not directly available in the parent program, but dlsym() needs to be used to resolve them. A more serious limitation is that on the system we tested, dlmopen() only allowed 15 instances of the library to be loaded. This limit can be raised (up to 256?), but loading the library multiple times certainly doesn't scale to hundreds or thousands of instances.","title":"Loading the library multiple times"},{"location":"articles/porting-code-into-omnetpp/#exploiting-object-orientation","text":"","title":"Exploiting object orientation"},{"location":"articles/porting-code-into-omnetpp/#porting-c-libraries","text":"If the library was written in C++ (as opposed to pure C), chances are that functionality is nicely encapsulated into a collection of classes. If the library is well-designed, one can just create as many instances of the network stack (or the component is question) as needed. Except of course, if the library makes use of the dreaded Singleton pattern, or otherwise uses global variables. However, singletons are usually easy to identify in the code, and the source can be modified to allow several instances to coexist in memory. In the rest of this post, we assume that the library was written in C.","title":"Porting C++ libraries"},{"location":"articles/porting-code-into-omnetpp/#wrapping-into-c","text":"That fact that library was written in C doesn't necessarily mean in cannot be turned into (non-idiomatic) C++ with some effort. After all, most C code compiles as C++ just fine. If you manage to wrap the code into a C++ class (so that C global variables become data members of the C++ class, and C functions become member functions of the same class), it will be easy to instantiate it in multiple instances. Notice that function bodies won't need to be modified much: since this-> is implicit for member access, C variable accesses and function calls will transparently compile as member accesses and calls in C++. What needs to be changed? Assuming that all important definitions in the library are in a single header file, adding class Foo { (replace Foo with a name of your choice) near the top and }; near the bottom of the file will turn types into nested types, and variable declarations and functions into class members. (Variable declarations need a little massaging: extern needs to be removed, and the initial value brought over from the implementation ( .c ) file.) The corresponding .c file should be renamed to .cc so that it is compiled as C++, functions definitions (and possibly some more elements) in it need to be decorated with Foo:: , and global variable definitions commented out (ensuring they are declared as class members now). Some constructs such as static functions and static variables inside functions (= global variables only visible inside the function) need extra care. The resulting code should largely compile without any further changes, notably without having to change function bodies. In practice, subtle incompatibilities between C and C++ often cause compilation errors, but they are usually not hard to fix. An example of this approach is how lwIP was ported into INET Framework.","title":"Wrapping into C++"},{"location":"articles/porting-code-into-omnetpp/#introducing-indirection","text":"An extra level of indirection solves everything, right? Maybe not, but here it really helps. In this solution, we create a separate set of global variables for each instance, and modify the library to access the variables via a pointer (which we define as a global variable so it doesn't need to be passed around). Before each call into the library, we set the pointer to the set of globals of the correct instance. There are several ways to achieve this effect. Let's see them!","title":"Introducing indirection"},{"location":"articles/porting-code-into-omnetpp/#indirection-by-modifying-the-source","text":"The most obvious solution is to modify the source code: collect all global variables into a struct type as fields, and prefix all accesses of the individual global variables with ptr-> (better names for the pointer can certainly be invented). Now the library will access its \"global\" variables via the ptr pointer (it needs to be defined as a global variable of course). Now, one can create and use an arbitrary number of instances from the library by allocating a globals struct for each, and setting the pointer to point to right one before each call. Note that private global variables, such as those declared static in C files and inside function bodies, so they'll also need a place in the struct. The difficulty with this solution is of course the need to modify each and every global variable access in the code. The task can be pulled off by hand (tedious), or with tools like Sam Jensen's globalizer tool. The number of textual changes could be reduced by the use of macros ( #define foo ptr->m_foo ), but that often causes more problems than it solves (the name of a global variable may also occur as the name of a parameter, local variable or even a type, causing weird compile errors). Another drawback is that the above code changes are usually quite intrusive, in the sense that they are not local, but occur all over the codebase. Chances are that when you want to upgrade to the next version of the library, the original \"globalization\" patch won't apply, and you'll end up having to do the replacements all over again.","title":"Indirection by modifying the source"},{"location":"articles/porting-code-into-omnetpp/#indirection-by-code-instrumentation","text":"A definitely more hardcore solution is to intervene during the compilation of the library, and add the indirection then. It may sound like science fiction, but quite surprisingly, it is actually feasible. They key is to add a plugin to the Clang compiler to perform a new compilation pass. The pass will operate on the LLVM intermediate representation (IR) of the compilation unit. It can iterate over all global variable accesses in the code, and modify each one to introduce the indirection by a pointer global variable. The struct of global variables can also be created during the compiler pass. This may sounds straightforward, but there are a number of subtle problems to solve along the way. For example, since the C++ compiler processes each file in isolation, it is not easy to decide in which file to place the new global variables so as not to cause multiple definitions. Attila T\u00f6r\u00f6k in our team has experimented with this approach, and while not solving it completely, he has gotten quite far with it. If you are interested in picking up where he left off, drop us an email. The drawback of this (otherwise very elegant) solution is the deployment. If you want your users to be able to compile the library, they'll need the compiler plugin. And if you distribute the compiler plugin in source form (it is written in C++), they'll need to install the clang-devel package to be able to build it.","title":"Indirection by code instrumentation"},{"location":"articles/porting-code-into-omnetpp/#indirection-by-hacking-the-dynamic-linker","text":"There might be a third solution as well, involving the dynamic linker. When accessing functions and global variables loaded from a shared library, there is already an indirection in place: all such accesses go through the Global Offset Table (GOT). In theory, it could be possible to update the corresponding entries in the GOT to point to the desired set of global variables before each call into the library. For that matter, it could also be possible to have separate instances of the GOT table for each library instance, and just activate the right GOT before calls into the library. So far we haven't figured out enough about the GOT data structure and the dynamic loader in general to judge whether the above idea is feasible or not. It is unclear whether there is enough information available at runtime to identify the relevant entries in the GOT, or even, whether GOT is writable at all. One thing is certain: such as solution, even if it worked, would be extremely platform dependent.","title":"Indirection by hacking the dynamic linker"},{"location":"articles/porting-code-into-omnetpp/#copying","text":"The alternative to indirection is copying. In this approach, we maintain a \"shadow\" set of global variables for each library instance. Before before each call into the library, we populate the global variables from the instance's \"shadow\" set, and after the call, we update the \"shadow\" set by copying the global variables into it. Whenever a new library instance is needed, we populate the new \"shadow\" set from a pristine backup copy that we saved early, before the first call into the library. This is possible to do with C libraries because, unlike C++ libraries, they normally don't contain initialization code that automatically runs on loading the library. (In C++, constructors of global objects are run automatically on library load, which makes things tricky. In C, global variables are only allowed to have constant initializers. While it is also possible in C to create initialization functions run automatically on library load, e.g. via __attribute__ ((constructor)) , that is not very common.) The cost of copying before/after calls may or may not be significant, depending on how the library was written. If the library mostly uses dynamically allocated data structures, the cost is small. Static allocations are the problem. Static buffers, for example, can add a lot to the amount of memory that needs to be copied. In such cases, modifying the library to allocate the buffers dynamically can make a lot of difference in performance. While the copying approach is certainly less efficient than the indirection approach (where all it takes to switch instance is to change the value of a single pointer), it is significantly easier to implement, and requires fewer (and less intrusive) modification to the library source code. There are several ways this approach can be implemented.","title":"Copying"},{"location":"articles/porting-code-into-omnetpp/#copying-variables-one-by-one","text":"A straightforward approach is to collect the global variables of the library into a struct type that represents the \"shadow\" set (while leaving the original source intact), and write two functions that contain a series of assignments: one would copy the global variables into a struct, and the other would copy in the reverse direction. If library contains global variables that are not accessible to the copying functions (because they are marked static and are local to a file or are inside functions), such places need to be modified in the library source. The drawback of this solution is that it requires you to collect the list of global variables (which requires attention when you upgrade to a newer version of the library), and may also require source code changes to the library. The next approach is also not without problems, but it eliminates both these issues.","title":"Copying variables one by one"},{"location":"articles/porting-code-into-omnetpp/#copying-the-librarys-data-segment","text":"It is possible to automate the above approach. Global variables of a shared library form a contiguous region in the memory, so it we know the range of addressses it occupies, we can copy all of them in one go, using a simple memcpy() call. The difficult problem is how to determine the address range. Unfortunately there is no \"official\" solution. Our first idea was this: if the compiler preserves the order of variables within a compilation unit, we can place guard variables like startVars and endVars at the top and bottom of each source file, and the address range between them will include all global variables. Unfortunately, the assumption was wrong: variables seemed to appear in the memory in an arbitrary order. Later we learned gcc has a -fno-toplevel-reorder option for exactly this purpose. Similarly, if the linker preserves the order the object files appear on its command line, it should be possible to surround the list objects files with a startvars.o and an endvars.o file that contain similarly named guard variables. Unfortunately, it also turned out that the actual order of variables in the shared library does not necessarily reflect their original order on the command line. And, the linker does not appear to have an analogous do-not-reorder option. The next option we explored was to get the address range from the dynamic linker. Indeed, there is a dl_iterate_phdr() API function that enumerates the loaded shared libraries and the segments in each. We figured out that global variables are in the segment that has type 1 (PT_LOAD) and flags 6 (R+W). However, getting the address range of that segment was not much use to us. First, it points to readonly memory within the area where the shared object file was loaded by the dynamic linker, and not to the region where the library's actual variables are. Second, it includes important internal data structures (a few hundred bytes at the start) in addition to the global variables, and apparently those data structures should not be messed with. These hurdles, especially the second one, could not be overcome. The third idea that finally worked was to use a linker script to insert the guard variables. The main purpose of the linker script is to describe for the linker how the sections in the input files (object files, libraries, etc) should be mapped into the output file, and to control the memory layout of the output file. Linker scripts are normally considered an internal matter of the linking process, but it's possible to export one, modify it, and let the linker use the modified version. For example, by editing the .data block, and adding startmarker.o(.data); at the top and endmarker.o(.data); at the bottom, one can achieve that the data of the two named object files ( startmarker.o and endmarker.o ) are placed at the top and bottom of the data segment. If those files contain guard variables, their address range at runtime denotes the data area of the library. Since we stopped experimenting before reaching a conclusion, it is still unclear whether this approach can be made to work.","title":"Copying the library's data segment"},{"location":"articles/porting-code-into-omnetpp/#conclusion","text":"With so many options, the question is which approach you should choose in your project. Here are a few guidelines. Multiple loading is relatively straightforward, and the required effort does not depend of on the size of the codebase (huge libraries are just as easy to load as small ones), but is limited in the number of instances you can create (usually a few dozen). Therefore, it is practical for libraries which have a large codebase or whose source is not available -- provided that it doesn't need to be instantiated on a massive scale. Copying is not very efficient (usually there too many variables, static tables, etc). Simulations using that approach may be slow. Wrapping into C++ might not be practical with C code that contains a lot of constructs that don't compile as C++ (too much patching required). Choose your own weapon.","title":"Conclusion"},{"location":"articles/running-omnetpp-in-docker/","text":"Goals \u00b6 It is often challenging to get models written for older versions of OMNeT++ working in contemporary Linux distributions. Older versions of OMNeT++ are often not compatible with newer Linux distibutions: changes in the C++ compiler and in the dependencies (library versions, etc.) often cause installation failure. One solution to this problem is using Docker. We provide Docker images that contain older versions OMNeT++, already in compiled form. It is possible to compile and run the simulation models in these Docker images. How to use the OMNeT++ Docker images \u00b6 As images are published on the Docker Hub, it is straightforward to deploy OMNeT++ on any machine that has Docker. You can see the list of the available images here: https://hub.docker.com/r/omnetpp/omnetpp/tags . Image tags have the syntax u18.04-5.5.1 , where the first part is the Ubuntu version, and the second part is the OMNeT++. After choosing the suitable image, change into the directory of the simulation model, and issue the following command (replace the end of the last argument with the proper image tag): docker run --rm -it -v \"$(pwd):/root/models\" -u \"$(id -u):$(id -g)\" omnetpp/omnetpp:u18.04-5.5.1 This command will download the image (unless already downloaded), and opens a shell inside the container. The current working directory will be mapped to /root/models inside the Docker container. Then follow the build instructions of the simulation model (for example, type make ). When the build process completes, you can run the simulation. Note that you can only run simulation under Cmdenv and in release mode, and cannot use the IDE. The reason is that we wanted to keep the size of these Docker images relatively small. Including the IDE, debug mode libraries, the Qt libraries for Qtenv, etc. would have blown up the image size considerably. Everything that is created under /root/models inside the Docker image, such as build artifacts and simulation result files, will be available in your local file system (in the directory where you issued the Docker command). This setup lets edit the model files outside of the container with your favorite editor. Further possible uses \u00b6 We are internally using these Docker images for continuous testing of INET. A future possible use case is creating and publishing reproducible simulations. The researcher who publishes the simulation would create a Docker image based on one of the OMNeT++ images, and push it to Docker Hub. The new image would contain a runnable version of the simulation. The label of the new image would be advertised, for example inside the corresponding research paper. Any researcher interested in the study would be able to pull and run the image to reproduce the results. Alternatively, it could also be sufficient to publish the hash of the commit containing the \"final\" version of the model in a public Git repository, plus the tag of the OMNeT++ Docker image required for running it. This would also allow 3 rd party researchers to build and run the model in a reproducible way.","title":"Using Docker for Running Simulations on Various Versions of OMNeT++"},{"location":"articles/running-omnetpp-in-docker/#goals","text":"It is often challenging to get models written for older versions of OMNeT++ working in contemporary Linux distributions. Older versions of OMNeT++ are often not compatible with newer Linux distibutions: changes in the C++ compiler and in the dependencies (library versions, etc.) often cause installation failure. One solution to this problem is using Docker. We provide Docker images that contain older versions OMNeT++, already in compiled form. It is possible to compile and run the simulation models in these Docker images.","title":"Goals"},{"location":"articles/running-omnetpp-in-docker/#how-to-use-the-omnet-docker-images","text":"As images are published on the Docker Hub, it is straightforward to deploy OMNeT++ on any machine that has Docker. You can see the list of the available images here: https://hub.docker.com/r/omnetpp/omnetpp/tags . Image tags have the syntax u18.04-5.5.1 , where the first part is the Ubuntu version, and the second part is the OMNeT++. After choosing the suitable image, change into the directory of the simulation model, and issue the following command (replace the end of the last argument with the proper image tag): docker run --rm -it -v \"$(pwd):/root/models\" -u \"$(id -u):$(id -g)\" omnetpp/omnetpp:u18.04-5.5.1 This command will download the image (unless already downloaded), and opens a shell inside the container. The current working directory will be mapped to /root/models inside the Docker container. Then follow the build instructions of the simulation model (for example, type make ). When the build process completes, you can run the simulation. Note that you can only run simulation under Cmdenv and in release mode, and cannot use the IDE. The reason is that we wanted to keep the size of these Docker images relatively small. Including the IDE, debug mode libraries, the Qt libraries for Qtenv, etc. would have blown up the image size considerably. Everything that is created under /root/models inside the Docker image, such as build artifacts and simulation result files, will be available in your local file system (in the directory where you issued the Docker command). This setup lets edit the model files outside of the container with your favorite editor.","title":"How to use the OMNeT++ Docker images"},{"location":"articles/running-omnetpp-in-docker/#further-possible-uses","text":"We are internally using these Docker images for continuous testing of INET. A future possible use case is creating and publishing reproducible simulations. The researcher who publishes the simulation would create a Docker image based on one of the OMNeT++ images, and push it to Docker Hub. The new image would contain a runnable version of the simulation. The label of the new image would be advertised, for example inside the corresponding research paper. Any researcher interested in the study would be able to pull and run the image to reproduce the results. Alternatively, it could also be sufficient to publish the hash of the commit containing the \"final\" version of the model in a public Git repository, plus the tag of the OMNeT++ Docker image required for running it. This would also allow 3 rd party researchers to build and run the model in a reproducible way.","title":"Further possible uses"},{"location":"articles/running-omnetppgui-in-docker/","text":"Goals \u00b6 In another article, we have shown how to (and why) run simulations in a Docker container. However, Docker in itself is not suited to running GUI applications, so it is not ideal for developing or exploring simulations due to the unavailability of the IDE and Qtenv. In this article, we explore options to overcome this limitation. Survey of options \u00b6 A common way of running GUI applications in Docker is X11, a network protocol for remote graphical user interfaces. X11 requires an X11 server to be running, to which clients can connect and display their GUIs on. On Linux systems, the desktop is by default X11-based, so one doesn't need to install any extra software. On Windows and macOS, one can install a 3 rd party X11 server (for example, VcXsrv on Windows and XQuartz on macOS). We also looked into remote desktop solutions (like VNC or RDP), where the server runs in the container, and the host connects to it as a client. Those, however, do not provide the most seamless experience, as you see a virtual desktop, not separate windows. Moreover, performance/quality might not be the best, even when running locally on the same machine. From these two, RDP seems to be more promising, as some versions support seamless mode where applications on the remote machine - in our case, inside the container - can launch separate windows on the host display. Sadly, the configuration of the RDP server is somewhat complicated. In the future, Wayland may become a viable alternative to X11 in our use case as well. The IDE uses the GTK3 backend and Qtenv uses Qt5, and both of these widget libraries come with perfectly usable Wayland support. Choosing the proper Docker image \u00b6 If you want to use the GUI, you need a Docker image that contains the IDE, Qtenv, and all the libraries necessary for their operation. (Many OMNeT++ Docker images don't contain these parts, in order to keep their sizes small.) The list of GUI-enabled OMNeT++ images is available at the following URL: https://hub.docker.com/r/omnetpp/omnetpp-gui/tags Docker with X11 \u00b6 With the following single command you can try out OMNeT++ on any system with Docker and an X11 server on it: docker run --rm -it -v \"$(pwd):/root/models\" -u \"$(id -u):$(id -g)\" \\ -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY omnetpp/omnetpp-gui:u18.04-5.5.1 The 2 nd -v option is used here to map the X11 socket into the container. The x11docker project can be useful here. It is a collection of tricks, flags, and methods to run graphical applications in Docker (with several ways of operation, including X11 and Wayland support, GPU passthrough, all with various levels of isolation and performance, etc). x11docker allows the above command line to be simplified to: x11docker -i -- --rm -v \"$(pwd):/root/models\" -- omnetpp/omnetpp-gui:u18.04-5.5.1 Feedback \u00b6 If you found this interesting or useful, make use of the it in practice, or you have ideas on how to take it further, please let us know!","title":"OMNeT++ GUI in Docker"},{"location":"articles/running-omnetppgui-in-docker/#goals","text":"In another article, we have shown how to (and why) run simulations in a Docker container. However, Docker in itself is not suited to running GUI applications, so it is not ideal for developing or exploring simulations due to the unavailability of the IDE and Qtenv. In this article, we explore options to overcome this limitation.","title":"Goals"},{"location":"articles/running-omnetppgui-in-docker/#survey-of-options","text":"A common way of running GUI applications in Docker is X11, a network protocol for remote graphical user interfaces. X11 requires an X11 server to be running, to which clients can connect and display their GUIs on. On Linux systems, the desktop is by default X11-based, so one doesn't need to install any extra software. On Windows and macOS, one can install a 3 rd party X11 server (for example, VcXsrv on Windows and XQuartz on macOS). We also looked into remote desktop solutions (like VNC or RDP), where the server runs in the container, and the host connects to it as a client. Those, however, do not provide the most seamless experience, as you see a virtual desktop, not separate windows. Moreover, performance/quality might not be the best, even when running locally on the same machine. From these two, RDP seems to be more promising, as some versions support seamless mode where applications on the remote machine - in our case, inside the container - can launch separate windows on the host display. Sadly, the configuration of the RDP server is somewhat complicated. In the future, Wayland may become a viable alternative to X11 in our use case as well. The IDE uses the GTK3 backend and Qtenv uses Qt5, and both of these widget libraries come with perfectly usable Wayland support.","title":"Survey of options"},{"location":"articles/running-omnetppgui-in-docker/#choosing-the-proper-docker-image","text":"If you want to use the GUI, you need a Docker image that contains the IDE, Qtenv, and all the libraries necessary for their operation. (Many OMNeT++ Docker images don't contain these parts, in order to keep their sizes small.) The list of GUI-enabled OMNeT++ images is available at the following URL: https://hub.docker.com/r/omnetpp/omnetpp-gui/tags","title":"Choosing the proper Docker image"},{"location":"articles/running-omnetppgui-in-docker/#docker-with-x11","text":"With the following single command you can try out OMNeT++ on any system with Docker and an X11 server on it: docker run --rm -it -v \"$(pwd):/root/models\" -u \"$(id -u):$(id -g)\" \\ -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY omnetpp/omnetpp-gui:u18.04-5.5.1 The 2 nd -v option is used here to map the X11 socket into the container. The x11docker project can be useful here. It is a collection of tricks, flags, and methods to run graphical applications in Docker (with several ways of operation, including X11 and Wayland support, GPU passthrough, all with various levels of isolation and performance, etc). x11docker allows the above command line to be simplified to: x11docker -i -- --rm -v \"$(pwd):/root/models\" -- omnetpp/omnetpp-gui:u18.04-5.5.1","title":"Docker with X11"},{"location":"articles/running-omnetppgui-in-docker/#feedback","text":"If you found this interesting or useful, make use of the it in practice, or you have ideas on how to take it further, please let us know!","title":"Feedback"},{"location":"articles/wsl2/","text":"Motivation \u00b6 Windows Subsystem for Linux version 2 (WSL 2) is a Windows facility for running a full-blown Linux distribution in Windows. Running OMNeT++ in WSL 2 has certain advantages compared to running OMNeT++ natively on Windows: Advantages: You will probably see significant speedup on certain tasks (like compilation) compared to the native Windows (MinGW64) toolchain, because the compiler toolchain and the filesystem (ext4) is much faster in WSL 2 than their Windows equivalents. The native MinGW64 toolchain on Windows is basically a mini (Unix-like) system, emulated on top of Windows. Because of the emulation, it may have incompatibilities and limitations compared to the Linux tools. You will have fewer issues and surprises when running OMNeT++ on Linux. There are a few drawbacks as well: WSL 2 does not (yet) support running Linux GUI applications. This means that you must install and run an X Server process on Windows to be able to use any GUI tools (i.e. IDE, Qtenv, etc.) from OMNeT++. Because of a limitation of the available X Server software, 3D acceleration is not working. You will not be able to use the OMNeT++ OpenSceneGraph and osgEarth integration in this setup and it is recommended to explicitly disable these features when you build OMNeT++. Supported Windows Versions \u00b6 Installing OMNeT++ on WSL 2 is supported on Windows 10 version 1903 (build 18362.1049) or later. Note especially the minor build number. Your Windows version must have at least 1049 as a minor build number. Enabling WSL 2 on Windows \u00b6 Open a PowerShell with Administrator privileges. On newer versions of Windows, you can install the WSL subsystem by typing: wsl --install Alternatively, if your system does not have a wsl command, use the following commands: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart After a successful installation, reboot your computer and open an Administrator PowerShell again to set the default WSL version to 2. wsl.exe --set-default-version 2 Tip We recommend installing and using the Windows Terminal application, which is available at https://www.microsoft.com/store/productId/9N0DX20HK701 Installing an Ubuntu distribution \u00b6 As a next step, you must install a Linux distribution from the Microsoft Store. We recommend using Ubuntu 20.04 from https://www.microsoft.com/store/productId/9n6svws3rx71 . Once the installation is done, run the distro and finish the setup process by setting up a user name and password. At this point, you could install OMNeT++, but GUI programs would not work. Install VcXserver \u00b6 To use GUI programs from Linux, you must install an X Server application from: https://sourceforge.net/projects/vcxsrv/ Start the installation and make sure that you: select \"Disable access control\" set display number to 0 check \"Private networks, such as my home or work network\" and click \"Allow access\" when the Windows Defender Firewall asks for permission. Open the Windows Terminal and launch the Ubuntu distribution from the dropdown menu. Add the following line to the /etc/bash.bashrc or ~/.bashrc file. export DISPLAY=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}'):0.0 This will ensure that Linux programs will always find the X Server process running on Windows. Exit from the Ubuntu shell, and restart it to make sure that the change was applied correctly. Check if $ echo $DISPLAY displays the correct IP address of the Windows machine. In the future, make sure that the X Server is always running when you want to run Linux GUI programs by either making the X Server automatically start or launching it manually. Tip There is ongoing work to make Linux GUI applications work on Windows by default. On later versions of Windows you may be able to skip the whole X Server installation step. Install the Ubuntu Version of OMNeT++ \u00b6 At this point, you have a fully functional Linux environment that can run GUI apps. You can go on and follow the Ubuntu specific installation steps to finally install OMNeT++ on your system. Usage \u00b6 Make sure the X server is running. Then, open the Windows Terminal and launch the Ubuntu distribution from the dropdown menu. From this point on, you can use OMNeT++ exactly as you would use it on Ubuntu.","title":"Installing OMNeT++ on Windows Subsystem for Linux (WSL 2)"},{"location":"articles/wsl2/#motivation","text":"Windows Subsystem for Linux version 2 (WSL 2) is a Windows facility for running a full-blown Linux distribution in Windows. Running OMNeT++ in WSL 2 has certain advantages compared to running OMNeT++ natively on Windows: Advantages: You will probably see significant speedup on certain tasks (like compilation) compared to the native Windows (MinGW64) toolchain, because the compiler toolchain and the filesystem (ext4) is much faster in WSL 2 than their Windows equivalents. The native MinGW64 toolchain on Windows is basically a mini (Unix-like) system, emulated on top of Windows. Because of the emulation, it may have incompatibilities and limitations compared to the Linux tools. You will have fewer issues and surprises when running OMNeT++ on Linux. There are a few drawbacks as well: WSL 2 does not (yet) support running Linux GUI applications. This means that you must install and run an X Server process on Windows to be able to use any GUI tools (i.e. IDE, Qtenv, etc.) from OMNeT++. Because of a limitation of the available X Server software, 3D acceleration is not working. You will not be able to use the OMNeT++ OpenSceneGraph and osgEarth integration in this setup and it is recommended to explicitly disable these features when you build OMNeT++.","title":"Motivation"},{"location":"articles/wsl2/#supported-windows-versions","text":"Installing OMNeT++ on WSL 2 is supported on Windows 10 version 1903 (build 18362.1049) or later. Note especially the minor build number. Your Windows version must have at least 1049 as a minor build number.","title":"Supported Windows Versions"},{"location":"articles/wsl2/#enabling-wsl-2-on-windows","text":"Open a PowerShell with Administrator privileges. On newer versions of Windows, you can install the WSL subsystem by typing: wsl --install Alternatively, if your system does not have a wsl command, use the following commands: dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart After a successful installation, reboot your computer and open an Administrator PowerShell again to set the default WSL version to 2. wsl.exe --set-default-version 2 Tip We recommend installing and using the Windows Terminal application, which is available at https://www.microsoft.com/store/productId/9N0DX20HK701","title":"Enabling WSL 2 on Windows"},{"location":"articles/wsl2/#installing-an-ubuntu-distribution","text":"As a next step, you must install a Linux distribution from the Microsoft Store. We recommend using Ubuntu 20.04 from https://www.microsoft.com/store/productId/9n6svws3rx71 . Once the installation is done, run the distro and finish the setup process by setting up a user name and password. At this point, you could install OMNeT++, but GUI programs would not work.","title":"Installing an Ubuntu distribution"},{"location":"articles/wsl2/#install-vcxserver","text":"To use GUI programs from Linux, you must install an X Server application from: https://sourceforge.net/projects/vcxsrv/ Start the installation and make sure that you: select \"Disable access control\" set display number to 0 check \"Private networks, such as my home or work network\" and click \"Allow access\" when the Windows Defender Firewall asks for permission. Open the Windows Terminal and launch the Ubuntu distribution from the dropdown menu. Add the following line to the /etc/bash.bashrc or ~/.bashrc file. export DISPLAY=$(grep -m 1 nameserver /etc/resolv.conf | awk '{print $2}'):0.0 This will ensure that Linux programs will always find the X Server process running on Windows. Exit from the Ubuntu shell, and restart it to make sure that the change was applied correctly. Check if $ echo $DISPLAY displays the correct IP address of the Windows machine. In the future, make sure that the X Server is always running when you want to run Linux GUI programs by either making the X Server automatically start or launching it manually. Tip There is ongoing work to make Linux GUI applications work on Windows by default. On later versions of Windows you may be able to skip the whole X Server installation step.","title":"Install VcXserver"},{"location":"articles/wsl2/#install-the-ubuntu-version-of-omnet","text":"At this point, you have a fully functional Linux environment that can run GUI apps. You can go on and follow the Ubuntu specific installation steps to finally install OMNeT++ on your system.","title":"Install the Ubuntu Version of OMNeT++"},{"location":"articles/wsl2/#usage","text":"Make sure the X server is running. Then, open the Windows Terminal and launch the Ubuntu distribution from the dropdown menu. From this point on, you can use OMNeT++ exactly as you would use it on Ubuntu.","title":"Usage"},{"location":"articles/zero-conf-parsim/","text":"Note This is a Proposed Research Topic . Proposed research topics are ideas that we find both very promising as a research topic, and practically very useful . We have already spent some time trying out the idea and proven (at least to ourselves) that it is feasible and the approach outlined here can be made to work, but we don't have the resources (mostly, time) to elaborate it in-house. If you are a researcher (e.g. PhD student) looking for an exciting and rewarding topic to work on, we'd love to hear from you! Abstract \u00b6 OMNeT++ already supports parallel simulation. However, the network must be partitioned manually, and the partitions run concurrently in separate processes communicating over MPI. Here we propose a parallel simulation approach that utilizes shared-memory multiprocessors (relies on multi-threaded execution instead of communicating processes), and requires no prior knowledge or manual configuration about the simulation model. This makes it potentially both more efficient and more convenient to use than the existing solution. Details \u00b6 The suggested research topic involves the following tasks: Change the simulation kernel to support concurrent event execution. Use lock-free data structures (e.g. FES) if possible or add locking if necessary. Eliminate global state and make the kernel API re-entrant. This part is quite difficult to do, because it requires deep knowledge of the simulation kernel. Develop a worker thread based approach for executing events concurrently. This is fairly straightforward to do, because the required techniques are well-known and widely-used. Develop a method for determining which future events can be concurrently executed. In other words, find the events which have no effect on other events in the FES (i.e. out of their light cone). The idea related to the last point is described in further detail here: The module structure of the simulation network can be thought of as a graph where each node corresponds to a module and each edge corresponds to a connection between the two modules. Other cross-module dependencies, such as possible C++ method calls or communication via signals, must also be included as edges. Each node and edge has a delay associated with it. Node delays are 0 by default (could be overridden by module), edge delays are either 0 or they are set to the delay of the corresponding connections. The shortest delay between any two nodes can be determined by analyzing the paths between the two nodes in the graph. The FES contains a set of events or most likely messages and each message belongs to a module. When a module processes a message, the message may have some effect on other future events in other modules, but this is limited by the shortest delay between the two modules in question. The earliest effect time for a message in the FES (i.e. the earliest input time for the receiver module) can be defined as the minimum of the arrival time plus the shortest delay (from the other receiver module to this receiver module) for any other message in the FES. If the earliest effect time for a message is greater than the arrival time, then the message can be executed concurrently. The above condition is conservative in the sense that if the FES is investigated over and over again, then any message that is already found to be concurrently executable remains to be so. In a large simulation, where the FES contains several thousands of events, the number of events that can be concurrently executed can be larger than the number of available CPUs, thus allowing efficient concurrent execution. It is also important to note that individual CPUs can grab several concurrently executable events from the FES at once, and execute them in any order. This approach can further decrease the contention on the FES when the workers concurrently access it. Let's colorize concurrently executable events as green and all other events as red. All events are red by default, but may be colored green later on. Once an event becomes green it, remains green until it is executed. The colorizing algorithm could work as follows: for each red event E1 in module M1 in the FES: for each event E2 in module M2 before E1 in the FES: Minimize arrivalTime(E2) + minimumDelay(M2, M1) as T if arrivalTime(E1) < T: mark E1 as green The above algorithm can be run concurrently with the parallel simulation worker threads, because it is conservative with respect to coloring events. The minimumDelay between two modules can be predetermined during initialization if the network topology is static or refreshed if necessary. This data structure could be quite large. For example, in a simulation containing 1000 nodes there could be 100,000 modules, so the table would contain 10,000,000,000 rows. Luckily most of the rows can be merged. For example, if from two modules M1 and M2 the delay D is the same towards module M3, then the two rows could be represented by one where the source is the set of M1 and M2. This can be done similarly when the source is the same and the destinations are different but the delay is still the same. If a set contains all modules within a compound module (i.e. the complete module hierarchy), then it can be simply represented by the compound module. In an INET simulation, most modules inside a network node are connected with each other via zero-delay connections and C++ method calls. Thus, the above method should naturally lead to a data structure where the connections between network nodes have separate rows and they use the connection delay between the network nodes. Please note that INET allows combining its modules in many different ways, so it is not necessarily so trivial. For example, sub-networks can be represented as extra compound module levels, etc. Another important optimization opportunity is to use the fact that most simulations don't allow terminating an ongoing transmission. So if a connection is used by a transmission, then the next message cannot be sent from the source to the destination earlier than the end of the ongoing transmission. And due to the fact that the transmission time is often orders of magnitude larger than the propagation delay, this could further increase the effectiveness of the above method. Current Status \u00b6 The above approach has been quickly tested with an INET simulation that consists of 4 sub-networks each containing an Ethernet switch and a few communicating hosts, plus some cross-sub-network communication. The result was that the above approach listed 4-6 concurrent messages out of usually 100 in the FES. This number may not seem very high, but it would scale linearly with the size of the FES. That is, in a much larger network with a FES containing several thousands of events, the number of concurrently executable events can be as large as a few hundred. This would perhaps allow using all CPUs of a modern computer with almost linear performance gains.","title":"Zero Configuration Automatic Parallel Simulation"},{"location":"articles/zero-conf-parsim/#abstract","text":"OMNeT++ already supports parallel simulation. However, the network must be partitioned manually, and the partitions run concurrently in separate processes communicating over MPI. Here we propose a parallel simulation approach that utilizes shared-memory multiprocessors (relies on multi-threaded execution instead of communicating processes), and requires no prior knowledge or manual configuration about the simulation model. This makes it potentially both more efficient and more convenient to use than the existing solution.","title":"Abstract"},{"location":"articles/zero-conf-parsim/#details","text":"The suggested research topic involves the following tasks: Change the simulation kernel to support concurrent event execution. Use lock-free data structures (e.g. FES) if possible or add locking if necessary. Eliminate global state and make the kernel API re-entrant. This part is quite difficult to do, because it requires deep knowledge of the simulation kernel. Develop a worker thread based approach for executing events concurrently. This is fairly straightforward to do, because the required techniques are well-known and widely-used. Develop a method for determining which future events can be concurrently executed. In other words, find the events which have no effect on other events in the FES (i.e. out of their light cone). The idea related to the last point is described in further detail here: The module structure of the simulation network can be thought of as a graph where each node corresponds to a module and each edge corresponds to a connection between the two modules. Other cross-module dependencies, such as possible C++ method calls or communication via signals, must also be included as edges. Each node and edge has a delay associated with it. Node delays are 0 by default (could be overridden by module), edge delays are either 0 or they are set to the delay of the corresponding connections. The shortest delay between any two nodes can be determined by analyzing the paths between the two nodes in the graph. The FES contains a set of events or most likely messages and each message belongs to a module. When a module processes a message, the message may have some effect on other future events in other modules, but this is limited by the shortest delay between the two modules in question. The earliest effect time for a message in the FES (i.e. the earliest input time for the receiver module) can be defined as the minimum of the arrival time plus the shortest delay (from the other receiver module to this receiver module) for any other message in the FES. If the earliest effect time for a message is greater than the arrival time, then the message can be executed concurrently. The above condition is conservative in the sense that if the FES is investigated over and over again, then any message that is already found to be concurrently executable remains to be so. In a large simulation, where the FES contains several thousands of events, the number of events that can be concurrently executed can be larger than the number of available CPUs, thus allowing efficient concurrent execution. It is also important to note that individual CPUs can grab several concurrently executable events from the FES at once, and execute them in any order. This approach can further decrease the contention on the FES when the workers concurrently access it. Let's colorize concurrently executable events as green and all other events as red. All events are red by default, but may be colored green later on. Once an event becomes green it, remains green until it is executed. The colorizing algorithm could work as follows: for each red event E1 in module M1 in the FES: for each event E2 in module M2 before E1 in the FES: Minimize arrivalTime(E2) + minimumDelay(M2, M1) as T if arrivalTime(E1) < T: mark E1 as green The above algorithm can be run concurrently with the parallel simulation worker threads, because it is conservative with respect to coloring events. The minimumDelay between two modules can be predetermined during initialization if the network topology is static or refreshed if necessary. This data structure could be quite large. For example, in a simulation containing 1000 nodes there could be 100,000 modules, so the table would contain 10,000,000,000 rows. Luckily most of the rows can be merged. For example, if from two modules M1 and M2 the delay D is the same towards module M3, then the two rows could be represented by one where the source is the set of M1 and M2. This can be done similarly when the source is the same and the destinations are different but the delay is still the same. If a set contains all modules within a compound module (i.e. the complete module hierarchy), then it can be simply represented by the compound module. In an INET simulation, most modules inside a network node are connected with each other via zero-delay connections and C++ method calls. Thus, the above method should naturally lead to a data structure where the connections between network nodes have separate rows and they use the connection delay between the network nodes. Please note that INET allows combining its modules in many different ways, so it is not necessarily so trivial. For example, sub-networks can be represented as extra compound module levels, etc. Another important optimization opportunity is to use the fact that most simulations don't allow terminating an ongoing transmission. So if a connection is used by a transmission, then the next message cannot be sent from the source to the destination earlier than the end of the ongoing transmission. And due to the fact that the transmission time is often orders of magnitude larger than the propagation delay, this could further increase the effectiveness of the above method.","title":"Details"},{"location":"articles/zero-conf-parsim/#current-status","text":"The above approach has been quickly tested with an INET simulation that consists of 4 sub-networks each containing an Ethernet switch and a few communicating hosts, plus some cross-sub-network communication. The result was that the above approach listed 4-6 concurrent messages out of usually 100 in the FES. This number may not seem very high, but it would scale linearly with the size of the FES. That is, in a much larger network with a FES containing several thousands of events, the number of concurrently executable events can be as large as a few hundred. This would perhaps allow using all CPUs of a modern computer with almost linear performance gains.","title":"Current Status"},{"location":"tutorials/","text":"(this page doesn't seem to appear anywhere)","title":"_Tutorials_"},{"location":"tutorials/cloud/","text":"Introduction \u00b6 The goal of the tutorial is to give you an insight on how to start harnessing the power of computing clouds for running simulation campaigns in order to reduce the time it takes for them to complete. We will focus on using Amazon's AWS, but the process can be easily ported to other cloud service providers. Motivation \u00b6 Simulation is a CPU-intensive task. A simulation campaign may easily consist of hundreds or thousands of simulation runs, and can easily exceed the capacity of computing resources usually available to the researcher. Nowadays, CPU power is available in abundance in the cloud for anyone, at very affordable prices. There are numerous cloud computing services (Amazon AWS, Microsoft Azure, DigitalOcean, Google Cloud Platform, etc.). These services, following an easy registration, allow the user to run their own code on a high number of CPUs, at a surprisingly low price. For example, one hour of usage of an 8-core CPU with 32 GiB RAM costs about $0.50 on AWS at the time of writing. There is also a free trial, which grants the user one year of CPU time for free. Simulation campaigns are often trivially parallelizable. Given enough CPUs, the whole campaign may complete in the time it takes for the longest run to finish. In this tutorial, we show you how to harness the power of computing clouds to dramatically speed up your simulation campaigns. The gain will not only save you time, but it may also allow you to expand the scope or increase the depth of your research, and come to new discoveries. Structure of this Tutorial \u00b6 In the first part of this tutorial we will explain the basics of cloud services, get you familiar with the concepts of the most commonly used container technology, Docker, and introduce job queues. In the second part, we present a concrete solution that allows you to upload and execute simulations in AWS. The process should be regarded as a starting point which illustrates the concept, and can serve as a base for future, more sophisticated solutions.","title":"Introduction"},{"location":"tutorials/cloud/#introduction","text":"The goal of the tutorial is to give you an insight on how to start harnessing the power of computing clouds for running simulation campaigns in order to reduce the time it takes for them to complete. We will focus on using Amazon's AWS, but the process can be easily ported to other cloud service providers.","title":"Introduction"},{"location":"tutorials/cloud/#motivation","text":"Simulation is a CPU-intensive task. A simulation campaign may easily consist of hundreds or thousands of simulation runs, and can easily exceed the capacity of computing resources usually available to the researcher. Nowadays, CPU power is available in abundance in the cloud for anyone, at very affordable prices. There are numerous cloud computing services (Amazon AWS, Microsoft Azure, DigitalOcean, Google Cloud Platform, etc.). These services, following an easy registration, allow the user to run their own code on a high number of CPUs, at a surprisingly low price. For example, one hour of usage of an 8-core CPU with 32 GiB RAM costs about $0.50 on AWS at the time of writing. There is also a free trial, which grants the user one year of CPU time for free. Simulation campaigns are often trivially parallelizable. Given enough CPUs, the whole campaign may complete in the time it takes for the longest run to finish. In this tutorial, we show you how to harness the power of computing clouds to dramatically speed up your simulation campaigns. The gain will not only save you time, but it may also allow you to expand the scope or increase the depth of your research, and come to new discoveries.","title":"Motivation"},{"location":"tutorials/cloud/#structure-of-this-tutorial","text":"In the first part of this tutorial we will explain the basics of cloud services, get you familiar with the concepts of the most commonly used container technology, Docker, and introduce job queues. In the second part, we present a concrete solution that allows you to upload and execute simulations in AWS. The process should be regarded as a starting point which illustrates the concept, and can serve as a base for future, more sophisticated solutions.","title":"Structure of this Tutorial"},{"location":"tutorials/cloud/page1/","text":"Concepts \u00b6 Computing Clouds \u00b6 A computing cloud contains a large collection of multi-core computing nodes, with a large amount of memory and storage. Nodes are connected with a high-speed internal network and are attached to the internet also with a high-bandwidth link. Users do not run their programs directly on the operating system of the nodes, but rather over some virtualization technology (Xen, KVM, VMWare). Virtualization isolates cloud users from each other, providing a confined environment to them, where they can't do any harm to others or the cloud. It also allows the cloud operator to freeze and resume the user's programs, transparently migrate them to other nodes, and control their access to resources like memory and CPU. This way the cloud operator can effectively treat the computing cluster as a resource pool. This kind of resource sharing is what allows the cloud services to be provided at a low cost. Some properties of computing clouds: on-demand or reserved allocation billing based on metered usage different configurations are available (CPU speed, memory size, storage type, network speed) guest OS is typically Linux SSH access is available specialized higher level services are often also offered (machine learning, data mining, 3D rendering, etc.) There are numerous cloud computing services; in this tutorial we will use AWS which even offers a free trial period of one year. Docker \u00b6 About Docker \u00b6 Docker is probably the most popular software container platform nowadays. A container is similar in purpose to virtual machines and emulators, but is much more lightweight. The overhead of running a container is much closer to running an ordinary operating system process. In fact, a Docker container is indeed an OS process, isolated from all other processes in a variety of ways -- a different root file system, network stack, resource limit group, etc. Linux kernel features that make Docker possible include chroot, kernel namespaces, and control groups a.k.a. cgroups. A Docker image is extremely portable: it can be run without changes on a variety of systems, including any Linux distribution, macOS, Windows, and many others. It is partly due to this portability that Docker images are extremely convenient to use as a way for packaging up for applications for running in the cloud. A Docker image contains a file system to be mounted as root for the process, additional settings and metadata such as environment variables to be set, and an entry point (the program to be started on container launch.) Images may be are based on each other like layers. Many standard and user-created images are easily accessible from registries like Docker Hub. A container is a running instance of an image, similar to how a process is a running instance of a program. Containers interact with each other and the outside world most commonly through a network connection, a terminal emulator, or sometimes via directories mounted into their file systems as volumes. Running Containers \u00b6 If you have Docker installed, you can try it out right away by typing the following commands into a terminal: $ docker run -ti hello-world hello-world is the name of a Docker image. Docker will first check if the hello-world image is already available on your machine. If not, it will fetch it from Docker Hub first. Then it starts the image as a container. The process in the container will simply print a message, then terminate. A bit more useful one is the ubuntu image, which packages the latest Ubuntu LTS version without the graphical parts: $ docker run -ti ubuntu After the image is downloaded and started, you will be presented with an Ubuntu bash prompt, and you can start issuing commands in the isolation of the container. It is important to note that running the ubuntu image does not involve booting up a complete Linux system inside the container with its own init process and all. Instead, only a plain bash process is started, which will have the illusion (via the chroot 'ed file system, kernel namespaces and other mechanisms) that it is running on a separate Ubuntu system. The -ti flag in the above command tells Docker to attach the container to the current terminal, so we can interact with the bash shell. About Docker Hub \u00b6 Docker Hub is an online service operated by Docker Inc. It is a registry and storage space for Docker images. It can be used for free for accessing and uploading public images. To be able to submit you own image to Docker Hub, you need to create an account, called Docker ID. For this, an e-mail address, a user name, and a password has to be provided, just like with any other online account registration. Images are identified by a hexadecimal ID (hash), and can have a name (or several names) as well. There are three kinds of image names you will see: - ubuntu : If it is a single word, then it refers to an \"official\" image on Docker Hub. - joe/foobar : This is an image submitted by a user on to Docker Hub. - repo1.company.com/foobar : This is an image in a third-party repository. Some image names end with a version like :2.42 , or typically :latest . Note: Docker Hub is now part of Docker Cloud, which offers Swarm management service as well, but we use the old name here to reduce confusion with the general term \"cloud\". Creating Docker Images \u00b6 The primary way of creating Docker images is using a Dockerfile. A Dockerfile is a kind of a recipe. It's a line-oriented text file where each line starts with an instruction, followed by arguments. Lines starting with a hashmark ( # ) are comments, and a backslash ( \\ ) at the end of a line means that the instruction is continued on the next line. When building an image, Docker executes instructions sequentially, creating a new image layer after each one. This way, subsequent builds can be incremental. The image layer after the last instruction is the final image output. Each Dockerfile must start with the FROM instruction, which determines the base image we are starting from. An example Dockerfile looks like this: # starting with Ubuntu 16.04 as base image FROM ubuntu:16.04 ## update the package cache, and install a C compiler RUN apt-get update -y && apt-get install build-essential -y ## set an environment variable ENV LANG C.UTF-8 ## copy a file into the image COPY main.c /opt/ ## change the working directory, for build- and runtime as well WORKDIR /opt/ ## compile the copied source file RUN gcc main.c -o program ## set the newly built program to start in the containers created from this image ENTRYPOINT ./program The build process takes place within a build context. This is usually a directory on the host machine, but it can be a URL as well. The context is where the main.c file is taken from in the above example. To build this image, save the example into a file named Dockerfile into an empty directory, and a C program of your choice (a \"Hello World\" will be fine) next to it, named main.c . Then issue the following command: $ docker build . -t myimage Here the . is the build context, the Dockerfile will be picked up by its name, as this is the default, and the -t myimage will add the myimage tag (an alternative name) to the image we are building. You will see all the instructions being executed. Once it finishes, you can run the new image with: $ docker run -ti myimage To push this image to Docker Hub so it can be reused by anyone, first log in to docker with you Docker ID: $ docker login Then add another tag to this image, substituting joe with your Docker ID: $ docker tag myimage joe/myimage And finally start the upload: $ docker push joe/myimage Now anyone anywhere can run the C program in this image by typing docker run -ti joe/myimage . And logging in to Docker Hub on the web interface, you will also see a new repository for this image has appeared in your account. Alternatives \u00b6 Docker also offers a build service called Docker Cloud, where a Dockerfile hosted in a source repository on GitHub or BitBucket is built on their servers automatically after each commit in the repository. A alternative way for creating images is constructing the desired result by issuing commands manually inside a running container, then saving the final state into an image using docker commit . This is not recommended, as the resulting image is a lot less reproducible. AWS Services \u00b6 AWS (Amazon Web Services) offers many services, the following two are important for us: EC2 and ECS. EC2 (Elastic Compute Cloud) \u00b6 EC2 is a classic cloud service, where one can run virtual machines (called Instances) typically with a Linux installation, and access them via SSH. They can also be set up to accept connections from the Internet. They come in a variety of configurations (Instance Types), each offering different capabilities, like the number of CPU cores and available RAM size. One of them is t2.micro , which is the only one eligible for use in the free trial. It is among the smaller ones, but it is still perfectly suitable for our purposes. Instances can reach each other on internal virtual networks. ECS (EC2 Container Service) \u00b6 ECS is a service that is offered on top of EC2, and allows us to run Docker containers on EC2 instances easily. The Docker images it runs can come from Docker Hub, from repositories hosted in AWS (ECR), or from anywhere else. The EC2 Instances used to run the containers are managed automatically by ECS. ECS Terminology \u00b6 AWS services, including EC2 and ECS, can be configured using command-line tools (AWS CLI), or a more comfortable web interface (AWS Management Console). We will only use the latter in this tutorial, as it requires less setup. On ECS, the managed EC2 Instances are grouped in Clusters. The number of instances can be easily scaled just by adjusting a parameter of the Cluster. There are ways to automate this based on system load using Auto Scaling Groups, but we won't do this to keep things simple. ECS Clusters run Tasks. Each Task consists of one or more Containers (Docker containers). Tasks are launched from Task Definitions, which describe which images will be used, what storage volumes will be attached, which network ports need to be exposed to the network, and so forth. Tasks can be launched and stopped manually, or by using Services. The job of a Service is to start and manage a given number of Tasks. The properties of a Service include: a Task Definition from which it will start its Tasks, the number of Tasks it should keep running, and a placement rule describing how the containers in the Tasks will be assigned to EC2 machines. Again, the number of Tasks in a Service can be scaled easily just by adjusting a parameter. The number of Tasks in a Service can also be set up to scale automatically based on load, but we won't use this feature. Job Queues \u00b6 An OMNeT++ simulation campaign typically consists of more runs than we have CPUs available, so we will need a kind of job queue. Job queues are widely used in applications where a large number of tasks need to be performed, using a bounded number of workers. A job queue automates the starting of jobs on workers and monitors their completions, thereby ensuring the optimal utilization of computing resources. General Operation \u00b6 Jobs are submitted to the queue by some producer, where they are stored together with their parameters. The queue can be a simple FIFO, where jobs are executed on a first-come, first-served basis, or they can be prioritized in a variety of ways. There are one or more workers connected to the job queue to perform the jobs. Workers sit in a loop, and wait for new jobs. When a job becomes available, they pop it off the queue and process it in the way appropriate for the job. Once it is done, the result will either be put back into a different queue for further processing, or simply stored somewhere. Then the worker starts the next job, until there are no more pending jobs left. RQ \u00b6 In this tutorial, we're going use the RQ library to implement our job queue. We chose it because of its lightweightness and emphasis on simplicity and ease of use. RQ is backed by Redis, a high-performance in-memory key-value store. This means that we will need to start a Redis server to operate the job queue, and have the workers connect to it. In RQ, each job is an invocation of an ordinary Python function. The parameters of the job are the function's arguments, and the result is its return value. The code itself is not passed through the queue, so the job function must be present on the worker nodes.","title":"Concepts"},{"location":"tutorials/cloud/page1/#concepts","text":"","title":"Concepts"},{"location":"tutorials/cloud/page1/#computing-clouds","text":"A computing cloud contains a large collection of multi-core computing nodes, with a large amount of memory and storage. Nodes are connected with a high-speed internal network and are attached to the internet also with a high-bandwidth link. Users do not run their programs directly on the operating system of the nodes, but rather over some virtualization technology (Xen, KVM, VMWare). Virtualization isolates cloud users from each other, providing a confined environment to them, where they can't do any harm to others or the cloud. It also allows the cloud operator to freeze and resume the user's programs, transparently migrate them to other nodes, and control their access to resources like memory and CPU. This way the cloud operator can effectively treat the computing cluster as a resource pool. This kind of resource sharing is what allows the cloud services to be provided at a low cost. Some properties of computing clouds: on-demand or reserved allocation billing based on metered usage different configurations are available (CPU speed, memory size, storage type, network speed) guest OS is typically Linux SSH access is available specialized higher level services are often also offered (machine learning, data mining, 3D rendering, etc.) There are numerous cloud computing services; in this tutorial we will use AWS which even offers a free trial period of one year.","title":"Computing Clouds"},{"location":"tutorials/cloud/page1/#docker","text":"","title":"Docker"},{"location":"tutorials/cloud/page1/#about-docker","text":"Docker is probably the most popular software container platform nowadays. A container is similar in purpose to virtual machines and emulators, but is much more lightweight. The overhead of running a container is much closer to running an ordinary operating system process. In fact, a Docker container is indeed an OS process, isolated from all other processes in a variety of ways -- a different root file system, network stack, resource limit group, etc. Linux kernel features that make Docker possible include chroot, kernel namespaces, and control groups a.k.a. cgroups. A Docker image is extremely portable: it can be run without changes on a variety of systems, including any Linux distribution, macOS, Windows, and many others. It is partly due to this portability that Docker images are extremely convenient to use as a way for packaging up for applications for running in the cloud. A Docker image contains a file system to be mounted as root for the process, additional settings and metadata such as environment variables to be set, and an entry point (the program to be started on container launch.) Images may be are based on each other like layers. Many standard and user-created images are easily accessible from registries like Docker Hub. A container is a running instance of an image, similar to how a process is a running instance of a program. Containers interact with each other and the outside world most commonly through a network connection, a terminal emulator, or sometimes via directories mounted into their file systems as volumes.","title":"About Docker"},{"location":"tutorials/cloud/page1/#running-containers","text":"If you have Docker installed, you can try it out right away by typing the following commands into a terminal: $ docker run -ti hello-world hello-world is the name of a Docker image. Docker will first check if the hello-world image is already available on your machine. If not, it will fetch it from Docker Hub first. Then it starts the image as a container. The process in the container will simply print a message, then terminate. A bit more useful one is the ubuntu image, which packages the latest Ubuntu LTS version without the graphical parts: $ docker run -ti ubuntu After the image is downloaded and started, you will be presented with an Ubuntu bash prompt, and you can start issuing commands in the isolation of the container. It is important to note that running the ubuntu image does not involve booting up a complete Linux system inside the container with its own init process and all. Instead, only a plain bash process is started, which will have the illusion (via the chroot 'ed file system, kernel namespaces and other mechanisms) that it is running on a separate Ubuntu system. The -ti flag in the above command tells Docker to attach the container to the current terminal, so we can interact with the bash shell.","title":"Running Containers"},{"location":"tutorials/cloud/page1/#about-docker-hub","text":"Docker Hub is an online service operated by Docker Inc. It is a registry and storage space for Docker images. It can be used for free for accessing and uploading public images. To be able to submit you own image to Docker Hub, you need to create an account, called Docker ID. For this, an e-mail address, a user name, and a password has to be provided, just like with any other online account registration. Images are identified by a hexadecimal ID (hash), and can have a name (or several names) as well. There are three kinds of image names you will see: - ubuntu : If it is a single word, then it refers to an \"official\" image on Docker Hub. - joe/foobar : This is an image submitted by a user on to Docker Hub. - repo1.company.com/foobar : This is an image in a third-party repository. Some image names end with a version like :2.42 , or typically :latest . Note: Docker Hub is now part of Docker Cloud, which offers Swarm management service as well, but we use the old name here to reduce confusion with the general term \"cloud\".","title":"About Docker Hub"},{"location":"tutorials/cloud/page1/#creating-docker-images","text":"The primary way of creating Docker images is using a Dockerfile. A Dockerfile is a kind of a recipe. It's a line-oriented text file where each line starts with an instruction, followed by arguments. Lines starting with a hashmark ( # ) are comments, and a backslash ( \\ ) at the end of a line means that the instruction is continued on the next line. When building an image, Docker executes instructions sequentially, creating a new image layer after each one. This way, subsequent builds can be incremental. The image layer after the last instruction is the final image output. Each Dockerfile must start with the FROM instruction, which determines the base image we are starting from. An example Dockerfile looks like this: # starting with Ubuntu 16.04 as base image FROM ubuntu:16.04 ## update the package cache, and install a C compiler RUN apt-get update -y && apt-get install build-essential -y ## set an environment variable ENV LANG C.UTF-8 ## copy a file into the image COPY main.c /opt/ ## change the working directory, for build- and runtime as well WORKDIR /opt/ ## compile the copied source file RUN gcc main.c -o program ## set the newly built program to start in the containers created from this image ENTRYPOINT ./program The build process takes place within a build context. This is usually a directory on the host machine, but it can be a URL as well. The context is where the main.c file is taken from in the above example. To build this image, save the example into a file named Dockerfile into an empty directory, and a C program of your choice (a \"Hello World\" will be fine) next to it, named main.c . Then issue the following command: $ docker build . -t myimage Here the . is the build context, the Dockerfile will be picked up by its name, as this is the default, and the -t myimage will add the myimage tag (an alternative name) to the image we are building. You will see all the instructions being executed. Once it finishes, you can run the new image with: $ docker run -ti myimage To push this image to Docker Hub so it can be reused by anyone, first log in to docker with you Docker ID: $ docker login Then add another tag to this image, substituting joe with your Docker ID: $ docker tag myimage joe/myimage And finally start the upload: $ docker push joe/myimage Now anyone anywhere can run the C program in this image by typing docker run -ti joe/myimage . And logging in to Docker Hub on the web interface, you will also see a new repository for this image has appeared in your account.","title":"Creating Docker Images"},{"location":"tutorials/cloud/page1/#alternatives","text":"Docker also offers a build service called Docker Cloud, where a Dockerfile hosted in a source repository on GitHub or BitBucket is built on their servers automatically after each commit in the repository. A alternative way for creating images is constructing the desired result by issuing commands manually inside a running container, then saving the final state into an image using docker commit . This is not recommended, as the resulting image is a lot less reproducible.","title":"Alternatives"},{"location":"tutorials/cloud/page1/#aws-services","text":"AWS (Amazon Web Services) offers many services, the following two are important for us: EC2 and ECS.","title":"AWS Services"},{"location":"tutorials/cloud/page1/#ec2-elastic-compute-cloud","text":"EC2 is a classic cloud service, where one can run virtual machines (called Instances) typically with a Linux installation, and access them via SSH. They can also be set up to accept connections from the Internet. They come in a variety of configurations (Instance Types), each offering different capabilities, like the number of CPU cores and available RAM size. One of them is t2.micro , which is the only one eligible for use in the free trial. It is among the smaller ones, but it is still perfectly suitable for our purposes. Instances can reach each other on internal virtual networks.","title":"EC2 (Elastic Compute Cloud)"},{"location":"tutorials/cloud/page1/#ecs-ec2-container-service","text":"ECS is a service that is offered on top of EC2, and allows us to run Docker containers on EC2 instances easily. The Docker images it runs can come from Docker Hub, from repositories hosted in AWS (ECR), or from anywhere else. The EC2 Instances used to run the containers are managed automatically by ECS.","title":"ECS (EC2 Container Service)"},{"location":"tutorials/cloud/page1/#ecs-terminology","text":"AWS services, including EC2 and ECS, can be configured using command-line tools (AWS CLI), or a more comfortable web interface (AWS Management Console). We will only use the latter in this tutorial, as it requires less setup. On ECS, the managed EC2 Instances are grouped in Clusters. The number of instances can be easily scaled just by adjusting a parameter of the Cluster. There are ways to automate this based on system load using Auto Scaling Groups, but we won't do this to keep things simple. ECS Clusters run Tasks. Each Task consists of one or more Containers (Docker containers). Tasks are launched from Task Definitions, which describe which images will be used, what storage volumes will be attached, which network ports need to be exposed to the network, and so forth. Tasks can be launched and stopped manually, or by using Services. The job of a Service is to start and manage a given number of Tasks. The properties of a Service include: a Task Definition from which it will start its Tasks, the number of Tasks it should keep running, and a placement rule describing how the containers in the Tasks will be assigned to EC2 machines. Again, the number of Tasks in a Service can be scaled easily just by adjusting a parameter. The number of Tasks in a Service can also be set up to scale automatically based on load, but we won't use this feature.","title":"ECS Terminology"},{"location":"tutorials/cloud/page1/#job-queues","text":"An OMNeT++ simulation campaign typically consists of more runs than we have CPUs available, so we will need a kind of job queue. Job queues are widely used in applications where a large number of tasks need to be performed, using a bounded number of workers. A job queue automates the starting of jobs on workers and monitors their completions, thereby ensuring the optimal utilization of computing resources.","title":"Job Queues"},{"location":"tutorials/cloud/page1/#general-operation","text":"Jobs are submitted to the queue by some producer, where they are stored together with their parameters. The queue can be a simple FIFO, where jobs are executed on a first-come, first-served basis, or they can be prioritized in a variety of ways. There are one or more workers connected to the job queue to perform the jobs. Workers sit in a loop, and wait for new jobs. When a job becomes available, they pop it off the queue and process it in the way appropriate for the job. Once it is done, the result will either be put back into a different queue for further processing, or simply stored somewhere. Then the worker starts the next job, until there are no more pending jobs left.","title":"General Operation"},{"location":"tutorials/cloud/page1/#rq","text":"In this tutorial, we're going use the RQ library to implement our job queue. We chose it because of its lightweightness and emphasis on simplicity and ease of use. RQ is backed by Redis, a high-performance in-memory key-value store. This means that we will need to start a Redis server to operate the job queue, and have the workers connect to it. In RQ, each job is an invocation of an ordinary Python function. The parameters of the job are the function's arguments, and the result is its return value. The code itself is not passed through the queue, so the job function must be present on the worker nodes.","title":"RQ"},{"location":"tutorials/cloud/page2/","text":"Implementation \u00b6 To put all of this together into a working solution, in the rest of this tutorial, we're going to: Install some necessary software packages on our machine Build a Docker image which includes OMNeT++ and the worker code Deploy the system on AWS Run a simple simulation campaign with it using a custom client Take a look at all the code needed to perform the above Before you begin, create a new empty folder. Later save all linked source files from this tutorial in that. Solution Architecture \u00b6 We will create the following architecture for running simulations on AWS: We want to use Docker images for easy deployment, so we will use an ECS cluster. One container will run a Redis-based job queue, and others will be workers. The workers will need to run simulations, so their image will have OMNeT++ installed in addition to the RQ worker client. After the worker completes a simulation run, the results will be stored in the Redis database. We will provide a custom tool that submits jobs to the job queue from the user's computer and downloads the results after simulation completion. The final architecture will look like this: In the following sections, we will show how to implement this architecture. We will discuss how to create the Docker images, how to configure the cluster, how to implement the worker and the end-user client and so on. Preparation \u00b6 First we need to install a few things on our computer. OMNeT++ \u00b6 Download the archive from the official website . Then follow the Installation Guide . The core version will work fine as well, if you don't need the IDE. Docker \u00b6 Follow the guides on the official website: for Ubuntu or for Fedora . Note that even if your distribution has Docker in its own native package archive, it is most likely an outdated, and you will have to uninstall it first. Python 3 and pip3 \u00b6 We will use Python version 3; and pip, which is the recommended tool for installing packages from the Python Package Index . Install these using the native package manager of your distribution. On Ubuntu: $ sudo apt install python3 python3-pip On Fedora: $ sudo dnf install python3 python3-pip Then, in any case, upgrade pip : $ sudo pip3 install --upgrade pip Feel free to use a virtualenv for this instead of sudo if you're familiar with the concept. RQ \u00b6 Use pip to install the RQ library: $ sudo pip3 install rq This will install the redis client module as well, as a dependency. Getting the Code \u00b6 Download the following files into the directory you just created: utils.py - Contains some common utilities used by both worker.py and client.py . worker.py - Contains the code to be executed by the workers as a job. Dockerfile - Is a recipe to build the Docker image for the workers. client.py - Is the client-side software to submit the simulations. We will take a closer look at their contents in the \"Examining the Code\" chapter. Building the Docker Image \u00b6 Building the image is done by issuing the following command in the directory where the above files can be found: $ docker build . -t worker The Dockerfile is picked up automatically by name, the build context is the current directory ( . ), and the resulting image will be named worker . This will likely take a few minutes to complete. If you see a couple of warnings written in red, but the process continues, don't worry, this is expected. Publishing \u00b6 Now that we built our image for the worker nodes, we need to make it available for our AWS Instances by uploading it to Docker Hub. First authenticate yourself (in case you haven't already) by typing in your Docker ID and password after running this command: $ docker login Now tag the image \"into your repo\", substituting your user name (Docker ID), so Docker knows where to push the image: $ docker tag worker username/worker And finally issue the actual push: $ docker push username/worker This will upload about 400-500 MB of data, so it can take a while. Once it's done, your image is available worldwide. You can see it appeared on Docker Hub . You may even get email notification if it was successful.","title":"Implementation"},{"location":"tutorials/cloud/page2/#implementation","text":"To put all of this together into a working solution, in the rest of this tutorial, we're going to: Install some necessary software packages on our machine Build a Docker image which includes OMNeT++ and the worker code Deploy the system on AWS Run a simple simulation campaign with it using a custom client Take a look at all the code needed to perform the above Before you begin, create a new empty folder. Later save all linked source files from this tutorial in that.","title":"Implementation"},{"location":"tutorials/cloud/page2/#solution-architecture","text":"We will create the following architecture for running simulations on AWS: We want to use Docker images for easy deployment, so we will use an ECS cluster. One container will run a Redis-based job queue, and others will be workers. The workers will need to run simulations, so their image will have OMNeT++ installed in addition to the RQ worker client. After the worker completes a simulation run, the results will be stored in the Redis database. We will provide a custom tool that submits jobs to the job queue from the user's computer and downloads the results after simulation completion. The final architecture will look like this: In the following sections, we will show how to implement this architecture. We will discuss how to create the Docker images, how to configure the cluster, how to implement the worker and the end-user client and so on.","title":"Solution Architecture"},{"location":"tutorials/cloud/page2/#preparation","text":"First we need to install a few things on our computer.","title":"Preparation"},{"location":"tutorials/cloud/page2/#omnet","text":"Download the archive from the official website . Then follow the Installation Guide . The core version will work fine as well, if you don't need the IDE.","title":"OMNeT++"},{"location":"tutorials/cloud/page2/#docker","text":"Follow the guides on the official website: for Ubuntu or for Fedora . Note that even if your distribution has Docker in its own native package archive, it is most likely an outdated, and you will have to uninstall it first.","title":"Docker"},{"location":"tutorials/cloud/page2/#python-3-and-pip3","text":"We will use Python version 3; and pip, which is the recommended tool for installing packages from the Python Package Index . Install these using the native package manager of your distribution. On Ubuntu: $ sudo apt install python3 python3-pip On Fedora: $ sudo dnf install python3 python3-pip Then, in any case, upgrade pip : $ sudo pip3 install --upgrade pip Feel free to use a virtualenv for this instead of sudo if you're familiar with the concept.","title":"Python 3 and pip3"},{"location":"tutorials/cloud/page2/#rq","text":"Use pip to install the RQ library: $ sudo pip3 install rq This will install the redis client module as well, as a dependency.","title":"RQ"},{"location":"tutorials/cloud/page2/#getting-the-code","text":"Download the following files into the directory you just created: utils.py - Contains some common utilities used by both worker.py and client.py . worker.py - Contains the code to be executed by the workers as a job. Dockerfile - Is a recipe to build the Docker image for the workers. client.py - Is the client-side software to submit the simulations. We will take a closer look at their contents in the \"Examining the Code\" chapter.","title":"Getting the Code"},{"location":"tutorials/cloud/page2/#building-the-docker-image","text":"Building the image is done by issuing the following command in the directory where the above files can be found: $ docker build . -t worker The Dockerfile is picked up automatically by name, the build context is the current directory ( . ), and the resulting image will be named worker . This will likely take a few minutes to complete. If you see a couple of warnings written in red, but the process continues, don't worry, this is expected.","title":"Building the Docker Image"},{"location":"tutorials/cloud/page2/#publishing","text":"Now that we built our image for the worker nodes, we need to make it available for our AWS Instances by uploading it to Docker Hub. First authenticate yourself (in case you haven't already) by typing in your Docker ID and password after running this command: $ docker login Now tag the image \"into your repo\", substituting your user name (Docker ID), so Docker knows where to push the image: $ docker tag worker username/worker And finally issue the actual push: $ docker push username/worker This will upload about 400-500 MB of data, so it can take a while. Once it's done, your image is available worldwide. You can see it appeared on Docker Hub . You may even get email notification if it was successful.","title":"Publishing"},{"location":"tutorials/cloud/page4/","text":"Deploying on AWS \u00b6 First you need an AWS account, so if you don't already have one, follow the sign-up procedure . You will need to provide some personal information to Amazon, including full name, address, phone number, and a valid credit card. Then you will need to verify your identity via a phone call, or similar method. Don't worry, the computing power we will use is included in the Free Tier package. If you're eligible for that, and won't leave virtual machines running and forget about them for days, you will only be charged for the network traffic you generate. From experience, the total cost of completing this tutorial is about $0.01 (one cent). This is mostly the price of the data transfer occurring when the Docker image is fetched from Docker Hub. You can keep track of your spending on the \"Billing Dashboard\" page. It's important to choose a Region, preferably the one closest to you geographically. Then make sure you always have that Region selected, because most resources in many AWS services are bound to the Region in which they were created. Creating an ECS Cluster \u00b6 In the AWS Console, select the ECS (EC2 Container Service). If this is your first time here, you will be greeted with a short introductory video. You can watch it if you're interested. Then click the blue \"Get started\" button below. On the next page, you will be presented with a few choices that introduce you to the usage of ECS, but for now, just hit Cancel. You can get back to the greeting video, and these example choices anytime, if you delete all your ECS Clusters. Click \"Create Cluster\". You could name your cluster anything, but let's type in opp-cluster now. If you wish to use the Free Tier, it's important to set the \"EC2 Instance Type\" to t2.micro . Set the \"Number of instances\" option to 4 . Scroll down, and in the Networking section, and the \"Security group inbound rules\" group, type 6379 to the \"Port range\" field. This is the port on which Redis listens for incoming connections, so we must let traffic into our virtual network on this port. No need to change anything else; scroll down and click \"Create\". In a few moments, your cluster will be ready. Creating the Redis Task Definition \u00b6 Once all resources are created, Switch to the \"Task Definitions\" page on the top left. Click \"Create new Task Definition\". Type in redis as Name. Click the blue \"Add container\" button. The Container name can be anything, but let's enter redis here as well, just for the sake of consistency. The image name should be redis , since we will run the official Redis image available on Docker Hub. We will also have to enter a memory limit. Type in 512 . You can refine this later if you run into issues. To make the server accessible from the outside, we need to add a Port mapping as well. Just enter 6379 in the \"Host port\" and \"Container port\" fields, and leave the \"Protocol\" on tcp . There is no need to change any of the dozens of additional options, just click \"Add\" on the bottom right. This Task definition is done, scroll down and click \"Create\". Starting The Redis Task \u00b6 On the details page of the redis Task Definition, select Actions / Run Task. There is only one Cluster, so it should be selected already. We only need a single Redis instance, so leave \"Number of tasks\" at 1 . No need to change any more settings, click \"Run Task\" below. You should see the task appeared in PENDING status. In a few moments, it should change to RUNNING . The table is updated automatically from time to time, but you can refresh it manually as well, with the circular arrow button. Getting the IP of the Redis Server \u00b6 Once the Task is running, click on its ID. Then on the \"Task details\" page, click on the \"EC2 instance id\". On the bottom right corner, take a note of the \"IPv4 Public IP\" of the selected Instance. We will need this later. The EC2 instance page most likely opened in a new browser tab. You can now close that. If it opened in the same tab, navigate back to the ECS console. Creating the Worker Task Definition \u00b6 Create another Task Definition, named worker . Add a container, name it worker , and in its configuration, set the image name to username/worker . Again, enter 512 as \"Memory Limit\". Setting up port mapping for this container is not necessary. Instead, enter this as \"Command\", substituting the IP address of the Redis server you noted in the previous step: -u,redis://172.17.1.19 . Don't forget to click \"Add\", then \"Create\". Creating the Worker Service \u00b6 Switch back to the Clusters page, and select the opp-cluster Cluster. On the Services tab, click \"Create\". Select the worker : 1 Task Definition. Name the Service worker as well. Set the number of tasks to 3 . Click \"Next step\". Leave ELB type on \"None\", and click \"Next step\". Skip auto scaling by clicking \"Next step\" yet again. On the review page, click \"Create Service\". Once it was succesfully created, click \"View Service\". The worker services should appear on the \"Tasks\" tab as PENDING . You can't see the redis task here, as it is not part of this service, but it is still running, and it's listed among the tasks of the cluster. Again, the table is updated automatically, but you can refresh it manually as well. Once all tasks change into RUNNING status, the cluster is ready to use.","title":"Deploying on AWS"},{"location":"tutorials/cloud/page4/#deploying-on-aws","text":"First you need an AWS account, so if you don't already have one, follow the sign-up procedure . You will need to provide some personal information to Amazon, including full name, address, phone number, and a valid credit card. Then you will need to verify your identity via a phone call, or similar method. Don't worry, the computing power we will use is included in the Free Tier package. If you're eligible for that, and won't leave virtual machines running and forget about them for days, you will only be charged for the network traffic you generate. From experience, the total cost of completing this tutorial is about $0.01 (one cent). This is mostly the price of the data transfer occurring when the Docker image is fetched from Docker Hub. You can keep track of your spending on the \"Billing Dashboard\" page. It's important to choose a Region, preferably the one closest to you geographically. Then make sure you always have that Region selected, because most resources in many AWS services are bound to the Region in which they were created.","title":"Deploying on AWS"},{"location":"tutorials/cloud/page4/#creating-an-ecs-cluster","text":"In the AWS Console, select the ECS (EC2 Container Service). If this is your first time here, you will be greeted with a short introductory video. You can watch it if you're interested. Then click the blue \"Get started\" button below. On the next page, you will be presented with a few choices that introduce you to the usage of ECS, but for now, just hit Cancel. You can get back to the greeting video, and these example choices anytime, if you delete all your ECS Clusters. Click \"Create Cluster\". You could name your cluster anything, but let's type in opp-cluster now. If you wish to use the Free Tier, it's important to set the \"EC2 Instance Type\" to t2.micro . Set the \"Number of instances\" option to 4 . Scroll down, and in the Networking section, and the \"Security group inbound rules\" group, type 6379 to the \"Port range\" field. This is the port on which Redis listens for incoming connections, so we must let traffic into our virtual network on this port. No need to change anything else; scroll down and click \"Create\". In a few moments, your cluster will be ready.","title":"Creating an ECS Cluster"},{"location":"tutorials/cloud/page4/#creating-the-redis-task-definition","text":"Once all resources are created, Switch to the \"Task Definitions\" page on the top left. Click \"Create new Task Definition\". Type in redis as Name. Click the blue \"Add container\" button. The Container name can be anything, but let's enter redis here as well, just for the sake of consistency. The image name should be redis , since we will run the official Redis image available on Docker Hub. We will also have to enter a memory limit. Type in 512 . You can refine this later if you run into issues. To make the server accessible from the outside, we need to add a Port mapping as well. Just enter 6379 in the \"Host port\" and \"Container port\" fields, and leave the \"Protocol\" on tcp . There is no need to change any of the dozens of additional options, just click \"Add\" on the bottom right. This Task definition is done, scroll down and click \"Create\".","title":"Creating the Redis Task Definition"},{"location":"tutorials/cloud/page4/#starting-the-redis-task","text":"On the details page of the redis Task Definition, select Actions / Run Task. There is only one Cluster, so it should be selected already. We only need a single Redis instance, so leave \"Number of tasks\" at 1 . No need to change any more settings, click \"Run Task\" below. You should see the task appeared in PENDING status. In a few moments, it should change to RUNNING . The table is updated automatically from time to time, but you can refresh it manually as well, with the circular arrow button.","title":"Starting The Redis Task"},{"location":"tutorials/cloud/page4/#getting-the-ip-of-the-redis-server","text":"Once the Task is running, click on its ID. Then on the \"Task details\" page, click on the \"EC2 instance id\". On the bottom right corner, take a note of the \"IPv4 Public IP\" of the selected Instance. We will need this later. The EC2 instance page most likely opened in a new browser tab. You can now close that. If it opened in the same tab, navigate back to the ECS console.","title":"Getting the IP of the Redis Server"},{"location":"tutorials/cloud/page4/#creating-the-worker-task-definition","text":"Create another Task Definition, named worker . Add a container, name it worker , and in its configuration, set the image name to username/worker . Again, enter 512 as \"Memory Limit\". Setting up port mapping for this container is not necessary. Instead, enter this as \"Command\", substituting the IP address of the Redis server you noted in the previous step: -u,redis://172.17.1.19 . Don't forget to click \"Add\", then \"Create\".","title":"Creating the Worker Task Definition"},{"location":"tutorials/cloud/page4/#creating-the-worker-service","text":"Switch back to the Clusters page, and select the opp-cluster Cluster. On the Services tab, click \"Create\". Select the worker : 1 Task Definition. Name the Service worker as well. Set the number of tasks to 3 . Click \"Next step\". Leave ELB type on \"None\", and click \"Next step\". Skip auto scaling by clicking \"Next step\" yet again. On the review page, click \"Create Service\". Once it was succesfully created, click \"View Service\". The worker services should appear on the \"Tasks\" tab as PENDING . You can't see the redis task here, as it is not part of this service, but it is still running, and it's listed among the tasks of the cluster. Again, the table is updated automatically, but you can refresh it manually as well. Once all tasks change into RUNNING status, the cluster is ready to use.","title":"Creating the Worker Service"},{"location":"tutorials/cloud/page5/","text":"Trying It Out \u00b6 Configuration \u00b6 To try it out, we add a new Configuration to the routing sample that comes with OMNeT++. Append this section to the omnetpp.ini file in the samples/routing directory: [Config MeshExperiment] network = networks.Mesh **.width = ${10..20 step 2} **.height = ${10..20 step 2} **.destAddresses = \"0 2 5 6 9 14 17 18 23 27 29 36 42 52 89 123 150 183 192\" **.sendIaTime = uniform(100us, 10ms) # high traffic **.vector-recording = false sim-time-limit = 10s This configuration tries to make each of its runs execute for a significant amount of time - about a minute each on average - by setting up a large network with a small packet inter-arrival time and a relatively long simulation time limit. It also disables vector recording to keep the size of the result files within the limits of the presented solution. Execution \u00b6 To run it, open a terminal, set up the necessary environment variables using setenv , then change into the directory of the sample: $ cd /path/to/omnetpp/ $ . setenv $ cd samples/routing Then execute the client.py script, substituting the IP address of your Redis server on AWS: $ python3 /path/to/client.py ./routing -c MeshExperiment --redis-host 172 .17.1.19 You should see all runs submitted to the job queue, and after a while, finishing them. The results should appear in the results directory, just like with local execution. Comparison \u00b6 You can compare the performance of the cloud solution with that of your own machine by running the same campaign on both, through the time utility: $ time python3 /path/to/client.py ./routing -c MeshExperiment --redis-host 172 .17.1.19 \u2026 $ time opp_runall ./routing -c MeshExperiment After the output of each command, time will print a summary of the execution times. Locally on my computer, using all 8 of its cores, the times were: real 4m37.610s user 25m4.576s sys 0m0.870s The most interesting from our perspective is the real time, as this shows the amount of wall-clock time it took to run each command. The user time is the net processing time. It is much more than the real time, because it is aggregated across all cores. And of the one running on AWS: real 4m12.310s user 0m1.931s sys 0m1.925s With just 3 single-core workers, the simulation completes faster than running on 8 local cores. The user time is next to negligible in this case, because my computer spent most of the time idling, waiting for the results to come back. Shutting Down \u00b6 To prevent the aimless and useless exhaustion of your Free Tier resource allowance (or in the long term, your credit card balance), it is important to terminate your allocated resources once you are done using them. In our case, these are the EC2 Instances started by the ECS Cluster. We need to shut these down, because when calculating their usage, the whole amount of time the instances are running (or paused, but kept available) is taken into account, not just the time when they were actually used, doing something useful. If you wish to use the Cluster again later, the easiest thing to do is scaling down the number of underlying EC2 Instances to 0 . This will of course cause all running tasks to stop, as they no longer have anything to run on. Alternatively, you can delete the whole Cluster. This will also terminate the EC2 Instances, but it will delete the worker Service as well. The Task Definitions are not deleted either way, but this is not a problem, since they can be kept around indefinitely, free of charge.","title":"Trying It Out"},{"location":"tutorials/cloud/page5/#trying-it-out","text":"","title":"Trying It Out"},{"location":"tutorials/cloud/page5/#configuration","text":"To try it out, we add a new Configuration to the routing sample that comes with OMNeT++. Append this section to the omnetpp.ini file in the samples/routing directory: [Config MeshExperiment] network = networks.Mesh **.width = ${10..20 step 2} **.height = ${10..20 step 2} **.destAddresses = \"0 2 5 6 9 14 17 18 23 27 29 36 42 52 89 123 150 183 192\" **.sendIaTime = uniform(100us, 10ms) # high traffic **.vector-recording = false sim-time-limit = 10s This configuration tries to make each of its runs execute for a significant amount of time - about a minute each on average - by setting up a large network with a small packet inter-arrival time and a relatively long simulation time limit. It also disables vector recording to keep the size of the result files within the limits of the presented solution.","title":"Configuration"},{"location":"tutorials/cloud/page5/#execution","text":"To run it, open a terminal, set up the necessary environment variables using setenv , then change into the directory of the sample: $ cd /path/to/omnetpp/ $ . setenv $ cd samples/routing Then execute the client.py script, substituting the IP address of your Redis server on AWS: $ python3 /path/to/client.py ./routing -c MeshExperiment --redis-host 172 .17.1.19 You should see all runs submitted to the job queue, and after a while, finishing them. The results should appear in the results directory, just like with local execution.","title":"Execution"},{"location":"tutorials/cloud/page5/#comparison","text":"You can compare the performance of the cloud solution with that of your own machine by running the same campaign on both, through the time utility: $ time python3 /path/to/client.py ./routing -c MeshExperiment --redis-host 172 .17.1.19 \u2026 $ time opp_runall ./routing -c MeshExperiment After the output of each command, time will print a summary of the execution times. Locally on my computer, using all 8 of its cores, the times were: real 4m37.610s user 25m4.576s sys 0m0.870s The most interesting from our perspective is the real time, as this shows the amount of wall-clock time it took to run each command. The user time is the net processing time. It is much more than the real time, because it is aggregated across all cores. And of the one running on AWS: real 4m12.310s user 0m1.931s sys 0m1.925s With just 3 single-core workers, the simulation completes faster than running on 8 local cores. The user time is next to negligible in this case, because my computer spent most of the time idling, waiting for the results to come back.","title":"Comparison"},{"location":"tutorials/cloud/page5/#shutting-down","text":"To prevent the aimless and useless exhaustion of your Free Tier resource allowance (or in the long term, your credit card balance), it is important to terminate your allocated resources once you are done using them. In our case, these are the EC2 Instances started by the ECS Cluster. We need to shut these down, because when calculating their usage, the whole amount of time the instances are running (or paused, but kept available) is taken into account, not just the time when they were actually used, doing something useful. If you wish to use the Cluster again later, the easiest thing to do is scaling down the number of underlying EC2 Instances to 0 . This will of course cause all running tasks to stop, as they no longer have anything to run on. Alternatively, you can delete the whole Cluster. This will also terminate the EC2 Instances, but it will delete the worker Service as well. The Task Definitions are not deleted either way, but this is not a problem, since they can be kept around indefinitely, free of charge.","title":"Shutting Down"},{"location":"tutorials/cloud/page6/","text":"Examining the Code \u00b6 This page describes in detail the contents of each used code file. Utilities \u00b6 There are a few things that both the workers and the client need. These are factored out into a common module, utils.py . You can download the entire file from here . In the first half of it, there are the imports it will use, and a QuietBytes helper class. We use this in place of its superclass, the built-in bytes type. It only changes the string representation of the base type, to make it shorter than the actual contents. There is a technical reason for using this: It reduces the amount of data transferred to, and stored in, the Redis database. There are also two functions in it to handle ZIP archives in memory. The first is for extracting, and the second is for compressing, with the option to exclude some directories. Worker code \u00b6 And the code for the jobs, worker.py : With the actual job function: The comments make its operation pretty straightforward. The model needs to be cleaned, then rebuilt inside the container, because the version of some basic system libraries might not match that of those present on the host system, which would lead to incompatibility problems, possibly preventing the simulation from starting. Dockerfile \u00b6 We select the base image to be ubuntu : 16.04 , then we install Python, pip, the dependencies of OMNeT++, and wget. We upgrade pip using itself, then install RQ with it. It will also install the Redis client module as a dependency. Then a few environment variables need to be set, to make RQ use the right character encoding. Next, we copy the worker source code into the image, and set the working directory. Downloading the OMNeT++ 5.1.1 Core release archive from the official website, extracting it, then deleting it. The referer URL has to be passed to wget , otherwise the server denies access. The --progress flag is there just to reduce the amount of textual output, which would overly pollute the build log. The bin directory added to the PATH environment variable (which would be done by setenv normally). Finally the standard building procedure is performed by running ./configure and make . Both graphical runtime environments and the support for 3D rendering are disabled. The -j $(nproc) arguments to make enable it to use all your local CPU cores when building OMNeT++ itself. Installing ccache to make subsequent builds of the same model sources faster: And finally setting up the entry point to launch the rq worker, asking it to keep the results only for one minute. This will be enough, because the client will start downloading them right away, and it will reduce the amount of data stored in the Redis database on average. Later, when we run containers from the image, we will be able to append additional arguments to the entrypoint. Client Software \u00b6 The following file, client.py , implements the command-line application for submitting jobs and getting the results. First, the usual imports: Then a helper function to resolve the run filter in a given configuration by invoking the opp_run tool locally, in \"query\" mode. This is not strictly necessary, since using a run filter is optional, but it's a nice addition. Defining the arguments of the tool and parsing their values: Setting up the connection to the job queue, then using the helper function to get the actual list of run numbers. Finally pack the model source into a compressed archive (ZIP) in memory. A few directories are excluded from this archive, because they are not needed by the workers, and are usually very large. Submitting a job into the queue for each run, storing the jobs in a list. The run number is also written into the meta field of each job, but that is necessary only so we know later which run was performed by a particular job, and we can print it when it is done. The job function itself doesn't use this, only its parameters. And finally poll for the results of each job, downloading and unpacking the output (the results) of completed jobs, and removing them from the list: And exit when all jobs are completed.","title":"Examining the Code"},{"location":"tutorials/cloud/page6/#examining-the-code","text":"This page describes in detail the contents of each used code file.","title":"Examining the Code"},{"location":"tutorials/cloud/page6/#utilities","text":"There are a few things that both the workers and the client need. These are factored out into a common module, utils.py . You can download the entire file from here . In the first half of it, there are the imports it will use, and a QuietBytes helper class. We use this in place of its superclass, the built-in bytes type. It only changes the string representation of the base type, to make it shorter than the actual contents. There is a technical reason for using this: It reduces the amount of data transferred to, and stored in, the Redis database. There are also two functions in it to handle ZIP archives in memory. The first is for extracting, and the second is for compressing, with the option to exclude some directories.","title":"Utilities"},{"location":"tutorials/cloud/page6/#worker-code","text":"And the code for the jobs, worker.py : With the actual job function: The comments make its operation pretty straightforward. The model needs to be cleaned, then rebuilt inside the container, because the version of some basic system libraries might not match that of those present on the host system, which would lead to incompatibility problems, possibly preventing the simulation from starting.","title":"Worker code"},{"location":"tutorials/cloud/page6/#dockerfile","text":"We select the base image to be ubuntu : 16.04 , then we install Python, pip, the dependencies of OMNeT++, and wget. We upgrade pip using itself, then install RQ with it. It will also install the Redis client module as a dependency. Then a few environment variables need to be set, to make RQ use the right character encoding. Next, we copy the worker source code into the image, and set the working directory. Downloading the OMNeT++ 5.1.1 Core release archive from the official website, extracting it, then deleting it. The referer URL has to be passed to wget , otherwise the server denies access. The --progress flag is there just to reduce the amount of textual output, which would overly pollute the build log. The bin directory added to the PATH environment variable (which would be done by setenv normally). Finally the standard building procedure is performed by running ./configure and make . Both graphical runtime environments and the support for 3D rendering are disabled. The -j $(nproc) arguments to make enable it to use all your local CPU cores when building OMNeT++ itself. Installing ccache to make subsequent builds of the same model sources faster: And finally setting up the entry point to launch the rq worker, asking it to keep the results only for one minute. This will be enough, because the client will start downloading them right away, and it will reduce the amount of data stored in the Redis database on average. Later, when we run containers from the image, we will be able to append additional arguments to the entrypoint.","title":"Dockerfile"},{"location":"tutorials/cloud/page6/#client-software","text":"The following file, client.py , implements the command-line application for submitting jobs and getting the results. First, the usual imports: Then a helper function to resolve the run filter in a given configuration by invoking the opp_run tool locally, in \"query\" mode. This is not strictly necessary, since using a run filter is optional, but it's a nice addition. Defining the arguments of the tool and parsing their values: Setting up the connection to the job queue, then using the helper function to get the actual list of run numbers. Finally pack the model source into a compressed archive (ZIP) in memory. A few directories are excluded from this archive, because they are not needed by the workers, and are usually very large. Submitting a job into the queue for each run, storing the jobs in a list. The run number is also written into the meta field of each job, but that is necessary only so we know later which run was performed by a particular job, and we can print it when it is done. The job function itself doesn't use this, only its parameters. And finally poll for the results of each job, downloading and unpacking the output (the results) of completed jobs, and removing them from the list: And exit when all jobs are completed.","title":"Client Software"},{"location":"tutorials/cloud/page7/","text":"Potential Improvements, Alternatives \u00b6 Keep in mind that the main point of this tutorial was to present a very simple solution. This means that there are some significant compromises we had to make. In this last section, we discuss a few of them, as well some alternatives to parts of the system. Limitations \u00b6 Some deficiencies of the current solution, with suggestions on how to alleviate them: It only works well with simple models (like the ones in the samples folder). It has no concept of any project features or referenced projects. Advanced use cases, for example ones involving multiple processes (like with Veins), are not supported. Some custom adjustments to the worker function are necessary to make these kinds of simulations possible. Model sources are always distributed as a whole. This is not well suited for quick iteration when experimenting with the code or with parameter values, since we can't take advantage of incremental building. This also generates more network traffic, which means longer startup times with large models. If the model is already in an online repository (GitHub or similar), the workers could be set up to pull a specific revision from there before each run. To avoid having to push the code in that repository after each change, a local git server can be started as well. To avoid even having to commit the changes into git before each iteration, the code can be synchronized to the workers using something like rsync , or shared with them via a network file system, for example sshfs or nfs . The model is built before every run. Even with ccache , every model is built from scratch in every worker container at least once, and the linking phase still happens before every run. The ideal solution would be building just once, either in a different kind of job on one of the workers, or on the local machine. Then distributing the built model among the workers, where they are cached locally, and shared among containers running on the same host (using a common volume), so they only pass through the network as many times as absolutely necessary. The client script might not be the most convenient to use. It could be useful to extend it with some more options, or even integrate it into the IDE. There is no error handling or logging to speak of, the robustness is questionable, and we paid no attention to security at all. These are relatively significant omissions. On multi-core worker machines, multiple worker containers need to be started to take full advantage of their capabilities, since currently a worker only performs one run at a time. This can be a good or a bad thing, depending on your needs. This way, there is more control over how much resources the system is allowed to use, but makes the overall picture a bit unwieldy. The way the model is passed to the jobs and the results are retrieved is not optimal. All data in both directions is stored in the Redis server operating the job queue. Since Redis is an in-memory database, this places a limit on the overall scalability of the solution, mostly on the size of the results. This is why vector recording and event logging is recommended to be turned off for now, at least for large simulations. A good solution for this would be using a dedicated storage space, accessible both by the client and the workers. On AWS, S3 (Simple Storage Service) is a promising candidate. Other cloud providers also have similar data storage services. Additionally, any of the options noted above for sharing the code while iterating can be used for result retrieval as well. The progress and console output of the runs are currently not reported at all, not while they are under execution, nor afterward. Real-time monitoring would be useful, and it can be implemented probably the most easily through the already available Redis server. Alternatives \u00b6 Many parts of the presented architecture can be swapped out for alternatives. A few examples: Instead of Docker Hub, the image for the workers could also be provided via AWS ECR (EC2 Container Registry) - or a similar service on other providers. This would likely improve privacy, and lessen out-of-cloud network traffic when the image needs to be fetched, also potentially improving container startup times. This tutorial is supposed to be adaptable to any other cloud provider: Azure, Google Cloud Platform, DigitalOcean, etc. The Docker image could be built automatically on Docker Cloud if the Dockerfile was hosted in a GitHub or BitBucket repository. While this is often useful, its advantages are questionable in this exact situation. Instead of RQ, Celery could also be used as job queue. AWS has a specialized service for job queuing, called Batch. Azure also offers a similar service with the same name. We could also have used AWS Batch for scheduling instead of running our own job queue. We chose not to use it to facilitate porting of the solution to other cloud providers. Docker Cloud can also be used to deploy and manage a swarm on AWS or Azure instead of their own container services. If the worker function itself needs to be adjusted often, the image needs to be rebuilt, and the containers need to be restarted each time. This can be avoided by synchronizing the script to the workers using the same methods as described above for model code distribution. The rq worker process would still need to be restarted when the script changed, so it is reloaded.","title":"Potential Improvements, Alternatives"},{"location":"tutorials/cloud/page7/#potential-improvements-alternatives","text":"Keep in mind that the main point of this tutorial was to present a very simple solution. This means that there are some significant compromises we had to make. In this last section, we discuss a few of them, as well some alternatives to parts of the system.","title":"Potential Improvements, Alternatives"},{"location":"tutorials/cloud/page7/#limitations","text":"Some deficiencies of the current solution, with suggestions on how to alleviate them: It only works well with simple models (like the ones in the samples folder). It has no concept of any project features or referenced projects. Advanced use cases, for example ones involving multiple processes (like with Veins), are not supported. Some custom adjustments to the worker function are necessary to make these kinds of simulations possible. Model sources are always distributed as a whole. This is not well suited for quick iteration when experimenting with the code or with parameter values, since we can't take advantage of incremental building. This also generates more network traffic, which means longer startup times with large models. If the model is already in an online repository (GitHub or similar), the workers could be set up to pull a specific revision from there before each run. To avoid having to push the code in that repository after each change, a local git server can be started as well. To avoid even having to commit the changes into git before each iteration, the code can be synchronized to the workers using something like rsync , or shared with them via a network file system, for example sshfs or nfs . The model is built before every run. Even with ccache , every model is built from scratch in every worker container at least once, and the linking phase still happens before every run. The ideal solution would be building just once, either in a different kind of job on one of the workers, or on the local machine. Then distributing the built model among the workers, where they are cached locally, and shared among containers running on the same host (using a common volume), so they only pass through the network as many times as absolutely necessary. The client script might not be the most convenient to use. It could be useful to extend it with some more options, or even integrate it into the IDE. There is no error handling or logging to speak of, the robustness is questionable, and we paid no attention to security at all. These are relatively significant omissions. On multi-core worker machines, multiple worker containers need to be started to take full advantage of their capabilities, since currently a worker only performs one run at a time. This can be a good or a bad thing, depending on your needs. This way, there is more control over how much resources the system is allowed to use, but makes the overall picture a bit unwieldy. The way the model is passed to the jobs and the results are retrieved is not optimal. All data in both directions is stored in the Redis server operating the job queue. Since Redis is an in-memory database, this places a limit on the overall scalability of the solution, mostly on the size of the results. This is why vector recording and event logging is recommended to be turned off for now, at least for large simulations. A good solution for this would be using a dedicated storage space, accessible both by the client and the workers. On AWS, S3 (Simple Storage Service) is a promising candidate. Other cloud providers also have similar data storage services. Additionally, any of the options noted above for sharing the code while iterating can be used for result retrieval as well. The progress and console output of the runs are currently not reported at all, not while they are under execution, nor afterward. Real-time monitoring would be useful, and it can be implemented probably the most easily through the already available Redis server.","title":"Limitations"},{"location":"tutorials/cloud/page7/#alternatives","text":"Many parts of the presented architecture can be swapped out for alternatives. A few examples: Instead of Docker Hub, the image for the workers could also be provided via AWS ECR (EC2 Container Registry) - or a similar service on other providers. This would likely improve privacy, and lessen out-of-cloud network traffic when the image needs to be fetched, also potentially improving container startup times. This tutorial is supposed to be adaptable to any other cloud provider: Azure, Google Cloud Platform, DigitalOcean, etc. The Docker image could be built automatically on Docker Cloud if the Dockerfile was hosted in a GitHub or BitBucket repository. While this is often useful, its advantages are questionable in this exact situation. Instead of RQ, Celery could also be used as job queue. AWS has a specialized service for job queuing, called Batch. Azure also offers a similar service with the same name. We could also have used AWS Batch for scheduling instead of running our own job queue. We chose not to use it to facilitate porting of the solution to other cloud providers. Docker Cloud can also be used to deploy and manage a swarm on AWS or Azure instead of their own container services. If the worker function itself needs to be adjusted often, the image needs to be rebuilt, and the containers need to be restarted each time. This can be avoided by synchronizing the script to the workers using the same methods as described above for model code distribution. The rq worker process would still need to be restarted when the script changed, so it is reloaded.","title":"Alternatives"},{"location":"tutorials/pandas/","text":"Attention This tutorial is obsolete - it was written before OMNeT++ 6. In that version, the result analysis toolset was completely overhauled, already relying heavily on Python, NumPy, Pandas, and Matplotlib. Both graphical and command-line tools, as well as libraries usable from standalone Python scripts are available now, which are preferred over the methods described below. 1. When to use Python? \u00b6 The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of simulation results. If you are doing sophisticated result analysis, you will notice after a while that you have outgrown the IDE. The need for customized charts, the necessity of multi-step computations to produce chart input, or the sheer volume of raw simulation results might all be causes to make you look for something else. If you are an R or Matlab expert, you'll probably reach for those tools, but for everyone else, Python with the right libraries is pretty much the best choice. Python has a big momentum for data science, and in addition to having excellent libraries for data analysis and visualization, it is also a great general-purpose programming language. Python is used for diverse problems ranging from building desktop GUIs to machine learning and AI, so the knowledge you gain by learning it will be convertible to other areas. This tutorial will walk you through the initial steps of using Python for analysing simulation results, and shows how to do some of the most common tasks. The tutorial assumes that you have a working knowledge of OMNeT++ with regard to result recording, and basic familiarity with Python. 2. Setting up \u00b6 Before we can start, you need to install the necessary software. First, make sure you have Python, either version 2.x or 3.x (they are slightly incompatible.) If you have both versions available on your system, we recommend version 3.x. You also need OMNeT++ version 5.2 or later. We will heavily rely on three Python packages: NumPy , Pandas , and Matplotlib . There are also optional packages that will be useful for certain tasks: SciPy , PivotTable.js . We also recommend that you install IPython and Jupyter , because they let you work much more comfortably than the bare Python shell. On most systems, these packages can be installed with pip , the Python package manager (if you go for Python 3, replace pip with pip3 in the commands below): sudo pip install ipython jupyter sudo pip install numpy pandas matplotlib sudo pip install scipy pivottablejs As packages continually evolve, there might be incompatibilities between versions. We used the following versions when writing this tutorial: Pandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0. An easy way to determine which versions you have installed is using the pip list command. (Note that the last one is the version of the Python interface library, the PivotTable.js main Javascript library uses different version numbers, e.g. 2.7.0.) 3. Getting your simulation results into Python \u00b6 OMNeT++ result files have their own file format which is not directly digestible by Python. There are a number of ways to get your data inside Python: Export from the IDE. The Analysis Tool can export data in a number of formats, the ones that are useful here are CSV and Python-flavoured JSON. In this tutorial we'll use the CSV export, and read the result into Pandas using its read_csv() function. Export using scavetool. Exporting from the IDE may become tedious after a while, because you have to go through the GUI every time your simulations are re-run. Luckily, you can automate the exporting with OMNeT++'s scavetool program. scavetool exposes the same export functionality as the IDE, and also allows filtering of the data. Read the OMNeT++ result files directly from Python. Development of a Python package to read these files into Pandas data frames is underway, but given that these files are line-oriented text files with a straightforward and well-documented structure, writing your own custom reader is also a perfectly feasible option. SQLite. Since version 5.1, OMNeT++ has the ability to record simulation results int SQLite3 database files, which can be opened directly from Python using the sqlite package. This lets you use SQL queries to select the input data for your charts or computations, which is kind of cool! You can even use GUIs like SQLiteBrowser to browse the database and craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3 output, you'll still get .vec and .sca files as before, only their format will change from textual to SQLite's binary format. When querying the contents of the files, one issue to deal with is that SQLite does not allow cross-database queries, so you either need to configure OMNeT++ to record everything into one file (i.e. each run should append instead of creating a new file), or use scavetool's export functionality to merge the files into one. Custom result recording. There is also the option to instrument the simulation (via C++ code) or OMNeT++ (via custom result recorders) to produce files that Python can directly digest, e.g. CSV. However, in the light of the above options, it is rarely necessary to go this far. With large-scale simulation studies, it can easily happen that the full set of simulation results do not fit into the memory at once. There are also multiple approaches to deal with this problem: If you don't need all simulation results for the analysis, you can configure OMNeT++ to record only a subset of them. Fine-grained control is available. Perform filtering and aggregation steps before analysis. The IDE and scavetool are both capable of filtering the results before export. When the above approaches are not enough, it can help to move part of the result processing (typically, filtering and aggregation) into the simulation model as dedicated result collection modules. However, this solution requires significantly more work than the previous two, so use with care. In this tutorial, we'll work with the contents of the samples/resultfiles directory distributed with OMNeT++. The directory contains result files produced by the Aloha and Routing sample simulations, both of which are parameter studies. We'll start by looking at the Aloha results. As the first step, we use OMNeT++'s scavetool to convert Aloha's scalar files to CSV. Run the following commands in the terminal (replace ~/omnetpp with the location of your OMNeT++ installation): cd ~/omnetpp/samples/resultfiles/aloha scavetool x *.sca -o aloha.csv In the scavetool command line, x means export, and the export format is inferred from the output file's extension. (Note that scavetool supports two different CSV output formats. We need CSV Records , or CSV-R for short, which is the default for the .csv extension.) Let us spend a minute on what the export has created. The CSV file has a fixed number of columns named run , type , module , name , value , etc. Each result item, i.e. scalar, statistic, histogram and vector, produces one row of output in the CSV. Other items such as run attributes, iteration variables of the parameter study and result attributes also generate their own rows. The content of the type column determines what type of information a given row contains. The type column also determines which other columns are in use. For example, the binedges and binvalues columns are only filled in for histogram items. The colums are: run : Identifies the simulation run type : Row type, one of the following: scalar , vector , statistics , histogram , runattr , itervar , param , attr module : Hierarchical name (a.k.a. full path) of the module that recorded the result item name : Name of the result item (scalar, statistic, histogram or vector) attrname : Name of the run attribute or result item attribute (in the latter case, the module and name columns identify the result item the attribute belongs to) attrvalue : Value of run and result item attributes, iteration variables, saved ini param settings ( runattr , attr , itervar , param ) value : Output scalar value count , sumweights , mean , min , max , stddev : Fields of the statistics or histogram binedges , binvalues : Histogram bin edges and bin values, as space-separated lists. len(binedges)==len(binvalues)+1 vectime , vecvalue : Output vector time and value arrays, as space-separated lists When the export is done, you can start Jupyter server with the following command: jupyter notebook Open a web browser with the displayed URL to access the Jupyter GUI. Once there, choose New -> Python3 in the top right corner to open a blank notebook. The notebook allows you to enter Python commands or sequences of commands, run them, and view the output. Note that Enter simply inserts a newline; hit Ctrl+Enter to execute the commands in the current cell, or Alt+Enter to execute them and also insert a new cell below. If you cannot use Jupyter for some reason, a terminal-based Python shell ( python or ipython ) will also allow you to follow the tutorial. On the Python prompt, enter the following lines to make the functionality of Pandas, NumpPy and Matplotlib available in the session. The last, %matplotlib line is only needed for Jupyter. (It is a \"magic command\" that arranges plots to be displayed within the notebook.) In[1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline We utilize the read_csv() function to import the contents of the CSV file into a data frame. The data frame is the central concept of Pandas. We will continue to work with this data frame throughout the whole tutorial. In[2]: aloha = pd . read_csv ( 'aloha.csv' ) 4. Exploring the data frame \u00b6 You can view the contents of the data frame by simply entering the name of the variable ( aloha ). Alternatively, you can use the head() method of the data frame to view just the first few lines. In[3]: aloha . head () Out[3]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN You can see that the structure of the data frame, i.e. rows and columns, directly corresponds to the contents of the CSV file. Column names have been taken from the first line of the CSV file. Missing values are represented with NaNs (not-a-number). The complementary tail() method shows the last few lines. There is also an iloc method that we use at places in this tutorial to show rows from the middle of the data frame. It accepts a range: aloha.iloc[20:30] selects 10 lines from line 20, aloha.iloc[:5] is like head() , and aloha.iloc[-5:] is like tail() . In[4]: aloha . iloc [ 1200 : 1205 ] Out[4]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 1200 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server collidedFrames:last NaN NaN 40692.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1201 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server collidedFrames:last source sum(collision) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1202 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server collidedFrames:last title collided frames, last NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1203 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156176 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1204 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server channelUtilization:last interpolationmode linear NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Hint: If you are in the terminal and you find that the data frame printout does not make use of the whole width of the terminal, you can increase the display width for better readability with the following commands: In[5]: pd . set_option ( 'display.width' , 180 ) pd . set_option ( 'display.max_colwidth' , 100 ) If you have not looked at any Pandas tutorial yet, now is a very good time to read one. (See References at the bottom of this page for hints.) Until you finish, here are some basics for your short-term survival. You can refer to a column as a whole with the array index syntax: aloha['run'] . Alternatively, the more convenient member access syntax ( aloha.run ) can also be used, with restrictions. (E.g. the column name must be valid as a Python identifier, and should not collide with existing methods of the data frame. Names that are known to cause trouble include name , min , max , mean ). In[6]: aloha . run . head () # .head() is for limiting the output to 5 lines here Out[6]: 0 PureAlohaExperiment-4-20170627-20:42:20-22739 1 PureAlohaExperiment-4-20170627-20:42:20-22739 2 PureAlohaExperiment-4-20170627-20:42:20-22739 3 PureAlohaExperiment-4-20170627-20:42:20-22739 4 PureAlohaExperiment-4-20170627-20:42:20-22739 Name: run, dtype: object Selecting multiple columns is also possible, one just needs to use a list of column names as index. The result will be another data frame. (The double brackets in the command are due to the fact that both the array indexing and the list syntax use square brackets.) In[7]: tmp = aloha [[ 'run' , 'attrname' , 'attrvalue' ]] tmp . head () Out[7]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run attrname attrvalue 0 PureAlohaExperiment-4-20170627-20:42:20-22739 configname PureAlohaExperiment 1 PureAlohaExperiment-4-20170627-20:42:20-22739 datetime 20170627-20:42:20 2 PureAlohaExperiment-4-20170627-20:42:20-22739 experiment PureAlohaExperiment 3 PureAlohaExperiment-4-20170627-20:42:20-22739 inifile omnetpp.ini 4 PureAlohaExperiment-4-20170627-20:42:20-22739 iterationvars numHosts=10, iaMean=3 The describe() method can be used to get an idea about the contents of a column. When applied to a non-numeric column, it prints the number of non-null elements in it ( count ), the number of unique values ( unique ), the most frequently occurring value ( top ) and its multiplicity ( freq ), and the inferred data type (more about that later.) In[8]: aloha . module . describe () Out[8]: count 1012 unique 11 top Aloha.server freq 932 Name: module, dtype: object You can get a list of the unique values using the unique() method. For example, the following command lists the names of modules that have recorded any statistics: In[9]: aloha . module . unique () Out[9]: array([nan, 'Aloha.server', 'Aloha.host[0]', 'Aloha.host[1]', 'Aloha.host[2]', 'Aloha.host[3]', 'Aloha.host[4]', 'Aloha.host[5]', 'Aloha.host[6]', 'Aloha.host[7]', 'Aloha.host[8]', 'Aloha.host[9]'], dtype=object) When you apply describe() to a numeric column, you get a statistical summary with things like mean, standard deviation, minimum, maximum, and various quantiles. In[10]: aloha . value . describe () Out[10]: count 294.000000 mean 4900.038749 std 11284.077075 min 0.045582 25% 0.192537 50% 668.925298 75% 5400.000000 max 95630.000000 Name: value, dtype: float64 Applying describe() to the whole data frame creates a similar report about all numeric columns. In[11]: aloha . describe () Out[11]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value count sumweights mean stddev min max count 294.000000 84.000000 0.0 84.000000 84.000000 84.000000 84.000000 mean 4900.038749 5591.380952 NaN 1.489369 0.599396 1.049606 6.560987 std 11284.077075 4528.796760 NaN 1.530455 0.962515 0.956102 9.774404 min 0.045582 470.000000 NaN 0.152142 0.031326 0.099167 0.272013 25% 0.192537 1803.000000 NaN 0.164796 0.049552 0.099186 0.498441 50% 668.925298 4065.500000 NaN 1.197140 0.243035 1.049776 3.084077 75% 5400.000000 8815.000000 NaN 2.384397 0.741081 2.000000 9.000000 max 95630.000000 14769.000000 NaN 6.936747 5.323887 2.000000 54.000000 Let's spend a minute on data types and column data types. Every column has a data type (abbreviated dtype ) that determines what type of values it may contain. Column dtypes can be printed with dtypes : In[12]: aloha . dtypes Out[12]: run object type object module object name object attrname object attrvalue object value float64 count float64 sumweights float64 mean float64 stddev float64 min float64 max float64 binedges object binvalues object vectime object vecvalue object dtype: object The two most commonly used dtypes are float64 and object . A float64 column contains floating-point numbers, and missing values are represented with NaNs. An object column may contain basically anything -- usually strings, but we'll also have NumPy arrays ( np.ndarray ) as elements in this tutorial. Numeric values and booleans may also occur in an object column. Missing values in an object column are usually represented with None , but Pandas also interprets the floating-point NaN like that. Some degree of confusion arises from fact that some Pandas functions check the column's dtype, while others are already happy if the contained elements are of the required type. To clarify: applying describe() to a column prints a type inferred from the individual elements, not the column dtype. The column dtype type can be changed with the astype() method; we'll see an example for using it later in this tutorial. The column dtype can be accessed as the dtype property of a column, for example aloha.stddev.dtype yields dtype('float64') . There are also convenience functions such as is_numeric_dtype() and is_string_dtype() for checking column dtype. (They need to be imported from the pandas.api.types package though.) Another vital thing to know, especially due of the existence of the type column in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly, the array index syntax can be used here as well. For example, the following expression selects the rows that contain iteration variables: aloha[aloha.type == 'itervar'] . With a healthy degree of sloppiness, here's how it works: aloha.type yields the values in the type column as an array-like data structure; aloha.type=='itervar' performs element-wise comparison and produces an array of booleans containing True where the condition holds and False where not; and indexing a data frame with an array of booleans returns the rows that correspond to True values in the array. Conditions can be combined with AND/OR using the \" & \" and \" | \" operators, but you need parentheses because of operator precedence. The following command selects the rows that contain scalars with a certain name and owner module: In[13]: tmp = aloha [( aloha . type == 'scalar' ) & ( aloha . module == 'Aloha.server' ) & ( aloha . name == 'channelUtilization:last' )] tmp . head () Out[13]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 1186 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156057 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1203 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156176 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1220 PureAlohaExperiment-2-20170627-20:42:19-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.196381 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1237 PureAlohaExperiment-3-20170627-20:42:20-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.193253 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1254 PureAlohaExperiment-4-20170627-20:42:20-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.176507 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN You'll also need to know how to add a new column to the data frame. Now that is a bit controversial topic, because at the time of writing, there is a \"convenient\" syntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple assignment, for example: In[14]: aloha [ 'qname' ] = aloha . module + \".\" + aloha . name aloha [ aloha . type == 'scalar' ] . head () # print excerpt Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue qname 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server duration NaN NaN 5400.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.duration 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:mean NaN NaN 0.198275 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:mean 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:sum NaN NaN 2457.026781 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:sum 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:max NaN NaN 0.901897 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:max 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collidedFrames:last NaN NaN 40805.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collidedFrames:last It looks nice and natural, but it is not entirely correct. It often results in a warning: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame... . The message essentially says that the operation (here, adding the new column) might have been applied to a temporary object instead of the original data frame, and thus might have been ineffective. Luckily, that is not the case most of the time (the operation does take effect). Nevertheless, for production code, i.e. scripts, the \"official\" solution, the assign() method of the data frame is recommended, like this: In[15]: aloha = aloha . assign ( qname = aloha . module + \".\" + aloha . name ) aloha [ aloha . type == 'scalar' ] . head () Out[15]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue qname 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server duration NaN NaN 5400.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.duration 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:mean NaN NaN 0.198275 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:mean 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:sum NaN NaN 2457.026781 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:sum 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:max NaN NaN 0.901897 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:max 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collidedFrames:last NaN NaN 40805.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collidedFrames:last For completeness, one can remove a column from a data frame using either the del operator or the drop() method of the data frame. Here we show the former (also to remove the column we added above, as we won't need it for now): In[16]: del aloha [ 'qname' ] 5. Revisiting CSV loading \u00b6 The way we have read the CSV file has one small deficiency: all data in the attrvalue column are represented as strings, event though many of them are really numbers, for example the values of the iaMean and numHosts iteration variables. You can verify that by printing the unique values ( aloha.attrvalue.unique() -- it will print all values with quotes), or using the type() operator on an element: In[17]: type ( aloha [ aloha . type == 'scalar' ] . iloc [ 0 ] . value ) Out[17]: numpy.float64 The reason is that read_csv() infers data types of columns from the data it finds in them. Since the attrvalue column is shared by run attributes, result item attributes, iteration variables and some other types of rows, there are many non-numeric strings in it, and read_csv() decides that it is a string column. A similar issue arises with the binedges , binvalues , vectime , vecvalue columns. These columns contain lists of numbers separated by spaces, so they are read into strings as well. However, we would like to store them as NumPy arrays ( ndarray ) inside the data frame, because that's the form we can use in plots or as computation input. Luckily, read_csv() allows us to specify conversion functions for each column. So, armed with the following two short functions: In[18]: def parse_if_number ( s ): try : return float ( s ) except : return True if s == \"true\" else False if s == \"false\" else s if s else None def parse_ndarray ( s ): return np . fromstring ( s , sep = ' ' ) if s else None we can read the CSV file again, this time with the correct conversions: In[19]: aloha = pd . read_csv ( 'aloha.csv' , converters = { 'attrvalue' : parse_if_number , 'binedges' : parse_ndarray , 'binvalues' : parse_ndarray , 'vectime' : parse_ndarray , 'vecvalue' : parse_ndarray }) You can verify the result e.g. by printing the unique values again. 6. Load-time filtering \u00b6 If the CSV file is large, you may want to skip certain columns or rows when reading it into memory. (File size is about the only valid reason for using load-time filtering, because you can also filter out or drop rows/columns from the data frame when it is already loaded.) To filter out columns, you need to specify in the usecols parameter the list of columns to keep: In[20]: tmp = pd . read_csv ( 'aloha.csv' , usecols = [ 'run' , 'type' , 'module' , 'name' , 'value' ]) There is no such direct support for filtering out rows based on their content, but we can implement it using the iterator API that reads the CSV file in chunks. We can filter each chunk before storing and finally concatenating them into a single data frame: In[21]: iter = pd . read_csv ( 'aloha.csv' , iterator = True , chunksize = 100 ) chunks = [ chunk [ chunk [ 'type' ] != 'histogram' ] for chunk in iter ] # discards type=='histogram' lines tmp = pd . concat ( chunks ) 7. Plotting scalars \u00b6 Scalars can serve as input for many different kinds of plots. Here we'll show how one can create a \"throughput versus offered load\" type plot. We will plot the channel utilization in the Aloha model in the function of the packet generation frequency. Channel utilization is also affected by the number of hosts in the network -- we want results belonging to the same number of hosts to form iso lines. Packet generation frequency and the number of hosts are present in the results as iteration variables named iaMean and numHosts ; channel utilization values are the channelUtilization : last scalars saved by the Aloha.server module. The data contains the results from two simulation runs for each (iaMean, numHosts) pair done with different seeds; we want to average them for the plot. The first few steps are fairly straightforward. We only need the scalars and the iteration variables from the data frame, so we filter out the rest. Then we create a qname column from other columns to hold the names of our variables: the names of scalars are in the module and name columns (we want to join them with a dot), and the names of iteration variables are in the attrname column. Since attrname is not filled in for scalar rows, we can take attrname as qname first, then fill in the holes with module.name . We use the combine_first() method for that: a.combine_first(b) fills the holes in a using the corresponding values from b . The similar issue arises with values: values of output scalars are in the value column, while that of iteration variables are in the attrvalue column. Since attrvalue is unfilled for scalar rows, we can again utilize combine_first() to merge two. There is one more catch: we need to change the dtype of the attrvalue to float64 , otherwise the resulting value column also becomes object dtype. (Luckily, all our iteration variables are numeric, so the dtype conversion is possible. In other simulations that contain non-numeric itervars, one needs to filter those out, force them into numeric values somehow, or find some other trick to make things work.) In[22]: scalars = aloha [( aloha . type == 'scalar' ) | ( aloha . type == 'itervar' )] # filter rows scalars = scalars . assign ( qname = scalars . attrname . combine_first ( scalars . module + '.' + scalars . name )) # add qname column scalars . value = scalars . value . combine_first ( scalars . attrvalue . astype ( 'float64' )) # merge value columns scalars [[ 'run' , 'type' , 'qname' , 'value' , 'module' , 'name' , 'attrname' ]] . iloc [ 80 : 90 ] # print an excerpt of the result Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type qname value module name attrname 1134 PureAlohaExperiment-40-20170627-20:42:22-22773 itervar iaMean 9.000000 NaN NaN iaMean 1135 PureAlohaExperiment-40-20170627-20:42:22-22773 itervar numHosts 20.000000 NaN NaN numHosts 1162 PureAlohaExperiment-41-20170627-20:42:22-22773 itervar iaMean 9.000000 NaN NaN iaMean 1163 PureAlohaExperiment-41-20170627-20:42:22-22773 itervar numHosts 20.000000 NaN NaN numHosts 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.duration 5400.000000 Aloha.server duration NaN 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:mean 0.198275 Aloha.server collisionLength:mean NaN 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:sum 2457.026781 Aloha.server collisionLength:sum NaN 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:max 0.901897 Aloha.server collisionLength:max NaN 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collidedFrames:last 40805.000000 Aloha.server collidedFrames:last NaN 1186 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.channelUtilization:last 0.156057 Aloha.server channelUtilization:last NaN To work further, it would be very convenient if we had a format where each simulation run corresponds to one row, and all variables produced by that run had their own columns. We can call it the wide format, and it can be produced using the pivot() method: In[23]: scalars_wide = scalars . pivot ( 'run' , columns = 'qname' , values = 'value' ) scalars_wide . head () Out[23]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qname Aloha.server.channelUtilization:last Aloha.server.collidedFrames:last Aloha.server.collisionLength:max Aloha.server.collisionLength:mean Aloha.server.collisionLength:sum Aloha.server.duration Aloha.server.receivedFrames:last iaMean numHosts run PureAlohaExperiment-0-20170627-20:42:16-22739 0.156057 40805.0 0.901897 0.198275 2457.026781 5400.0 8496.0 1.0 10.0 PureAlohaExperiment-1-20170627-20:42:17-22739 0.156176 40692.0 0.958902 0.198088 2456.494983 5400.0 8503.0 1.0 10.0 PureAlohaExperiment-10-20170627-20:42:16-22741 0.109571 1760.0 0.326138 0.155154 126.450220 5400.0 5965.0 7.0 10.0 PureAlohaExperiment-11-20170627-20:42:16-22741 0.108992 1718.0 0.340096 0.154529 125.477252 5400.0 5934.0 7.0 10.0 PureAlohaExperiment-12-20170627-20:42:16-22741 0.090485 1069.0 0.272013 0.152142 78.201174 5400.0 4926.0 9.0 10.0 We are interested in only three columns for our plot: In[24]: scalars_wide [[ 'numHosts' , 'iaMean' , 'Aloha.server.channelUtilization:last' ]] . head () Out[24]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qname numHosts iaMean Aloha.server.channelUtilization:last run PureAlohaExperiment-0-20170627-20:42:16-22739 10.0 1.0 0.156057 PureAlohaExperiment-1-20170627-20:42:17-22739 10.0 1.0 0.156176 PureAlohaExperiment-10-20170627-20:42:16-22741 10.0 7.0 0.109571 PureAlohaExperiment-11-20170627-20:42:16-22741 10.0 7.0 0.108992 PureAlohaExperiment-12-20170627-20:42:16-22741 10.0 9.0 0.090485 Since we have our x and y data in separate columns now, we can utilize the scatter plot feature of the data frame for plotting it: In[25]: # set the default image resolution and size plt . rcParams [ 'figure.figsize' ] = [ 8.0 , 3.0 ] plt . rcParams [ 'figure.dpi' ] = 144 # create a scatter plot scalars_wide . plot . scatter ( 'iaMean' , 'Aloha.server.channelUtilization:last' ) plt . show () Out[25]: NOTE: Although plt.show() is not needed in Jupyter ( %matplotlib inline turns on immediate display), we'll continue to include it in further code fragments, so that they work without change when you use another Python shell. The resulting chart looks quite good as the first attempt. However, it has some shortcomings: Dots are not connected. The dots that have the same numHosts value should be connected with iso lines. As the result of having two simulation runs for each (iaMean,numHosts) pair, the dots appear in pairs. We'd like to see their averages instead. Unfortunately, scatter plot can only take us this far, we need to look for another way. What we really need as chart input is a table where rows correspond to different iaMean values, columns correspond to different numHosts values, and cells contain channel utilization values (the average of the repetitions). Such table can be produced from the \"wide format\" with another pivoting operation. We use pivot_table() , a cousin of the pivot() method we've seen above. The difference between them is that pivot() is a reshaping operation (it just rearranges elements), while pivot_table() is more of a spreadsheet-style pivot table creation operation, and primarily intended for numerical data. pivot_table() accepts an aggregation function with the default being mean , which is quite convenient for us now (we want to average channel utilization over repetitions.) In[26]: aloha_pivot = scalars_wide . pivot_table ( index = 'iaMean' , columns = 'numHosts' , values = 'Aloha.server.channelUtilization:last' ) # note: aggregation function = mean (that's the default) aloha_pivot . head () Out[26]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } numHosts 10.0 15.0 20.0 iaMean 1.0 0.156116 0.089539 0.046586 2.0 0.194817 0.178159 0.147564 3.0 0.176321 0.191571 0.183976 4.0 0.153569 0.182324 0.190452 5.0 0.136997 0.168780 0.183742 Note that rows correspond to various iaMean values ( iaMean serves as index); there is one column for each value of numHosts ; and that data in the table are the averages of the channel utilizations produced by the simulations performed with the respective iaMean and numHosts values. For the plot, every column should generate a separate line (with the x values coming from the index column, iaMean ) labelled with the column name. The basic Matplotlib interface cannot create such plot in one step. However, the Pandas data frame itself has a plotting interface which knows how to interpret the data, and produces the correct plot without much convincing: In[27]: aloha_pivot . plot . line () plt . ylabel ( 'channel utilization' ) plt . show () Out[27]: 8. Interactive pivot tables \u00b6 Getting the pivot table right is not always easy, so having a GUI where one can drag columns around and immediately see the result is definitely a blessing. Pivottable.js presents such a GUI inside a browser, and although the bulk of the code is Javascript, it has a Python frond-end that integrates nicely with Jupyter. Let's try it! In[28]: import pivottablejs as pj pj . pivot_ui ( scalars_wide ) Out[28]: An interactive panel containing the pivot table will appear. Here is how you can reproduce the above \"Channel utilization vs iaMean\" plot in it: Drag numHosts to the \"rows\" area of the pivot table. The table itself is the area on the left that initially only displays \"Totals | 42\", and the \"rows\" area is the empty rectangle directly of left it. The table should show have two columns ( numHosts and Totals ) and five rows in total after dragging. Drag iaMean to the \"columns\" area (above the table). Columns for each value of iaMean should appear in the table. Near the top-left corner of the table, select Average from the combo box that originally displays Count , and select ChannelUtilization : last from the combo box that appears below it. In the top-left corner of the panel, select Line Chart from the combo box that originally displays Table . If you can't get to see it, the following command will programmatically configure the pivot table in the appropriate way: In[29]: pj . pivot_ui ( scalars_wide , rows = [ 'numHosts' ], cols = [ 'iaMean' ], vals = [ 'Aloha.server.channelUtilization:last' ], aggregatorName = 'Average' , rendererName = 'Line Chart' ) Out[29]: If you want experiment with Excel's or LibreOffice's built-in pivot table functionality, the data frame's to_clipboard() and to_csv() methods will help you transfer the data. For example, you can issue the scalars_wide.to_clipboard() command to put the data on the clipboard, then paste it into the spreadsheet. Alternatively, type print(scalars_wide.to_csv()) to print the data in CSV format that you can select and then copy/paste. Or, use scalars_wide.to_csv(\"scalars.csv\") to save the data into a file which you can import. 9. Plotting histograms \u00b6 In this section we explore how to plot histograms recorded by the simulation. Histograms are in rows that have \"histogram\" in the type column. Histogram bin edges and bin values (counts) are in the binedges and binvalues columns as NumPy array objects ( ndarray ). Let us begin by selecting the histograms into a new data frame for convenience. In[30]: histograms = aloha [ aloha . type == 'histogram' ] len ( histograms ) Out[30]: 84 We have 84 histograms. It makes no sense to plot so many histograms on one chart, so let's just take one on them, and examine its content. In[31]: hist = histograms . iloc [ 0 ] # the first histogram hist . binedges , hist . binvalues Out[31]: (array([-0.11602833, -0.08732314, -0.05861794, -0.02991275, -0.00120756, 0.02749763, 0.05620283, 0.08490802, 0.11361321, 0.1423184 , 0.1710236 , 0.19972879, 0.22843398, 0.25713917, 0.28584437, 0.31454956, 0.34325475, 0.37195994, 0.40066514, 0.42937033, 0.45807552, 0.48678071, 0.51548591, 0.5441911 , 0.57289629, 0.60160148, 0.63030668, 0.65901187, 0.68771706, 0.71642225, 0.74512745]), array([ 0., 0., 0., 0., 0., 0., 0., 1234., 2372., 2180., 2115., 1212., 917., 663., 473., 353., 251., 186., 123., 99., 60., 44., 31., 25., 15., 13., 9., 3., 5., 3.])) The easiest way to plot the histogram from these two arrays is to look at it as a step function, and create a line plot with the appropriate drawing style. The only caveat is that we need to add an extra 0 element to draw the right side of the last histogram bin. In[32]: plt . plot ( hist . binedges , np . append ( hist . binvalues , 0 ), drawstyle = 'steps-post' ) # or maybe steps-mid, for integers plt . show () Out[32]: Another way to plot a recorded histogram is Matplotlib's hist() method, although that is a bit tricky. Instead of taking histogram data, hist() insists on computing the histogram itself from an array of values -- but we only have the histogram, and not the data it was originally computed from. Fortunately, hist() can accept a bin edges array, and another array as weights for the values. Thus, we can trick it into doing what we want by passing in our binedges array twice, once as bin edges and once as values, and specifying binvalues as weights. In[33]: plt . hist ( bins = hist . binedges , x = hist . binedges [: - 1 ], weights = hist . binvalues ) plt . show () Out[33]: hist() has some interesting options. For example, we can change the plotting style to be similar to a line plot by setting histtype='step' . To plot the normalized version of the histogram, specify density=True . To draw the cumulative density function, also specify cumulative=True . The following plot shows the effect of some of these options. In[34]: plt . hist ( bins = hist . binedges , x = hist . binedges [: - 1 ], weights = hist . binvalues , histtype = 'step' , density = True ) plt . show () Out[34]: To plot several histograms, we can iterate over the histograms and draw them one by one on the same plot. The following code does that, and also adds a legend and adjusts the bounds of the x axis. In[35]: somehistograms = histograms [ histograms . name == 'collisionLength:histogram' ][: 5 ] for row in somehistograms . itertuples (): plt . plot ( row . binedges , np . append ( row . binvalues , 0 ), drawstyle = 'steps-post' ) plt . legend ( somehistograms . module + \".\" + somehistograms . name ) plt . xlim ( 0 , 0.5 ) plt . show () Out[35]: Note, however, that the legend contains the same string for all histograms, which is not very meaningful. We could improve that by including some characteristics of the simulation that generated them, i.e. the number of hosts ( numHosts iteration variable) and frame interarrival times ( iaTime iteration variable). We'll see in the next section how that can be achieved. 10. Adding iteration variables as columns \u00b6 In this step, we add the iteration variables associated with the simulation run to the data frame as columns. There are several reasons why this is a good idea: they are very useful for generating the legends for plots of e.g. histograms and vectors (e.g. \"collision multiplicity histogram for numHosts=20 and iaMean=2s\"), and often needed as chart input as well. First, we select the iteration variables vars as a smaller data frame. In[36]: itervars_df = aloha . loc [ aloha . type == 'itervar' , [ 'run' , 'attrname' , 'attrvalue' ]] itervars_df . head () Out[36]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run attrname attrvalue 14 PureAlohaExperiment-4-20170627-20:42:20-22739 iaMean 3 15 PureAlohaExperiment-4-20170627-20:42:20-22739 numHosts 10 42 PureAlohaExperiment-3-20170627-20:42:20-22739 iaMean 2 43 PureAlohaExperiment-3-20170627-20:42:20-22739 numHosts 10 70 PureAlohaExperiment-0-20170627-20:42:16-22739 iaMean 1 We reshape the result by using the pivot() method. The following statement will convert unique values in the attrname column into separate columns: iaMean and numHosts . The new data frame will be indexed with the run id. In[37]: itervarspivot_df = itervars_df . pivot ( index = 'run' , columns = 'attrname' , values = 'attrvalue' ) itervarspivot_df . head () Out[37]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } attrname iaMean numHosts run PureAlohaExperiment-0-20170627-20:42:16-22739 1 10 PureAlohaExperiment-1-20170627-20:42:17-22739 1 10 PureAlohaExperiment-10-20170627-20:42:16-22741 7 10 PureAlohaExperiment-11-20170627-20:42:16-22741 7 10 PureAlohaExperiment-12-20170627-20:42:16-22741 9 10 Now, we only need to add the new columns back into the original dataframe, using merge() . This operation is not quite unlike an SQL join of two tables on the run column. In[38]: aloha2 = aloha . merge ( itervarspivot_df , left_on = 'run' , right_index = True , how = 'outer' ) aloha2 . head () Out[38]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue iaMean numHosts 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 For plot legends, it is also useful to have a single iterationvars column with string values like numHosts=10, iaMean=2 . This is easier than the above: we can just select the rows containing the run attribute named iterationvars (it contains exactly the string we need), take only the run and attrvalue columns, rename the attrvalue column to iterationvars , and then merge back the result into the original data frame in a way we did above. The selection and renaming step can be done as follows. (Note: we need .astype ( str ) in the condition so that rows where attrname is not filled in do not cause trouble.) In[39]: itervarscol_df = aloha . loc [( aloha . type == 'runattr' ) & ( aloha . attrname . astype ( str ) == 'iterationvars' ), [ 'run' , 'attrvalue' ]] itervarscol_df = itervarscol_df . rename ( columns = { 'attrvalue' : 'iterationvars' }) itervarscol_df . head () Out[39]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run iterationvars 4 PureAlohaExperiment-4-20170627-20:42:20-22739 numHosts=10, iaMean=3 32 PureAlohaExperiment-3-20170627-20:42:20-22739 numHosts=10, iaMean=2 60 PureAlohaExperiment-0-20170627-20:42:16-22739 numHosts=10, iaMean=1 88 PureAlohaExperiment-1-20170627-20:42:17-22739 numHosts=10, iaMean=1 116 PureAlohaExperiment-2-20170627-20:42:19-22739 numHosts=10, iaMean=2 In the merging step, we join the two tables (I mean, data frames) on the run column: In[40]: aloha3 = aloha2 . merge ( itervarscol_df , left_on = 'run' , right_on = 'run' , how = 'outer' ) aloha3 . head () Out[40]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue iaMean numHosts iterationvars 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 To see the result of our work, let's try plotting the same histograms again, this time with a proper legend: In[41]: histograms = aloha3 [ aloha3 . type == 'histogram' ] somehistograms = histograms [ histograms . name == 'collisionLength:histogram' ][: 5 ] for row in somehistograms . itertuples (): plt . plot ( row . binedges , np . append ( row . binvalues , 0 ), drawstyle = 'steps-post' ) plt . title ( 'collisionLength:histogram' ) plt . legend ( somehistograms . iterationvars ) plt . xlim ( 0 , 0.5 ) plt . show () Out[41]: 11. Plotting vectors \u00b6 This section deals with basic plotting of output vectors. Output vectors are basically time series data, but values have timestamps instead of being evenly spaced. Vectors are in rows that have \"vector\" in the type column. The values and their timestamps are in the vecvalue and vectime columns as NumPy array objects ( ndarray ). We'll use a different data set for exploring output vector plotting, one from the routing example simulation. There are pre-recorded result files in the samples/resultfiles/routing directory; change into it in the terminal, and issue the following command to convert them to CSV: scavetool x *.sca *.vec -o routing.csv Then we read the the CSV file into a data frame in the same way we saw with the aloha dataset: In[42]: routing = pd . read_csv ( 'routing.csv' , converters = { 'attrvalue' : parse_if_number , 'binedges' : parse_ndarray , 'binvalues' : parse_ndarray , 'vectime' : parse_ndarray , 'vecvalue' : parse_ndarray }) Let us begin by selecting the vectors into a new data frame for convenience. In[43]: vectors = routing [ routing . type == 'vector' ] len ( vectors ) Out[43]: 65 Our data frame contains results from one run. To get some idea what vectors we have, let's print the list unique vector names and module names: In[44]: vectors . name . unique (), vectors . module . unique () Out[44]: (array(['busy:vector', 'qlen:vector', 'txBytes:vector', 'endToEndDelay:vector', 'hopCount:vector', 'sourceAddress:vector', 'rxBytes:vector', 'drop:vector'], dtype=object), array(['Net5.rte[0].port$o[0].channel', 'Net5.rte[0].port$o[1].channel', 'Net5.rte[1].port$o[0].channel', 'Net5.rte[1].port$o[1].channel', 'Net5.rte[1].port$o[2].channel', 'Net5.rte[2].port$o[0].channel', 'Net5.rte[2].port$o[1].channel', 'Net5.rte[2].port$o[2].channel', 'Net5.rte[2].port$o[3].channel', 'Net5.rte[3].port$o[0].channel', 'Net5.rte[3].port$o[1].channel', 'Net5.rte[3].port$o[2].channel', 'Net5.rte[4].port$o[0].channel', 'Net5.rte[4].port$o[1].channel', 'Net5.rte[0].queue[0]', 'Net5.rte[0].queue[1]', 'Net5.rte[1].queue[0]', 'Net5.rte[1].queue[1]', 'Net5.rte[1].queue[2]', 'Net5.rte[2].queue[0]', 'Net5.rte[2].queue[1]', 'Net5.rte[2].queue[2]', 'Net5.rte[2].queue[3]', 'Net5.rte[3].queue[0]', 'Net5.rte[3].queue[1]', 'Net5.rte[3].queue[2]', 'Net5.rte[4].queue[0]', 'Net5.rte[4].queue[1]', 'Net5.rte[4].app', 'Net5.rte[1].app'], dtype=object)) A vector can be plotted on a line chart by simply passing the vectime and vecvalue arrays to plt.plot() : In[45]: vec = vectors [ vectors . name == 'qlen:vector' ] . iloc [ 4 ] # take some vector plt . plot ( vec . vectime , vec . vecvalue , drawstyle = 'steps-post' ) plt . xlim ( 0 , 100 ) plt . show () Out[45]: When several vectors need to be placed on the same plot, one can simply use a for loop. In[46]: somevectors = vectors [ vectors . name == 'qlen:vector' ][: 5 ] for row in somevectors . itertuples (): plt . plot ( row . vectime , row . vecvalue , drawstyle = 'steps-post' ) plt . title ( somevectors . name . values [ 0 ]) plt . legend ( somevectors . module ) plt . show () Out[46]: 12. Vector Filtering \u00b6 Plotting vectors \"as is\" is often not practical, as the result will be a crowded plot that's difficult to draw conclusions from. To remedy that, one can apply some kind of filtering before plotting, or plot a derived quantity such as the integral, sum or running average instead of the original. Such things can easily be achieved with the help of NumPy. Vector time and value are already stored in the data frame as NumPy arrays ( ndarray ), so we can apply NumPy functions to them. For example, let's try np.cumsum() which computes cumulative sum: In[47]: x = np . array ([ 8 , 2 , 1 , 5 , 7 ]) np . cumsum ( x ) Out[47]: array([ 8, 10, 11, 16, 23]) In[48]: for row in somevectors . itertuples (): plt . plot ( row . vectime , np . cumsum ( row . vecvalue )) plt . show () Out[48]: Plotting cumulative sum against time might be useful e.g. for an output vector where the simulation emits the packet length for each packet that has arrived at its destination. There, the sum would represent \"total bytes received\". Plotting the count against time for the same output vector would represent \"number of packets received\". For such a plot, we can utilize np.arange(1,n) which simply returns the numbers 1, 2, .., n-1 as an array: In[49]: for row in somevectors . itertuples (): plt . plot ( row . vectime , np . arange ( 1 , row . vecvalue . size + 1 ), '.-' , drawstyle = 'steps-post' ) plt . xlim ( 0 , 5 ); plt . ylim ( 0 , 20 ) plt . show () Out[49]: Note that we changed the plotting style to \"steps-post\", so that for any t time the plot accurately represents the number of values whose timestamp is less than or equal to t . As another warm-up exercise, let's plot the time interval that elapses between adjacent values; that is, for each element we want to plot the time difference between the that element and the previous one. This can be achieved by computing t [ 1 : ] - t [:- 1 ] , which is the elementwise subtraction of the t array and its shifted version. Array indexing starts at 0, so t[1:] means \"drop the first element\". Negative indices count from the end of the array, so t [:- 1 ] means \"without the last element\". The latter is necessary because the sizes of the two arrays must match. or convenience, we encapsulate the formula into a Python function: In[50]: def diff ( t ): return t [ 1 :] - t [: - 1 ] # example t = np . array ([ 0.1 , 1.5 , 1.6 , 2.0 , 3.1 ]) diff ( t ) Out[50]: array([1.4, 0.1, 0.4, 1.1]) We can now plot it. Note that as diff() makes the array one element shorter, we need to write row.vectime[1:] to drop the first element (it has no preceding element, so diff() cannot be computed for it.) Also, we use dots for plotting instead of lines, as it makes more sense here. In[51]: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], diff ( row . vectime ), 'o' ) plt . xlim ( 0 , 100 ) plt . show () Out[51]: We now know enough NumPy to be able to write a function that computes running average (a.k.a. \"mean filter\"). Let's try it out in a plot immediately. In[52]: def running_avg ( x ): return np . cumsum ( x ) / np . arange ( 1 , x . size + 1 ) # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime , running_avg ( row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Out[52]: For certain quantities such as queue length or on-off status, weighted average (with time intervals used as weights) makes more sense. Here is a function that computes running time-average: In[53]: def running_timeavg ( t , x ): dt = t [ 1 :] - t [: - 1 ] return np . cumsum ( x [: - 1 ] * dt ) / t [ 1 :] # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], running_timeavg ( row . vectime , row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Out[53]: Computing the integral of the vector as a step function is very similar to the running_timeavg() function. (Note: Computing integral in other ways is part of NumPy and SciPy, if you ever need it. For example, np.trapz(y,x) computes integral using the trapezoidal rule.) In[54]: def integrate_steps ( t , x ): dt = t [ 1 :] - t [: - 1 ] return np . cumsum ( x [: - 1 ] * dt ) # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], integrate_steps ( row . vectime , row . vecvalue )) plt . show () Out[54]: As the last example in this section, here is a function that computes moving window average. It relies on the clever trick of subtracting the cumulative sum of the original vector from its shifted version to get the sum of values in every N -sized window. In[55]: def winavg ( x , N ): xpad = np . concatenate (( np . zeros ( N ), x )) # pad with zeroes s = np . cumsum ( xpad ) ss = s [ N :] - s [: - N ] ss [ N - 1 :] /= N ss [: N - 1 ] /= np . arange ( 1 , min ( N - 1 , ss . size ) + 1 ) return ss # example: for row in somevectors . itertuples (): plt . plot ( row . vectime , winavg ( row . vecvalue , 10 )) plt . xlim ( 0 , 200 ) plt . show () Out[55]: You can find further hints for smoothing the plot of an output vector in the signal processing chapter of the SciPy Cookbook (see References). Resources \u00b6 The primary and authentic source of information on Pandas, Matplotlib and other libraries is their official documentation. I do not link them here because they are trivial to find via Google. Instead, here is a random collection of other resources that I found useful while writing this tutorial (not counting all the StackOverflow pages I visited.) Pandas tutorial from Greg Reda: http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/ On reshaping data frames: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping Matplotlib tutorial of Nicolas P. Rougier: https://www.labri.fr/perso/nrougier/teaching/matplotlib/ Creating boxplots with Matplotlib, from Bharat Bhole: http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/ SciPy Cookbook on signal smoothing: http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html Visual Guide on Pandas (video): https://www.youtube.com/watch?v=9d5-Ti6onew Python Pandas Cookbook (videos): https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3 Acknowledgements \u00b6 The author, Andras Varga would like to thank the participants of the 2016 OMNeT++ Summit for the valuable feedback, and especially Dr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.","title":"Result Analysis with Python"},{"location":"tutorials/pandas/#1-when-to-use-python","text":"The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of simulation results. If you are doing sophisticated result analysis, you will notice after a while that you have outgrown the IDE. The need for customized charts, the necessity of multi-step computations to produce chart input, or the sheer volume of raw simulation results might all be causes to make you look for something else. If you are an R or Matlab expert, you'll probably reach for those tools, but for everyone else, Python with the right libraries is pretty much the best choice. Python has a big momentum for data science, and in addition to having excellent libraries for data analysis and visualization, it is also a great general-purpose programming language. Python is used for diverse problems ranging from building desktop GUIs to machine learning and AI, so the knowledge you gain by learning it will be convertible to other areas. This tutorial will walk you through the initial steps of using Python for analysing simulation results, and shows how to do some of the most common tasks. The tutorial assumes that you have a working knowledge of OMNeT++ with regard to result recording, and basic familiarity with Python.","title":"1. When to use Python?"},{"location":"tutorials/pandas/#2-setting-up","text":"Before we can start, you need to install the necessary software. First, make sure you have Python, either version 2.x or 3.x (they are slightly incompatible.) If you have both versions available on your system, we recommend version 3.x. You also need OMNeT++ version 5.2 or later. We will heavily rely on three Python packages: NumPy , Pandas , and Matplotlib . There are also optional packages that will be useful for certain tasks: SciPy , PivotTable.js . We also recommend that you install IPython and Jupyter , because they let you work much more comfortably than the bare Python shell. On most systems, these packages can be installed with pip , the Python package manager (if you go for Python 3, replace pip with pip3 in the commands below): sudo pip install ipython jupyter sudo pip install numpy pandas matplotlib sudo pip install scipy pivottablejs As packages continually evolve, there might be incompatibilities between versions. We used the following versions when writing this tutorial: Pandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0. An easy way to determine which versions you have installed is using the pip list command. (Note that the last one is the version of the Python interface library, the PivotTable.js main Javascript library uses different version numbers, e.g. 2.7.0.)","title":"2. Setting up"},{"location":"tutorials/pandas/#3-getting-your-simulation-results-into-python","text":"OMNeT++ result files have their own file format which is not directly digestible by Python. There are a number of ways to get your data inside Python: Export from the IDE. The Analysis Tool can export data in a number of formats, the ones that are useful here are CSV and Python-flavoured JSON. In this tutorial we'll use the CSV export, and read the result into Pandas using its read_csv() function. Export using scavetool. Exporting from the IDE may become tedious after a while, because you have to go through the GUI every time your simulations are re-run. Luckily, you can automate the exporting with OMNeT++'s scavetool program. scavetool exposes the same export functionality as the IDE, and also allows filtering of the data. Read the OMNeT++ result files directly from Python. Development of a Python package to read these files into Pandas data frames is underway, but given that these files are line-oriented text files with a straightforward and well-documented structure, writing your own custom reader is also a perfectly feasible option. SQLite. Since version 5.1, OMNeT++ has the ability to record simulation results int SQLite3 database files, which can be opened directly from Python using the sqlite package. This lets you use SQL queries to select the input data for your charts or computations, which is kind of cool! You can even use GUIs like SQLiteBrowser to browse the database and craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3 output, you'll still get .vec and .sca files as before, only their format will change from textual to SQLite's binary format. When querying the contents of the files, one issue to deal with is that SQLite does not allow cross-database queries, so you either need to configure OMNeT++ to record everything into one file (i.e. each run should append instead of creating a new file), or use scavetool's export functionality to merge the files into one. Custom result recording. There is also the option to instrument the simulation (via C++ code) or OMNeT++ (via custom result recorders) to produce files that Python can directly digest, e.g. CSV. However, in the light of the above options, it is rarely necessary to go this far. With large-scale simulation studies, it can easily happen that the full set of simulation results do not fit into the memory at once. There are also multiple approaches to deal with this problem: If you don't need all simulation results for the analysis, you can configure OMNeT++ to record only a subset of them. Fine-grained control is available. Perform filtering and aggregation steps before analysis. The IDE and scavetool are both capable of filtering the results before export. When the above approaches are not enough, it can help to move part of the result processing (typically, filtering and aggregation) into the simulation model as dedicated result collection modules. However, this solution requires significantly more work than the previous two, so use with care. In this tutorial, we'll work with the contents of the samples/resultfiles directory distributed with OMNeT++. The directory contains result files produced by the Aloha and Routing sample simulations, both of which are parameter studies. We'll start by looking at the Aloha results. As the first step, we use OMNeT++'s scavetool to convert Aloha's scalar files to CSV. Run the following commands in the terminal (replace ~/omnetpp with the location of your OMNeT++ installation): cd ~/omnetpp/samples/resultfiles/aloha scavetool x *.sca -o aloha.csv In the scavetool command line, x means export, and the export format is inferred from the output file's extension. (Note that scavetool supports two different CSV output formats. We need CSV Records , or CSV-R for short, which is the default for the .csv extension.) Let us spend a minute on what the export has created. The CSV file has a fixed number of columns named run , type , module , name , value , etc. Each result item, i.e. scalar, statistic, histogram and vector, produces one row of output in the CSV. Other items such as run attributes, iteration variables of the parameter study and result attributes also generate their own rows. The content of the type column determines what type of information a given row contains. The type column also determines which other columns are in use. For example, the binedges and binvalues columns are only filled in for histogram items. The colums are: run : Identifies the simulation run type : Row type, one of the following: scalar , vector , statistics , histogram , runattr , itervar , param , attr module : Hierarchical name (a.k.a. full path) of the module that recorded the result item name : Name of the result item (scalar, statistic, histogram or vector) attrname : Name of the run attribute or result item attribute (in the latter case, the module and name columns identify the result item the attribute belongs to) attrvalue : Value of run and result item attributes, iteration variables, saved ini param settings ( runattr , attr , itervar , param ) value : Output scalar value count , sumweights , mean , min , max , stddev : Fields of the statistics or histogram binedges , binvalues : Histogram bin edges and bin values, as space-separated lists. len(binedges)==len(binvalues)+1 vectime , vecvalue : Output vector time and value arrays, as space-separated lists When the export is done, you can start Jupyter server with the following command: jupyter notebook Open a web browser with the displayed URL to access the Jupyter GUI. Once there, choose New -> Python3 in the top right corner to open a blank notebook. The notebook allows you to enter Python commands or sequences of commands, run them, and view the output. Note that Enter simply inserts a newline; hit Ctrl+Enter to execute the commands in the current cell, or Alt+Enter to execute them and also insert a new cell below. If you cannot use Jupyter for some reason, a terminal-based Python shell ( python or ipython ) will also allow you to follow the tutorial. On the Python prompt, enter the following lines to make the functionality of Pandas, NumpPy and Matplotlib available in the session. The last, %matplotlib line is only needed for Jupyter. (It is a \"magic command\" that arranges plots to be displayed within the notebook.) In[1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline We utilize the read_csv() function to import the contents of the CSV file into a data frame. The data frame is the central concept of Pandas. We will continue to work with this data frame throughout the whole tutorial. In[2]: aloha = pd . read_csv ( 'aloha.csv' )","title":"3. Getting your simulation results into Python"},{"location":"tutorials/pandas/#4-exploring-the-data-frame","text":"You can view the contents of the data frame by simply entering the name of the variable ( aloha ). Alternatively, you can use the head() method of the data frame to view just the first few lines. In[3]: aloha . head () Out[3]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN You can see that the structure of the data frame, i.e. rows and columns, directly corresponds to the contents of the CSV file. Column names have been taken from the first line of the CSV file. Missing values are represented with NaNs (not-a-number). The complementary tail() method shows the last few lines. There is also an iloc method that we use at places in this tutorial to show rows from the middle of the data frame. It accepts a range: aloha.iloc[20:30] selects 10 lines from line 20, aloha.iloc[:5] is like head() , and aloha.iloc[-5:] is like tail() . In[4]: aloha . iloc [ 1200 : 1205 ] Out[4]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 1200 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server collidedFrames:last NaN NaN 40692.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1201 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server collidedFrames:last source sum(collision) NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1202 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server collidedFrames:last title collided frames, last NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1203 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156176 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1204 PureAlohaExperiment-1-20170627-20:42:17-22739 attr Aloha.server channelUtilization:last interpolationmode linear NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Hint: If you are in the terminal and you find that the data frame printout does not make use of the whole width of the terminal, you can increase the display width for better readability with the following commands: In[5]: pd . set_option ( 'display.width' , 180 ) pd . set_option ( 'display.max_colwidth' , 100 ) If you have not looked at any Pandas tutorial yet, now is a very good time to read one. (See References at the bottom of this page for hints.) Until you finish, here are some basics for your short-term survival. You can refer to a column as a whole with the array index syntax: aloha['run'] . Alternatively, the more convenient member access syntax ( aloha.run ) can also be used, with restrictions. (E.g. the column name must be valid as a Python identifier, and should not collide with existing methods of the data frame. Names that are known to cause trouble include name , min , max , mean ). In[6]: aloha . run . head () # .head() is for limiting the output to 5 lines here Out[6]: 0 PureAlohaExperiment-4-20170627-20:42:20-22739 1 PureAlohaExperiment-4-20170627-20:42:20-22739 2 PureAlohaExperiment-4-20170627-20:42:20-22739 3 PureAlohaExperiment-4-20170627-20:42:20-22739 4 PureAlohaExperiment-4-20170627-20:42:20-22739 Name: run, dtype: object Selecting multiple columns is also possible, one just needs to use a list of column names as index. The result will be another data frame. (The double brackets in the command are due to the fact that both the array indexing and the list syntax use square brackets.) In[7]: tmp = aloha [[ 'run' , 'attrname' , 'attrvalue' ]] tmp . head () Out[7]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run attrname attrvalue 0 PureAlohaExperiment-4-20170627-20:42:20-22739 configname PureAlohaExperiment 1 PureAlohaExperiment-4-20170627-20:42:20-22739 datetime 20170627-20:42:20 2 PureAlohaExperiment-4-20170627-20:42:20-22739 experiment PureAlohaExperiment 3 PureAlohaExperiment-4-20170627-20:42:20-22739 inifile omnetpp.ini 4 PureAlohaExperiment-4-20170627-20:42:20-22739 iterationvars numHosts=10, iaMean=3 The describe() method can be used to get an idea about the contents of a column. When applied to a non-numeric column, it prints the number of non-null elements in it ( count ), the number of unique values ( unique ), the most frequently occurring value ( top ) and its multiplicity ( freq ), and the inferred data type (more about that later.) In[8]: aloha . module . describe () Out[8]: count 1012 unique 11 top Aloha.server freq 932 Name: module, dtype: object You can get a list of the unique values using the unique() method. For example, the following command lists the names of modules that have recorded any statistics: In[9]: aloha . module . unique () Out[9]: array([nan, 'Aloha.server', 'Aloha.host[0]', 'Aloha.host[1]', 'Aloha.host[2]', 'Aloha.host[3]', 'Aloha.host[4]', 'Aloha.host[5]', 'Aloha.host[6]', 'Aloha.host[7]', 'Aloha.host[8]', 'Aloha.host[9]'], dtype=object) When you apply describe() to a numeric column, you get a statistical summary with things like mean, standard deviation, minimum, maximum, and various quantiles. In[10]: aloha . value . describe () Out[10]: count 294.000000 mean 4900.038749 std 11284.077075 min 0.045582 25% 0.192537 50% 668.925298 75% 5400.000000 max 95630.000000 Name: value, dtype: float64 Applying describe() to the whole data frame creates a similar report about all numeric columns. In[11]: aloha . describe () Out[11]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } value count sumweights mean stddev min max count 294.000000 84.000000 0.0 84.000000 84.000000 84.000000 84.000000 mean 4900.038749 5591.380952 NaN 1.489369 0.599396 1.049606 6.560987 std 11284.077075 4528.796760 NaN 1.530455 0.962515 0.956102 9.774404 min 0.045582 470.000000 NaN 0.152142 0.031326 0.099167 0.272013 25% 0.192537 1803.000000 NaN 0.164796 0.049552 0.099186 0.498441 50% 668.925298 4065.500000 NaN 1.197140 0.243035 1.049776 3.084077 75% 5400.000000 8815.000000 NaN 2.384397 0.741081 2.000000 9.000000 max 95630.000000 14769.000000 NaN 6.936747 5.323887 2.000000 54.000000 Let's spend a minute on data types and column data types. Every column has a data type (abbreviated dtype ) that determines what type of values it may contain. Column dtypes can be printed with dtypes : In[12]: aloha . dtypes Out[12]: run object type object module object name object attrname object attrvalue object value float64 count float64 sumweights float64 mean float64 stddev float64 min float64 max float64 binedges object binvalues object vectime object vecvalue object dtype: object The two most commonly used dtypes are float64 and object . A float64 column contains floating-point numbers, and missing values are represented with NaNs. An object column may contain basically anything -- usually strings, but we'll also have NumPy arrays ( np.ndarray ) as elements in this tutorial. Numeric values and booleans may also occur in an object column. Missing values in an object column are usually represented with None , but Pandas also interprets the floating-point NaN like that. Some degree of confusion arises from fact that some Pandas functions check the column's dtype, while others are already happy if the contained elements are of the required type. To clarify: applying describe() to a column prints a type inferred from the individual elements, not the column dtype. The column dtype type can be changed with the astype() method; we'll see an example for using it later in this tutorial. The column dtype can be accessed as the dtype property of a column, for example aloha.stddev.dtype yields dtype('float64') . There are also convenience functions such as is_numeric_dtype() and is_string_dtype() for checking column dtype. (They need to be imported from the pandas.api.types package though.) Another vital thing to know, especially due of the existence of the type column in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly, the array index syntax can be used here as well. For example, the following expression selects the rows that contain iteration variables: aloha[aloha.type == 'itervar'] . With a healthy degree of sloppiness, here's how it works: aloha.type yields the values in the type column as an array-like data structure; aloha.type=='itervar' performs element-wise comparison and produces an array of booleans containing True where the condition holds and False where not; and indexing a data frame with an array of booleans returns the rows that correspond to True values in the array. Conditions can be combined with AND/OR using the \" & \" and \" | \" operators, but you need parentheses because of operator precedence. The following command selects the rows that contain scalars with a certain name and owner module: In[13]: tmp = aloha [( aloha . type == 'scalar' ) & ( aloha . module == 'Aloha.server' ) & ( aloha . name == 'channelUtilization:last' )] tmp . head () Out[13]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue 1186 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156057 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1203 PureAlohaExperiment-1-20170627-20:42:17-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.156176 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1220 PureAlohaExperiment-2-20170627-20:42:19-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.196381 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1237 PureAlohaExperiment-3-20170627-20:42:20-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.193253 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 1254 PureAlohaExperiment-4-20170627-20:42:20-22739 scalar Aloha.server channelUtilization:last NaN NaN 0.176507 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN You'll also need to know how to add a new column to the data frame. Now that is a bit controversial topic, because at the time of writing, there is a \"convenient\" syntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple assignment, for example: In[14]: aloha [ 'qname' ] = aloha . module + \".\" + aloha . name aloha [ aloha . type == 'scalar' ] . head () # print excerpt Out[14]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue qname 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server duration NaN NaN 5400.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.duration 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:mean NaN NaN 0.198275 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:mean 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:sum NaN NaN 2457.026781 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:sum 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:max NaN NaN 0.901897 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:max 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collidedFrames:last NaN NaN 40805.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collidedFrames:last It looks nice and natural, but it is not entirely correct. It often results in a warning: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame... . The message essentially says that the operation (here, adding the new column) might have been applied to a temporary object instead of the original data frame, and thus might have been ineffective. Luckily, that is not the case most of the time (the operation does take effect). Nevertheless, for production code, i.e. scripts, the \"official\" solution, the assign() method of the data frame is recommended, like this: In[15]: aloha = aloha . assign ( qname = aloha . module + \".\" + aloha . name ) aloha [ aloha . type == 'scalar' ] . head () Out[15]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue qname 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server duration NaN NaN 5400.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.duration 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:mean NaN NaN 0.198275 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:mean 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:sum NaN NaN 2457.026781 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:sum 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collisionLength:max NaN NaN 0.901897 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collisionLength:max 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server collidedFrames:last NaN NaN 40805.000000 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN Aloha.server.collidedFrames:last For completeness, one can remove a column from a data frame using either the del operator or the drop() method of the data frame. Here we show the former (also to remove the column we added above, as we won't need it for now): In[16]: del aloha [ 'qname' ]","title":"4. Exploring the data frame"},{"location":"tutorials/pandas/#5-revisiting-csv-loading","text":"The way we have read the CSV file has one small deficiency: all data in the attrvalue column are represented as strings, event though many of them are really numbers, for example the values of the iaMean and numHosts iteration variables. You can verify that by printing the unique values ( aloha.attrvalue.unique() -- it will print all values with quotes), or using the type() operator on an element: In[17]: type ( aloha [ aloha . type == 'scalar' ] . iloc [ 0 ] . value ) Out[17]: numpy.float64 The reason is that read_csv() infers data types of columns from the data it finds in them. Since the attrvalue column is shared by run attributes, result item attributes, iteration variables and some other types of rows, there are many non-numeric strings in it, and read_csv() decides that it is a string column. A similar issue arises with the binedges , binvalues , vectime , vecvalue columns. These columns contain lists of numbers separated by spaces, so they are read into strings as well. However, we would like to store them as NumPy arrays ( ndarray ) inside the data frame, because that's the form we can use in plots or as computation input. Luckily, read_csv() allows us to specify conversion functions for each column. So, armed with the following two short functions: In[18]: def parse_if_number ( s ): try : return float ( s ) except : return True if s == \"true\" else False if s == \"false\" else s if s else None def parse_ndarray ( s ): return np . fromstring ( s , sep = ' ' ) if s else None we can read the CSV file again, this time with the correct conversions: In[19]: aloha = pd . read_csv ( 'aloha.csv' , converters = { 'attrvalue' : parse_if_number , 'binedges' : parse_ndarray , 'binvalues' : parse_ndarray , 'vectime' : parse_ndarray , 'vecvalue' : parse_ndarray }) You can verify the result e.g. by printing the unique values again.","title":"5. Revisiting CSV loading"},{"location":"tutorials/pandas/#6-load-time-filtering","text":"If the CSV file is large, you may want to skip certain columns or rows when reading it into memory. (File size is about the only valid reason for using load-time filtering, because you can also filter out or drop rows/columns from the data frame when it is already loaded.) To filter out columns, you need to specify in the usecols parameter the list of columns to keep: In[20]: tmp = pd . read_csv ( 'aloha.csv' , usecols = [ 'run' , 'type' , 'module' , 'name' , 'value' ]) There is no such direct support for filtering out rows based on their content, but we can implement it using the iterator API that reads the CSV file in chunks. We can filter each chunk before storing and finally concatenating them into a single data frame: In[21]: iter = pd . read_csv ( 'aloha.csv' , iterator = True , chunksize = 100 ) chunks = [ chunk [ chunk [ 'type' ] != 'histogram' ] for chunk in iter ] # discards type=='histogram' lines tmp = pd . concat ( chunks )","title":"6. Load-time filtering"},{"location":"tutorials/pandas/#7-plotting-scalars","text":"Scalars can serve as input for many different kinds of plots. Here we'll show how one can create a \"throughput versus offered load\" type plot. We will plot the channel utilization in the Aloha model in the function of the packet generation frequency. Channel utilization is also affected by the number of hosts in the network -- we want results belonging to the same number of hosts to form iso lines. Packet generation frequency and the number of hosts are present in the results as iteration variables named iaMean and numHosts ; channel utilization values are the channelUtilization : last scalars saved by the Aloha.server module. The data contains the results from two simulation runs for each (iaMean, numHosts) pair done with different seeds; we want to average them for the plot. The first few steps are fairly straightforward. We only need the scalars and the iteration variables from the data frame, so we filter out the rest. Then we create a qname column from other columns to hold the names of our variables: the names of scalars are in the module and name columns (we want to join them with a dot), and the names of iteration variables are in the attrname column. Since attrname is not filled in for scalar rows, we can take attrname as qname first, then fill in the holes with module.name . We use the combine_first() method for that: a.combine_first(b) fills the holes in a using the corresponding values from b . The similar issue arises with values: values of output scalars are in the value column, while that of iteration variables are in the attrvalue column. Since attrvalue is unfilled for scalar rows, we can again utilize combine_first() to merge two. There is one more catch: we need to change the dtype of the attrvalue to float64 , otherwise the resulting value column also becomes object dtype. (Luckily, all our iteration variables are numeric, so the dtype conversion is possible. In other simulations that contain non-numeric itervars, one needs to filter those out, force them into numeric values somehow, or find some other trick to make things work.) In[22]: scalars = aloha [( aloha . type == 'scalar' ) | ( aloha . type == 'itervar' )] # filter rows scalars = scalars . assign ( qname = scalars . attrname . combine_first ( scalars . module + '.' + scalars . name )) # add qname column scalars . value = scalars . value . combine_first ( scalars . attrvalue . astype ( 'float64' )) # merge value columns scalars [[ 'run' , 'type' , 'qname' , 'value' , 'module' , 'name' , 'attrname' ]] . iloc [ 80 : 90 ] # print an excerpt of the result Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type qname value module name attrname 1134 PureAlohaExperiment-40-20170627-20:42:22-22773 itervar iaMean 9.000000 NaN NaN iaMean 1135 PureAlohaExperiment-40-20170627-20:42:22-22773 itervar numHosts 20.000000 NaN NaN numHosts 1162 PureAlohaExperiment-41-20170627-20:42:22-22773 itervar iaMean 9.000000 NaN NaN iaMean 1163 PureAlohaExperiment-41-20170627-20:42:22-22773 itervar numHosts 20.000000 NaN NaN numHosts 1176 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.duration 5400.000000 Aloha.server duration NaN 1177 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:mean 0.198275 Aloha.server collisionLength:mean NaN 1179 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:sum 2457.026781 Aloha.server collisionLength:sum NaN 1181 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collisionLength:max 0.901897 Aloha.server collisionLength:max NaN 1183 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.collidedFrames:last 40805.000000 Aloha.server collidedFrames:last NaN 1186 PureAlohaExperiment-0-20170627-20:42:16-22739 scalar Aloha.server.channelUtilization:last 0.156057 Aloha.server channelUtilization:last NaN To work further, it would be very convenient if we had a format where each simulation run corresponds to one row, and all variables produced by that run had their own columns. We can call it the wide format, and it can be produced using the pivot() method: In[23]: scalars_wide = scalars . pivot ( 'run' , columns = 'qname' , values = 'value' ) scalars_wide . head () Out[23]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qname Aloha.server.channelUtilization:last Aloha.server.collidedFrames:last Aloha.server.collisionLength:max Aloha.server.collisionLength:mean Aloha.server.collisionLength:sum Aloha.server.duration Aloha.server.receivedFrames:last iaMean numHosts run PureAlohaExperiment-0-20170627-20:42:16-22739 0.156057 40805.0 0.901897 0.198275 2457.026781 5400.0 8496.0 1.0 10.0 PureAlohaExperiment-1-20170627-20:42:17-22739 0.156176 40692.0 0.958902 0.198088 2456.494983 5400.0 8503.0 1.0 10.0 PureAlohaExperiment-10-20170627-20:42:16-22741 0.109571 1760.0 0.326138 0.155154 126.450220 5400.0 5965.0 7.0 10.0 PureAlohaExperiment-11-20170627-20:42:16-22741 0.108992 1718.0 0.340096 0.154529 125.477252 5400.0 5934.0 7.0 10.0 PureAlohaExperiment-12-20170627-20:42:16-22741 0.090485 1069.0 0.272013 0.152142 78.201174 5400.0 4926.0 9.0 10.0 We are interested in only three columns for our plot: In[24]: scalars_wide [[ 'numHosts' , 'iaMean' , 'Aloha.server.channelUtilization:last' ]] . head () Out[24]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } qname numHosts iaMean Aloha.server.channelUtilization:last run PureAlohaExperiment-0-20170627-20:42:16-22739 10.0 1.0 0.156057 PureAlohaExperiment-1-20170627-20:42:17-22739 10.0 1.0 0.156176 PureAlohaExperiment-10-20170627-20:42:16-22741 10.0 7.0 0.109571 PureAlohaExperiment-11-20170627-20:42:16-22741 10.0 7.0 0.108992 PureAlohaExperiment-12-20170627-20:42:16-22741 10.0 9.0 0.090485 Since we have our x and y data in separate columns now, we can utilize the scatter plot feature of the data frame for plotting it: In[25]: # set the default image resolution and size plt . rcParams [ 'figure.figsize' ] = [ 8.0 , 3.0 ] plt . rcParams [ 'figure.dpi' ] = 144 # create a scatter plot scalars_wide . plot . scatter ( 'iaMean' , 'Aloha.server.channelUtilization:last' ) plt . show () Out[25]: NOTE: Although plt.show() is not needed in Jupyter ( %matplotlib inline turns on immediate display), we'll continue to include it in further code fragments, so that they work without change when you use another Python shell. The resulting chart looks quite good as the first attempt. However, it has some shortcomings: Dots are not connected. The dots that have the same numHosts value should be connected with iso lines. As the result of having two simulation runs for each (iaMean,numHosts) pair, the dots appear in pairs. We'd like to see their averages instead. Unfortunately, scatter plot can only take us this far, we need to look for another way. What we really need as chart input is a table where rows correspond to different iaMean values, columns correspond to different numHosts values, and cells contain channel utilization values (the average of the repetitions). Such table can be produced from the \"wide format\" with another pivoting operation. We use pivot_table() , a cousin of the pivot() method we've seen above. The difference between them is that pivot() is a reshaping operation (it just rearranges elements), while pivot_table() is more of a spreadsheet-style pivot table creation operation, and primarily intended for numerical data. pivot_table() accepts an aggregation function with the default being mean , which is quite convenient for us now (we want to average channel utilization over repetitions.) In[26]: aloha_pivot = scalars_wide . pivot_table ( index = 'iaMean' , columns = 'numHosts' , values = 'Aloha.server.channelUtilization:last' ) # note: aggregation function = mean (that's the default) aloha_pivot . head () Out[26]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } numHosts 10.0 15.0 20.0 iaMean 1.0 0.156116 0.089539 0.046586 2.0 0.194817 0.178159 0.147564 3.0 0.176321 0.191571 0.183976 4.0 0.153569 0.182324 0.190452 5.0 0.136997 0.168780 0.183742 Note that rows correspond to various iaMean values ( iaMean serves as index); there is one column for each value of numHosts ; and that data in the table are the averages of the channel utilizations produced by the simulations performed with the respective iaMean and numHosts values. For the plot, every column should generate a separate line (with the x values coming from the index column, iaMean ) labelled with the column name. The basic Matplotlib interface cannot create such plot in one step. However, the Pandas data frame itself has a plotting interface which knows how to interpret the data, and produces the correct plot without much convincing: In[27]: aloha_pivot . plot . line () plt . ylabel ( 'channel utilization' ) plt . show () Out[27]:","title":"7. Plotting scalars"},{"location":"tutorials/pandas/#8-interactive-pivot-tables","text":"Getting the pivot table right is not always easy, so having a GUI where one can drag columns around and immediately see the result is definitely a blessing. Pivottable.js presents such a GUI inside a browser, and although the bulk of the code is Javascript, it has a Python frond-end that integrates nicely with Jupyter. Let's try it! In[28]: import pivottablejs as pj pj . pivot_ui ( scalars_wide ) Out[28]: An interactive panel containing the pivot table will appear. Here is how you can reproduce the above \"Channel utilization vs iaMean\" plot in it: Drag numHosts to the \"rows\" area of the pivot table. The table itself is the area on the left that initially only displays \"Totals | 42\", and the \"rows\" area is the empty rectangle directly of left it. The table should show have two columns ( numHosts and Totals ) and five rows in total after dragging. Drag iaMean to the \"columns\" area (above the table). Columns for each value of iaMean should appear in the table. Near the top-left corner of the table, select Average from the combo box that originally displays Count , and select ChannelUtilization : last from the combo box that appears below it. In the top-left corner of the panel, select Line Chart from the combo box that originally displays Table . If you can't get to see it, the following command will programmatically configure the pivot table in the appropriate way: In[29]: pj . pivot_ui ( scalars_wide , rows = [ 'numHosts' ], cols = [ 'iaMean' ], vals = [ 'Aloha.server.channelUtilization:last' ], aggregatorName = 'Average' , rendererName = 'Line Chart' ) Out[29]: If you want experiment with Excel's or LibreOffice's built-in pivot table functionality, the data frame's to_clipboard() and to_csv() methods will help you transfer the data. For example, you can issue the scalars_wide.to_clipboard() command to put the data on the clipboard, then paste it into the spreadsheet. Alternatively, type print(scalars_wide.to_csv()) to print the data in CSV format that you can select and then copy/paste. Or, use scalars_wide.to_csv(\"scalars.csv\") to save the data into a file which you can import.","title":"8. Interactive pivot tables"},{"location":"tutorials/pandas/#9-plotting-histograms","text":"In this section we explore how to plot histograms recorded by the simulation. Histograms are in rows that have \"histogram\" in the type column. Histogram bin edges and bin values (counts) are in the binedges and binvalues columns as NumPy array objects ( ndarray ). Let us begin by selecting the histograms into a new data frame for convenience. In[30]: histograms = aloha [ aloha . type == 'histogram' ] len ( histograms ) Out[30]: 84 We have 84 histograms. It makes no sense to plot so many histograms on one chart, so let's just take one on them, and examine its content. In[31]: hist = histograms . iloc [ 0 ] # the first histogram hist . binedges , hist . binvalues Out[31]: (array([-0.11602833, -0.08732314, -0.05861794, -0.02991275, -0.00120756, 0.02749763, 0.05620283, 0.08490802, 0.11361321, 0.1423184 , 0.1710236 , 0.19972879, 0.22843398, 0.25713917, 0.28584437, 0.31454956, 0.34325475, 0.37195994, 0.40066514, 0.42937033, 0.45807552, 0.48678071, 0.51548591, 0.5441911 , 0.57289629, 0.60160148, 0.63030668, 0.65901187, 0.68771706, 0.71642225, 0.74512745]), array([ 0., 0., 0., 0., 0., 0., 0., 1234., 2372., 2180., 2115., 1212., 917., 663., 473., 353., 251., 186., 123., 99., 60., 44., 31., 25., 15., 13., 9., 3., 5., 3.])) The easiest way to plot the histogram from these two arrays is to look at it as a step function, and create a line plot with the appropriate drawing style. The only caveat is that we need to add an extra 0 element to draw the right side of the last histogram bin. In[32]: plt . plot ( hist . binedges , np . append ( hist . binvalues , 0 ), drawstyle = 'steps-post' ) # or maybe steps-mid, for integers plt . show () Out[32]: Another way to plot a recorded histogram is Matplotlib's hist() method, although that is a bit tricky. Instead of taking histogram data, hist() insists on computing the histogram itself from an array of values -- but we only have the histogram, and not the data it was originally computed from. Fortunately, hist() can accept a bin edges array, and another array as weights for the values. Thus, we can trick it into doing what we want by passing in our binedges array twice, once as bin edges and once as values, and specifying binvalues as weights. In[33]: plt . hist ( bins = hist . binedges , x = hist . binedges [: - 1 ], weights = hist . binvalues ) plt . show () Out[33]: hist() has some interesting options. For example, we can change the plotting style to be similar to a line plot by setting histtype='step' . To plot the normalized version of the histogram, specify density=True . To draw the cumulative density function, also specify cumulative=True . The following plot shows the effect of some of these options. In[34]: plt . hist ( bins = hist . binedges , x = hist . binedges [: - 1 ], weights = hist . binvalues , histtype = 'step' , density = True ) plt . show () Out[34]: To plot several histograms, we can iterate over the histograms and draw them one by one on the same plot. The following code does that, and also adds a legend and adjusts the bounds of the x axis. In[35]: somehistograms = histograms [ histograms . name == 'collisionLength:histogram' ][: 5 ] for row in somehistograms . itertuples (): plt . plot ( row . binedges , np . append ( row . binvalues , 0 ), drawstyle = 'steps-post' ) plt . legend ( somehistograms . module + \".\" + somehistograms . name ) plt . xlim ( 0 , 0.5 ) plt . show () Out[35]: Note, however, that the legend contains the same string for all histograms, which is not very meaningful. We could improve that by including some characteristics of the simulation that generated them, i.e. the number of hosts ( numHosts iteration variable) and frame interarrival times ( iaTime iteration variable). We'll see in the next section how that can be achieved.","title":"9. Plotting histograms"},{"location":"tutorials/pandas/#10-adding-iteration-variables-as-columns","text":"In this step, we add the iteration variables associated with the simulation run to the data frame as columns. There are several reasons why this is a good idea: they are very useful for generating the legends for plots of e.g. histograms and vectors (e.g. \"collision multiplicity histogram for numHosts=20 and iaMean=2s\"), and often needed as chart input as well. First, we select the iteration variables vars as a smaller data frame. In[36]: itervars_df = aloha . loc [ aloha . type == 'itervar' , [ 'run' , 'attrname' , 'attrvalue' ]] itervars_df . head () Out[36]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run attrname attrvalue 14 PureAlohaExperiment-4-20170627-20:42:20-22739 iaMean 3 15 PureAlohaExperiment-4-20170627-20:42:20-22739 numHosts 10 42 PureAlohaExperiment-3-20170627-20:42:20-22739 iaMean 2 43 PureAlohaExperiment-3-20170627-20:42:20-22739 numHosts 10 70 PureAlohaExperiment-0-20170627-20:42:16-22739 iaMean 1 We reshape the result by using the pivot() method. The following statement will convert unique values in the attrname column into separate columns: iaMean and numHosts . The new data frame will be indexed with the run id. In[37]: itervarspivot_df = itervars_df . pivot ( index = 'run' , columns = 'attrname' , values = 'attrvalue' ) itervarspivot_df . head () Out[37]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } attrname iaMean numHosts run PureAlohaExperiment-0-20170627-20:42:16-22739 1 10 PureAlohaExperiment-1-20170627-20:42:17-22739 1 10 PureAlohaExperiment-10-20170627-20:42:16-22741 7 10 PureAlohaExperiment-11-20170627-20:42:16-22741 7 10 PureAlohaExperiment-12-20170627-20:42:16-22741 9 10 Now, we only need to add the new columns back into the original dataframe, using merge() . This operation is not quite unlike an SQL join of two tables on the run column. In[38]: aloha2 = aloha . merge ( itervarspivot_df , left_on = 'run' , right_index = True , how = 'outer' ) aloha2 . head () Out[38]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue iaMean numHosts 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 For plot legends, it is also useful to have a single iterationvars column with string values like numHosts=10, iaMean=2 . This is easier than the above: we can just select the rows containing the run attribute named iterationvars (it contains exactly the string we need), take only the run and attrvalue columns, rename the attrvalue column to iterationvars , and then merge back the result into the original data frame in a way we did above. The selection and renaming step can be done as follows. (Note: we need .astype ( str ) in the condition so that rows where attrname is not filled in do not cause trouble.) In[39]: itervarscol_df = aloha . loc [( aloha . type == 'runattr' ) & ( aloha . attrname . astype ( str ) == 'iterationvars' ), [ 'run' , 'attrvalue' ]] itervarscol_df = itervarscol_df . rename ( columns = { 'attrvalue' : 'iterationvars' }) itervarscol_df . head () Out[39]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run iterationvars 4 PureAlohaExperiment-4-20170627-20:42:20-22739 numHosts=10, iaMean=3 32 PureAlohaExperiment-3-20170627-20:42:20-22739 numHosts=10, iaMean=2 60 PureAlohaExperiment-0-20170627-20:42:16-22739 numHosts=10, iaMean=1 88 PureAlohaExperiment-1-20170627-20:42:17-22739 numHosts=10, iaMean=1 116 PureAlohaExperiment-2-20170627-20:42:19-22739 numHosts=10, iaMean=2 In the merging step, we join the two tables (I mean, data frames) on the run column: In[40]: aloha3 = aloha2 . merge ( itervarscol_df , left_on = 'run' , right_on = 'run' , how = 'outer' ) aloha3 . head () Out[40]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } run type module name attrname attrvalue value count sumweights mean stddev min max binedges binvalues vectime vecvalue iaMean numHosts iterationvars 0 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN configname PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 1 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN datetime 20170627-20:42:20 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 2 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN experiment PureAlohaExperiment NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 3 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN inifile omnetpp.ini NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 4 PureAlohaExperiment-4-20170627-20:42:20-22739 runattr NaN NaN iterationvars numHosts=10, iaMean=3 NaN NaN NaN NaN NaN NaN NaN None None None None 3 10 numHosts=10, iaMean=3 To see the result of our work, let's try plotting the same histograms again, this time with a proper legend: In[41]: histograms = aloha3 [ aloha3 . type == 'histogram' ] somehistograms = histograms [ histograms . name == 'collisionLength:histogram' ][: 5 ] for row in somehistograms . itertuples (): plt . plot ( row . binedges , np . append ( row . binvalues , 0 ), drawstyle = 'steps-post' ) plt . title ( 'collisionLength:histogram' ) plt . legend ( somehistograms . iterationvars ) plt . xlim ( 0 , 0.5 ) plt . show () Out[41]:","title":"10. Adding iteration variables as columns"},{"location":"tutorials/pandas/#11-plotting-vectors","text":"This section deals with basic plotting of output vectors. Output vectors are basically time series data, but values have timestamps instead of being evenly spaced. Vectors are in rows that have \"vector\" in the type column. The values and their timestamps are in the vecvalue and vectime columns as NumPy array objects ( ndarray ). We'll use a different data set for exploring output vector plotting, one from the routing example simulation. There are pre-recorded result files in the samples/resultfiles/routing directory; change into it in the terminal, and issue the following command to convert them to CSV: scavetool x *.sca *.vec -o routing.csv Then we read the the CSV file into a data frame in the same way we saw with the aloha dataset: In[42]: routing = pd . read_csv ( 'routing.csv' , converters = { 'attrvalue' : parse_if_number , 'binedges' : parse_ndarray , 'binvalues' : parse_ndarray , 'vectime' : parse_ndarray , 'vecvalue' : parse_ndarray }) Let us begin by selecting the vectors into a new data frame for convenience. In[43]: vectors = routing [ routing . type == 'vector' ] len ( vectors ) Out[43]: 65 Our data frame contains results from one run. To get some idea what vectors we have, let's print the list unique vector names and module names: In[44]: vectors . name . unique (), vectors . module . unique () Out[44]: (array(['busy:vector', 'qlen:vector', 'txBytes:vector', 'endToEndDelay:vector', 'hopCount:vector', 'sourceAddress:vector', 'rxBytes:vector', 'drop:vector'], dtype=object), array(['Net5.rte[0].port$o[0].channel', 'Net5.rte[0].port$o[1].channel', 'Net5.rte[1].port$o[0].channel', 'Net5.rte[1].port$o[1].channel', 'Net5.rte[1].port$o[2].channel', 'Net5.rte[2].port$o[0].channel', 'Net5.rte[2].port$o[1].channel', 'Net5.rte[2].port$o[2].channel', 'Net5.rte[2].port$o[3].channel', 'Net5.rte[3].port$o[0].channel', 'Net5.rte[3].port$o[1].channel', 'Net5.rte[3].port$o[2].channel', 'Net5.rte[4].port$o[0].channel', 'Net5.rte[4].port$o[1].channel', 'Net5.rte[0].queue[0]', 'Net5.rte[0].queue[1]', 'Net5.rte[1].queue[0]', 'Net5.rte[1].queue[1]', 'Net5.rte[1].queue[2]', 'Net5.rte[2].queue[0]', 'Net5.rte[2].queue[1]', 'Net5.rte[2].queue[2]', 'Net5.rte[2].queue[3]', 'Net5.rte[3].queue[0]', 'Net5.rte[3].queue[1]', 'Net5.rte[3].queue[2]', 'Net5.rte[4].queue[0]', 'Net5.rte[4].queue[1]', 'Net5.rte[4].app', 'Net5.rte[1].app'], dtype=object)) A vector can be plotted on a line chart by simply passing the vectime and vecvalue arrays to plt.plot() : In[45]: vec = vectors [ vectors . name == 'qlen:vector' ] . iloc [ 4 ] # take some vector plt . plot ( vec . vectime , vec . vecvalue , drawstyle = 'steps-post' ) plt . xlim ( 0 , 100 ) plt . show () Out[45]: When several vectors need to be placed on the same plot, one can simply use a for loop. In[46]: somevectors = vectors [ vectors . name == 'qlen:vector' ][: 5 ] for row in somevectors . itertuples (): plt . plot ( row . vectime , row . vecvalue , drawstyle = 'steps-post' ) plt . title ( somevectors . name . values [ 0 ]) plt . legend ( somevectors . module ) plt . show () Out[46]:","title":"11. Plotting vectors"},{"location":"tutorials/pandas/#12-vector-filtering","text":"Plotting vectors \"as is\" is often not practical, as the result will be a crowded plot that's difficult to draw conclusions from. To remedy that, one can apply some kind of filtering before plotting, or plot a derived quantity such as the integral, sum or running average instead of the original. Such things can easily be achieved with the help of NumPy. Vector time and value are already stored in the data frame as NumPy arrays ( ndarray ), so we can apply NumPy functions to them. For example, let's try np.cumsum() which computes cumulative sum: In[47]: x = np . array ([ 8 , 2 , 1 , 5 , 7 ]) np . cumsum ( x ) Out[47]: array([ 8, 10, 11, 16, 23]) In[48]: for row in somevectors . itertuples (): plt . plot ( row . vectime , np . cumsum ( row . vecvalue )) plt . show () Out[48]: Plotting cumulative sum against time might be useful e.g. for an output vector where the simulation emits the packet length for each packet that has arrived at its destination. There, the sum would represent \"total bytes received\". Plotting the count against time for the same output vector would represent \"number of packets received\". For such a plot, we can utilize np.arange(1,n) which simply returns the numbers 1, 2, .., n-1 as an array: In[49]: for row in somevectors . itertuples (): plt . plot ( row . vectime , np . arange ( 1 , row . vecvalue . size + 1 ), '.-' , drawstyle = 'steps-post' ) plt . xlim ( 0 , 5 ); plt . ylim ( 0 , 20 ) plt . show () Out[49]: Note that we changed the plotting style to \"steps-post\", so that for any t time the plot accurately represents the number of values whose timestamp is less than or equal to t . As another warm-up exercise, let's plot the time interval that elapses between adjacent values; that is, for each element we want to plot the time difference between the that element and the previous one. This can be achieved by computing t [ 1 : ] - t [:- 1 ] , which is the elementwise subtraction of the t array and its shifted version. Array indexing starts at 0, so t[1:] means \"drop the first element\". Negative indices count from the end of the array, so t [:- 1 ] means \"without the last element\". The latter is necessary because the sizes of the two arrays must match. or convenience, we encapsulate the formula into a Python function: In[50]: def diff ( t ): return t [ 1 :] - t [: - 1 ] # example t = np . array ([ 0.1 , 1.5 , 1.6 , 2.0 , 3.1 ]) diff ( t ) Out[50]: array([1.4, 0.1, 0.4, 1.1]) We can now plot it. Note that as diff() makes the array one element shorter, we need to write row.vectime[1:] to drop the first element (it has no preceding element, so diff() cannot be computed for it.) Also, we use dots for plotting instead of lines, as it makes more sense here. In[51]: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], diff ( row . vectime ), 'o' ) plt . xlim ( 0 , 100 ) plt . show () Out[51]: We now know enough NumPy to be able to write a function that computes running average (a.k.a. \"mean filter\"). Let's try it out in a plot immediately. In[52]: def running_avg ( x ): return np . cumsum ( x ) / np . arange ( 1 , x . size + 1 ) # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime , running_avg ( row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Out[52]: For certain quantities such as queue length or on-off status, weighted average (with time intervals used as weights) makes more sense. Here is a function that computes running time-average: In[53]: def running_timeavg ( t , x ): dt = t [ 1 :] - t [: - 1 ] return np . cumsum ( x [: - 1 ] * dt ) / t [ 1 :] # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], running_timeavg ( row . vectime , row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Out[53]: Computing the integral of the vector as a step function is very similar to the running_timeavg() function. (Note: Computing integral in other ways is part of NumPy and SciPy, if you ever need it. For example, np.trapz(y,x) computes integral using the trapezoidal rule.) In[54]: def integrate_steps ( t , x ): dt = t [ 1 :] - t [: - 1 ] return np . cumsum ( x [: - 1 ] * dt ) # example plot: for row in somevectors . itertuples (): plt . plot ( row . vectime [ 1 :], integrate_steps ( row . vectime , row . vecvalue )) plt . show () Out[54]: As the last example in this section, here is a function that computes moving window average. It relies on the clever trick of subtracting the cumulative sum of the original vector from its shifted version to get the sum of values in every N -sized window. In[55]: def winavg ( x , N ): xpad = np . concatenate (( np . zeros ( N ), x )) # pad with zeroes s = np . cumsum ( xpad ) ss = s [ N :] - s [: - N ] ss [ N - 1 :] /= N ss [: N - 1 ] /= np . arange ( 1 , min ( N - 1 , ss . size ) + 1 ) return ss # example: for row in somevectors . itertuples (): plt . plot ( row . vectime , winavg ( row . vecvalue , 10 )) plt . xlim ( 0 , 200 ) plt . show () Out[55]: You can find further hints for smoothing the plot of an output vector in the signal processing chapter of the SciPy Cookbook (see References).","title":"12. Vector Filtering"},{"location":"tutorials/pandas/#resources","text":"The primary and authentic source of information on Pandas, Matplotlib and other libraries is their official documentation. I do not link them here because they are trivial to find via Google. Instead, here is a random collection of other resources that I found useful while writing this tutorial (not counting all the StackOverflow pages I visited.) Pandas tutorial from Greg Reda: http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/ On reshaping data frames: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping Matplotlib tutorial of Nicolas P. Rougier: https://www.labri.fr/perso/nrougier/teaching/matplotlib/ Creating boxplots with Matplotlib, from Bharat Bhole: http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/ SciPy Cookbook on signal smoothing: http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html Visual Guide on Pandas (video): https://www.youtube.com/watch?v=9d5-Ti6onew Python Pandas Cookbook (videos): https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3","title":"Resources"},{"location":"tutorials/pandas/#acknowledgements","text":"The author, Andras Varga would like to thank the participants of the 2016 OMNeT++ Summit for the valuable feedback, and especially Dr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.","title":"Acknowledgements"},{"location":"tutorials/pandas/index-src/","text":"Attention This tutorial is obsolete - it was written before OMNeT++ 6. In that version, the result analysis toolset was completely overhauled, already relying heavily on Python, NumPy, Pandas, and Matplotlib. Both graphical and command-line tools, as well as libraries usable from standalone Python scripts are available now, which are preferred over the methods described below. 1. When to use Python? \u00b6 The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of simulation results. If you are doing sophisticated result analysis, you will notice after a while that you have outgrown the IDE. The need for customized charts, the necessity of multi-step computations to produce chart input, or the sheer volume of raw simulation results might all be causes to make you look for something else. If you are an R or Matlab expert, you'll probably reach for those tools, but for everyone else, Python with the right libraries is pretty much the best choice. Python has a big momentum for data science, and in addition to having excellent libraries for data analysis and visualization, it is also a great general-purpose programming language. Python is used for diverse problems ranging from building desktop GUIs to machine learning and AI, so the knowledge you gain by learning it will be convertible to other areas. This tutorial will walk you through the initial steps of using Python for analysing simulation results, and shows how to do some of the most common tasks. The tutorial assumes that you have a working knowledge of OMNeT++ with regard to result recording, and basic familiarity with Python. 2. Setting up \u00b6 Before we can start, you need to install the necessary software. First, make sure you have Python, either version 2.x or 3.x (they are slightly incompatible.) If you have both versions available on your system, we recommend version 3.x. You also need OMNeT++ version 5.2 or later. We will heavily rely on three Python packages: NumPy , Pandas , and Matplotlib . There are also optional packages that will be useful for certain tasks: SciPy , PivotTable.js . We also recommend that you install IPython and Jupyter , because they let you work much more comfortably than the bare Python shell. On most systems, these packages can be installed with pip , the Python package manager (if you go for Python 3, replace pip with pip3 in the commands below): sudo pip install ipython jupyter sudo pip install numpy pandas matplotlib sudo pip install scipy pivottablejs As packages continually evolve, there might be incompatibilities between versions. We used the following versions when writing this tutorial: Pandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0. An easy way to determine which versions you have installed is using the pip list command. (Note that the last one is the version of the Python interface library, the PivotTable.js main Javascript library uses different version numbers, e.g. 2.7.0.) 3. Getting your simulation results into Python \u00b6 OMNeT++ result files have their own file format which is not directly digestible by Python. There are a number of ways to get your data inside Python: Export from the IDE. The Analysis Tool can export data in a number of formats, the ones that are useful here are CSV and Python-flavoured JSON. In this tutorial we'll use the CSV export, and read the result into Pandas using its read_csv() function. Export using scavetool. Exporting from the IDE may become tedious after a while, because you have to go through the GUI every time your simulations are re-run. Luckily, you can automate the exporting with OMNeT++'s scavetool program. scavetool exposes the same export functionality as the IDE, and also allows filtering of the data. Read the OMNeT++ result files directly from Python. Development of a Python package to read these files into Pandas data frames is underway, but given that these files are line-oriented text files with a straightforward and well-documented structure, writing your own custom reader is also a perfectly feasible option. SQLite. Since version 5.1, OMNeT++ has the ability to record simulation results int SQLite3 database files, which can be opened directly from Python using the sqlite package. This lets you use SQL queries to select the input data for your charts or computations, which is kind of cool! You can even use GUIs like SQLiteBrowser to browse the database and craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3 output, you'll still get .vec and .sca files as before, only their format will change from textual to SQLite's binary format. When querying the contents of the files, one issue to deal with is that SQLite does not allow cross-database queries, so you either need to configure OMNeT++ to record everything into one file (i.e. each run should append instead of creating a new file), or use scavetool's export functionality to merge the files into one. Custom result recording. There is also the option to instrument the simulation (via C++ code) or OMNeT++ (via custom result recorders) to produce files that Python can directly digest, e.g. CSV. However, in the light of the above options, it is rarely necessary to go this far. With large-scale simulation studies, it can easily happen that the full set of simulation results do not fit into the memory at once. There are also multiple approaches to deal with this problem: If you don't need all simulation results for the analysis, you can configure OMNeT++ to record only a subset of them. Fine-grained control is available. Perform filtering and aggregation steps before analysis. The IDE and scavetool are both capable of filtering the results before export. When the above approaches are not enough, it can help to move part of the result processing (typically, filtering and aggregation) into the simulation model as dedicated result collection modules. However, this solution requires significantly more work than the previous two, so use with care. In this tutorial, we'll work with the contents of the samples/resultfiles directory distributed with OMNeT++. The directory contains result files produced by the Aloha and Routing sample simulations, both of which are parameter studies. We'll start by looking at the Aloha results. As the first step, we use OMNeT++'s scavetool to convert Aloha's scalar files to CSV. Run the following commands in the terminal (replace ~/omnetpp with the location of your OMNeT++ installation): cd ~/omnetpp/samples/resultfiles/aloha scavetool x *.sca -o aloha.csv In the scavetool command line, x means export, and the export format is inferred from the output file's extension. (Note that scavetool supports two different CSV output formats. We need CSV Records , or CSV-R for short, which is the default for the .csv extension.) Let us spend a minute on what the export has created. The CSV file has a fixed number of columns named run , type , module , name , value , etc. Each result item, i.e. scalar, statistic, histogram and vector, produces one row of output in the CSV. Other items such as run attributes, iteration variables of the parameter study and result attributes also generate their own rows. The content of the type column determines what type of information a given row contains. The type column also determines which other columns are in use. For example, the binedges and binvalues columns are only filled in for histogram items. The colums are: run : Identifies the simulation run type : Row type, one of the following: scalar , vector , statistics , histogram , runattr , itervar , param , attr module : Hierarchical name (a.k.a. full path) of the module that recorded the result item name : Name of the result item (scalar, statistic, histogram or vector) attrname : Name of the run attribute or result item attribute (in the latter case, the module and name columns identify the result item the attribute belongs to) attrvalue : Value of run and result item attributes, iteration variables, saved ini param settings ( runattr , attr , itervar , param ) value : Output scalar value count , sumweights , mean , min , max , stddev : Fields of the statistics or histogram binedges , binvalues : Histogram bin edges and bin values, as space-separated lists. len(binedges)==len(binvalues)+1 vectime , vecvalue : Output vector time and value arrays, as space-separated lists When the export is done, you can start Jupyter server with the following command: jupyter notebook Open a web browser with the displayed URL to access the Jupyter GUI. Once there, choose New -> Python3 in the top right corner to open a blank notebook. The notebook allows you to enter Python commands or sequences of commands, run them, and view the output. Note that Enter simply inserts a newline; hit Ctrl+Enter to execute the commands in the current cell, or Alt+Enter to execute them and also insert a new cell below. If you cannot use Jupyter for some reason, a terminal-based Python shell ( python or ipython ) will also allow you to follow the tutorial. On the Python prompt, enter the following lines to make the functionality of Pandas, NumpPy and Matplotlib available in the session. The last, %matplotlib line is only needed for Jupyter. (It is a \"magic command\" that arranges plots to be displayed within the notebook.) import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline We utilize the read_csv() function to import the contents of the CSV file into a data frame. The data frame is the central concept of Pandas. We will continue to work with this data frame throughout the whole tutorial. aloha = pd.read_csv('aloha.csv') 4. Exploring the data frame \u00b6 You can view the contents of the data frame by simply entering the name of the variable ( aloha ). Alternatively, you can use the head() method of the data frame to view just the first few lines. aloha.head() You can see that the structure of the data frame, i.e. rows and columns, directly corresponds to the contents of the CSV file. Column names have been taken from the first line of the CSV file. Missing values are represented with NaNs (not-a-number). The complementary tail() method shows the last few lines. There is also an iloc method that we use at places in this tutorial to show rows from the middle of the data frame. It accepts a range: aloha.iloc[20:30] selects 10 lines from line 20, aloha.iloc[:5] is like head() , and aloha.iloc[-5:] is like tail() . aloha.iloc[1200:1205] Hint: If you are in the terminal and you find that the data frame printout does not make use of the whole width of the terminal, you can increase the display width for better readability with the following commands: pd.set_option('display.width', 180) pd.set_option('display.max_colwidth', 100) If you have not looked at any Pandas tutorial yet, now is a very good time to read one. (See References at the bottom of this page for hints.) Until you finish, here are some basics for your short-term survival. You can refer to a column as a whole with the array index syntax: aloha['run'] . Alternatively, the more convenient member access syntax ( aloha.run ) can also be used, with restrictions. (E.g. the column name must be valid as a Python identifier, and should not collide with existing methods of the data frame. Names that are known to cause trouble include name , min , max , mean ). aloha.run.head() # .head() is for limiting the output to 5 lines here Selecting multiple columns is also possible, one just needs to use a list of column names as index. The result will be another data frame. (The double brackets in the command are due to the fact that both the array indexing and the list syntax use square brackets.) tmp = aloha[['run', 'attrname', 'attrvalue']] tmp.head() The describe() method can be used to get an idea about the contents of a column. When applied to a non-numeric column, it prints the number of non-null elements in it ( count ), the number of unique values ( unique ), the most frequently occurring value ( top ) and its multiplicity ( freq ), and the inferred data type (more about that later.) aloha.module.describe() You can get a list of the unique values using the unique() method. For example, the following command lists the names of modules that have recorded any statistics: aloha.module.unique() When you apply describe() to a numeric column, you get a statistical summary with things like mean, standard deviation, minimum, maximum, and various quantiles. aloha.value.describe() Applying describe() to the whole data frame creates a similar report about all numeric columns. aloha.describe() Let's spend a minute on data types and column data types. Every column has a data type (abbreviated dtype ) that determines what type of values it may contain. Column dtypes can be printed with dtypes : aloha.dtypes The two most commonly used dtypes are float64 and object . A float64 column contains floating-point numbers, and missing values are represented with NaNs. An object column may contain basically anything -- usually strings, but we'll also have NumPy arrays ( np.ndarray ) as elements in this tutorial. Numeric values and booleans may also occur in an object column. Missing values in an object column are usually represented with None , but Pandas also interprets the floating-point NaN like that. Some degree of confusion arises from fact that some Pandas functions check the column's dtype, while others are already happy if the contained elements are of the required type. To clarify: applying describe() to a column prints a type inferred from the individual elements, not the column dtype. The column dtype type can be changed with the astype() method; we'll see an example for using it later in this tutorial. The column dtype can be accessed as the dtype property of a column, for example aloha.stddev.dtype yields dtype('float64') . There are also convenience functions such as is_numeric_dtype() and is_string_dtype() for checking column dtype. (They need to be imported from the pandas.api.types package though.) Another vital thing to know, especially due of the existence of the type column in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly, the array index syntax can be used here as well. For example, the following expression selects the rows that contain iteration variables: aloha[aloha.type == 'itervar'] . With a healthy degree of sloppiness, here's how it works: aloha.type yields the values in the type column as an array-like data structure; aloha.type=='itervar' performs element-wise comparison and produces an array of booleans containing True where the condition holds and False where not; and indexing a data frame with an array of booleans returns the rows that correspond to True values in the array. Conditions can be combined with AND/OR using the \" & \" and \" | \" operators, but you need parentheses because of operator precedence. The following command selects the rows that contain scalars with a certain name and owner module: tmp = aloha[(aloha.type=='scalar') & (aloha.module=='Aloha.server') & (aloha.name=='channelUtilization:last')] tmp.head() You'll also need to know how to add a new column to the data frame. Now that is a bit controversial topic, because at the time of writing, there is a \"convenient\" syntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple assignment, for example: aloha['qname'] = aloha.module + \".\" + aloha.name aloha[aloha.type=='scalar'].head() # print excerpt It looks nice and natural, but it is not entirely correct. It often results in a warning: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame... . The message essentially says that the operation (here, adding the new column) might have been applied to a temporary object instead of the original data frame, and thus might have been ineffective. Luckily, that is not the case most of the time (the operation does take effect). Nevertheless, for production code, i.e. scripts, the \"official\" solution, the assign() method of the data frame is recommended, like this: aloha = aloha.assign(qname = aloha.module + \".\" + aloha.name) aloha[aloha.type=='scalar'].head() For completeness, one can remove a column from a data frame using either the del operator or the drop() method of the data frame. Here we show the former (also to remove the column we added above, as we won't need it for now): del aloha['qname'] 5. Revisiting CSV loading \u00b6 The way we have read the CSV file has one small deficiency: all data in the attrvalue column are represented as strings, event though many of them are really numbers, for example the values of the iaMean and numHosts iteration variables. You can verify that by printing the unique values ( aloha.attrvalue.unique() -- it will print all values with quotes), or using the type() operator on an element: type( aloha[aloha.type=='scalar'].iloc[0].value ) The reason is that read_csv() infers data types of columns from the data it finds in them. Since the attrvalue column is shared by run attributes, result item attributes, iteration variables and some other types of rows, there are many non-numeric strings in it, and read_csv() decides that it is a string column. A similar issue arises with the binedges , binvalues , vectime , vecvalue columns. These columns contain lists of numbers separated by spaces, so they are read into strings as well. However, we would like to store them as NumPy arrays ( ndarray ) inside the data frame, because that's the form we can use in plots or as computation input. Luckily, read_csv() allows us to specify conversion functions for each column. So, armed with the following two short functions: def parse_if_number ( s ): try : return float ( s ) except : return True if s == \"true\" else False if s == \"false\" else s if s else None def parse_ndarray ( s ): return np . fromstring ( s , sep = ' ' ) if s else None we can read the CSV file again, this time with the correct conversions: aloha = pd.read_csv('aloha.csv', converters = { 'attrvalue': parse_if_number, 'binedges': parse_ndarray, 'binvalues': parse_ndarray, 'vectime': parse_ndarray, 'vecvalue': parse_ndarray}) You can verify the result e.g. by printing the unique values again. 6. Load-time filtering \u00b6 If the CSV file is large, you may want to skip certain columns or rows when reading it into memory. (File size is about the only valid reason for using load-time filtering, because you can also filter out or drop rows/columns from the data frame when it is already loaded.) To filter out columns, you need to specify in the usecols parameter the list of columns to keep: tmp = pd.read_csv('aloha.csv', usecols=['run', 'type', 'module', 'name', 'value']) There is no such direct support for filtering out rows based on their content, but we can implement it using the iterator API that reads the CSV file in chunks. We can filter each chunk before storing and finally concatenating them into a single data frame: iter = pd.read_csv('aloha.csv', iterator=True, chunksize=100) chunks = [ chunk[chunk['type']!='histogram'] for chunk in iter ] # discards type=='histogram' lines tmp = pd.concat(chunks) 7. Plotting scalars \u00b6 Scalars can serve as input for many different kinds of plots. Here we'll show how one can create a \"throughput versus offered load\" type plot. We will plot the channel utilization in the Aloha model in the function of the packet generation frequency. Channel utilization is also affected by the number of hosts in the network -- we want results belonging to the same number of hosts to form iso lines. Packet generation frequency and the number of hosts are present in the results as iteration variables named iaMean and numHosts ; channel utilization values are the channelUtilization : last scalars saved by the Aloha.server module. The data contains the results from two simulation runs for each (iaMean, numHosts) pair done with different seeds; we want to average them for the plot. The first few steps are fairly straightforward. We only need the scalars and the iteration variables from the data frame, so we filter out the rest. Then we create a qname column from other columns to hold the names of our variables: the names of scalars are in the module and name columns (we want to join them with a dot), and the names of iteration variables are in the attrname column. Since attrname is not filled in for scalar rows, we can take attrname as qname first, then fill in the holes with module.name . We use the combine_first() method for that: a.combine_first(b) fills the holes in a using the corresponding values from b . The similar issue arises with values: values of output scalars are in the value column, while that of iteration variables are in the attrvalue column. Since attrvalue is unfilled for scalar rows, we can again utilize combine_first() to merge two. There is one more catch: we need to change the dtype of the attrvalue to float64 , otherwise the resulting value column also becomes object dtype. (Luckily, all our iteration variables are numeric, so the dtype conversion is possible. In other simulations that contain non-numeric itervars, one needs to filter those out, force them into numeric values somehow, or find some other trick to make things work.) scalars = aloha[(aloha.type=='scalar') | (aloha.type=='itervar')] # filter rows scalars = scalars.assign(qname = scalars.attrname.combine_first(scalars.module + '.' + scalars.name)) # add qname column scalars.value = scalars.value.combine_first(scalars.attrvalue.astype('float64')) # merge value columns scalars[['run', 'type', 'qname', 'value', 'module', 'name', 'attrname']].iloc[80:90] # print an excerpt of the result To work further, it would be very convenient if we had a format where each simulation run corresponds to one row, and all variables produced by that run had their own columns. We can call it the wide format, and it can be produced using the pivot() method: scalars_wide = scalars.pivot('run', columns='qname', values='value') scalars_wide.head() We are interested in only three columns for our plot: scalars_wide[['numHosts', 'iaMean', 'Aloha.server.channelUtilization:last']].head() Since we have our x and y data in separate columns now, we can utilize the scatter plot feature of the data frame for plotting it: # set the default image resolution and size plt.rcParams['figure.figsize'] = [8.0, 3.0] plt.rcParams['figure.dpi'] = 144 # create a scatter plot scalars_wide.plot.scatter('iaMean', 'Aloha.server.channelUtilization:last') plt.show() NOTE: Although plt.show() is not needed in Jupyter ( %matplotlib inline turns on immediate display), we'll continue to include it in further code fragments, so that they work without change when you use another Python shell. The resulting chart looks quite good as the first attempt. However, it has some shortcomings: Dots are not connected. The dots that have the same numHosts value should be connected with iso lines. As the result of having two simulation runs for each (iaMean,numHosts) pair, the dots appear in pairs. We'd like to see their averages instead. Unfortunately, scatter plot can only take us this far, we need to look for another way. What we really need as chart input is a table where rows correspond to different iaMean values, columns correspond to different numHosts values, and cells contain channel utilization values (the average of the repetitions). Such table can be produced from the \"wide format\" with another pivoting operation. We use pivot_table() , a cousin of the pivot() method we've seen above. The difference between them is that pivot() is a reshaping operation (it just rearranges elements), while pivot_table() is more of a spreadsheet-style pivot table creation operation, and primarily intended for numerical data. pivot_table() accepts an aggregation function with the default being mean , which is quite convenient for us now (we want to average channel utilization over repetitions.) aloha_pivot = scalars_wide.pivot_table(index='iaMean', columns='numHosts', values='Aloha.server.channelUtilization:last') # note: aggregation function = mean (that's the default) aloha_pivot.head() Note that rows correspond to various iaMean values ( iaMean serves as index); there is one column for each value of numHosts ; and that data in the table are the averages of the channel utilizations produced by the simulations performed with the respective iaMean and numHosts values. For the plot, every column should generate a separate line (with the x values coming from the index column, iaMean ) labelled with the column name. The basic Matplotlib interface cannot create such plot in one step. However, the Pandas data frame itself has a plotting interface which knows how to interpret the data, and produces the correct plot without much convincing: aloha_pivot.plot.line() plt.ylabel('channel utilization') plt.show() 8. Interactive pivot tables \u00b6 Getting the pivot table right is not always easy, so having a GUI where one can drag columns around and immediately see the result is definitely a blessing. Pivottable.js presents such a GUI inside a browser, and although the bulk of the code is Javascript, it has a Python frond-end that integrates nicely with Jupyter. Let's try it! import pivottablejs as pj pj . pivot_ui ( scalars_wide ) An interactive panel containing the pivot table will appear. Here is how you can reproduce the above \"Channel utilization vs iaMean\" plot in it: Drag numHosts to the \"rows\" area of the pivot table. The table itself is the area on the left that initially only displays \"Totals | 42\", and the \"rows\" area is the empty rectangle directly of left it. The table should show have two columns ( numHosts and Totals ) and five rows in total after dragging. Drag iaMean to the \"columns\" area (above the table). Columns for each value of iaMean should appear in the table. Near the top-left corner of the table, select Average from the combo box that originally displays Count , and select ChannelUtilization : last from the combo box that appears below it. In the top-left corner of the panel, select Line Chart from the combo box that originally displays Table . If you can't get to see it, the following command will programmatically configure the pivot table in the appropriate way: pj.pivot_ui(scalars_wide, rows=['numHosts'], cols=['iaMean'], vals=['Aloha.server.channelUtilization:last'], aggregatorName='Average', rendererName='Line Chart') If you want experiment with Excel's or LibreOffice's built-in pivot table functionality, the data frame's to_clipboard() and to_csv() methods will help you transfer the data. For example, you can issue the scalars_wide.to_clipboard() command to put the data on the clipboard, then paste it into the spreadsheet. Alternatively, type print(scalars_wide.to_csv()) to print the data in CSV format that you can select and then copy/paste. Or, use scalars_wide.to_csv(\"scalars.csv\") to save the data into a file which you can import. 9. Plotting histograms \u00b6 In this section we explore how to plot histograms recorded by the simulation. Histograms are in rows that have \"histogram\" in the type column. Histogram bin edges and bin values (counts) are in the binedges and binvalues columns as NumPy array objects ( ndarray ). Let us begin by selecting the histograms into a new data frame for convenience. histograms = aloha[aloha.type=='histogram'] len(histograms) We have 84 histograms. It makes no sense to plot so many histograms on one chart, so let's just take one on them, and examine its content. hist = histograms.iloc[0] # the first histogram hist.binedges, hist.binvalues The easiest way to plot the histogram from these two arrays is to look at it as a step function, and create a line plot with the appropriate drawing style. The only caveat is that we need to add an extra 0 element to draw the right side of the last histogram bin. plt.plot(hist.binedges, np.append(hist.binvalues, 0), drawstyle='steps-post') # or maybe steps-mid, for integers plt.show() Another way to plot a recorded histogram is Matplotlib's hist() method, although that is a bit tricky. Instead of taking histogram data, hist() insists on computing the histogram itself from an array of values -- but we only have the histogram, and not the data it was originally computed from. Fortunately, hist() can accept a bin edges array, and another array as weights for the values. Thus, we can trick it into doing what we want by passing in our binedges array twice, once as bin edges and once as values, and specifying binvalues as weights. plt . hist ( bins = hist . binedges , x = hist . binedges [:- 1 ], weights = hist . binvalues ) plt . show () hist() has some interesting options. For example, we can change the plotting style to be similar to a line plot by setting histtype='step' . To plot the normalized version of the histogram, specify density=True . To draw the cumulative density function, also specify cumulative=True . The following plot shows the effect of some of these options. plt . hist ( bins = hist . binedges , x = hist . binedges [:- 1 ], weights = hist . binvalues , histtype='step' , density = True ) plt . show () To plot several histograms, we can iterate over the histograms and draw them one by one on the same plot. The following code does that, and also adds a legend and adjusts the bounds of the x axis. somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5] for row in somehistograms.itertuples(): plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post') plt.legend(somehistograms.module + \".\" + somehistograms.name) plt.xlim(0, 0.5) plt.show() Note, however, that the legend contains the same string for all histograms, which is not very meaningful. We could improve that by including some characteristics of the simulation that generated them, i.e. the number of hosts ( numHosts iteration variable) and frame interarrival times ( iaTime iteration variable). We'll see in the next section how that can be achieved. 10. Adding iteration variables as columns \u00b6 In this step, we add the iteration variables associated with the simulation run to the data frame as columns. There are several reasons why this is a good idea: they are very useful for generating the legends for plots of e.g. histograms and vectors (e.g. \"collision multiplicity histogram for numHosts=20 and iaMean=2s\"), and often needed as chart input as well. First, we select the iteration variables vars as a smaller data frame. itervars_df = aloha.loc[aloha.type=='itervar', ['run', 'attrname', 'attrvalue']] itervars_df.head() We reshape the result by using the pivot() method. The following statement will convert unique values in the attrname column into separate columns: iaMean and numHosts . The new data frame will be indexed with the run id. itervarspivot_df = itervars_df.pivot(index='run', columns='attrname', values='attrvalue') itervarspivot_df.head() Now, we only need to add the new columns back into the original dataframe, using merge() . This operation is not quite unlike an SQL join of two tables on the run column. aloha2 = aloha.merge(itervarspivot_df, left_on='run', right_index=True, how='outer') aloha2.head() For plot legends, it is also useful to have a single iterationvars column with string values like numHosts=10, iaMean=2 . This is easier than the above: we can just select the rows containing the run attribute named iterationvars (it contains exactly the string we need), take only the run and attrvalue columns, rename the attrvalue column to iterationvars , and then merge back the result into the original data frame in a way we did above. The selection and renaming step can be done as follows. (Note: we need .astype ( str ) in the condition so that rows where attrname is not filled in do not cause trouble.) itervarscol_df = aloha.loc[(aloha.type=='runattr') & (aloha.attrname.astype(str)=='iterationvars'), ['run', 'attrvalue']] itervarscol_df = itervarscol_df.rename(columns={'attrvalue': 'iterationvars'}) itervarscol_df.head() In the merging step, we join the two tables (I mean, data frames) on the run column: aloha3 = aloha2.merge(itervarscol_df, left_on='run', right_on='run', how='outer') aloha3.head() To see the result of our work, let's try plotting the same histograms again, this time with a proper legend: histograms = aloha3[aloha3.type=='histogram'] somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5] for row in somehistograms.itertuples(): plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post') plt.title('collisionLength:histogram') plt.legend(somehistograms.iterationvars) plt.xlim(0, 0.5) plt.show() 11. Plotting vectors \u00b6 This section deals with basic plotting of output vectors. Output vectors are basically time series data, but values have timestamps instead of being evenly spaced. Vectors are in rows that have \"vector\" in the type column. The values and their timestamps are in the vecvalue and vectime columns as NumPy array objects ( ndarray ). We'll use a different data set for exploring output vector plotting, one from the routing example simulation. There are pre-recorded result files in the samples/resultfiles/routing directory; change into it in the terminal, and issue the following command to convert them to CSV: scavetool x *.sca *.vec -o routing.csv Then we read the the CSV file into a data frame in the same way we saw with the aloha dataset: routing = pd.read_csv('routing.csv', converters = { 'attrvalue': parse_if_number, 'binedges': parse_ndarray, 'binvalues': parse_ndarray, 'vectime': parse_ndarray, 'vecvalue': parse_ndarray}) Let us begin by selecting the vectors into a new data frame for convenience. vectors = routing[routing.type=='vector'] len(vectors) Our data frame contains results from one run. To get some idea what vectors we have, let's print the list unique vector names and module names: vectors.name.unique(), vectors.module.unique() A vector can be plotted on a line chart by simply passing the vectime and vecvalue arrays to plt.plot() : vec = vectors[vectors.name == 'qlen:vector'].iloc[4] # take some vector plt.plot(vec.vectime, vec.vecvalue, drawstyle='steps-post') plt.xlim(0,100) plt.show() When several vectors need to be placed on the same plot, one can simply use a for loop. somevectors = vectors[vectors.name == 'qlen:vector'][:5] for row in somevectors.itertuples(): plt.plot(row.vectime, row.vecvalue, drawstyle='steps-post') plt.title(somevectors.name.values[0]) plt.legend(somevectors.module) plt.show() 12. Vector Filtering \u00b6 Plotting vectors \"as is\" is often not practical, as the result will be a crowded plot that's difficult to draw conclusions from. To remedy that, one can apply some kind of filtering before plotting, or plot a derived quantity such as the integral, sum or running average instead of the original. Such things can easily be achieved with the help of NumPy. Vector time and value are already stored in the data frame as NumPy arrays ( ndarray ), so we can apply NumPy functions to them. For example, let's try np.cumsum() which computes cumulative sum: x = np.array([8, 2, 1, 5, 7]) np.cumsum(x) for row in somevectors.itertuples(): plt.plot(row.vectime, np.cumsum(row.vecvalue)) plt.show() Plotting cumulative sum against time might be useful e.g. for an output vector where the simulation emits the packet length for each packet that has arrived at its destination. There, the sum would represent \"total bytes received\". Plotting the count against time for the same output vector would represent \"number of packets received\". For such a plot, we can utilize np.arange(1,n) which simply returns the numbers 1, 2, .., n-1 as an array: for row in somevectors.itertuples(): plt.plot(row.vectime, np.arange(1, row.vecvalue.size+1), '.-', drawstyle='steps-post') plt.xlim(0,5); plt.ylim(0,20) plt.show() Note that we changed the plotting style to \"steps-post\", so that for any t time the plot accurately represents the number of values whose timestamp is less than or equal to t . As another warm-up exercise, let's plot the time interval that elapses between adjacent values; that is, for each element we want to plot the time difference between the that element and the previous one. This can be achieved by computing t [ 1 : ] - t [:- 1 ] , which is the elementwise subtraction of the t array and its shifted version. Array indexing starts at 0, so t[1:] means \"drop the first element\". Negative indices count from the end of the array, so t [:- 1 ] means \"without the last element\". The latter is necessary because the sizes of the two arrays must match. or convenience, we encapsulate the formula into a Python function: def diff ( t ) : return t [ 1 : ] - t [:- 1 ] # example t = np . array ([ 0.1 , 1.5 , 1.6 , 2.0 , 3.1 ]) diff ( t ) We can now plot it. Note that as diff() makes the array one element shorter, we need to write row.vectime[1:] to drop the first element (it has no preceding element, so diff() cannot be computed for it.) Also, we use dots for plotting instead of lines, as it makes more sense here. for row in somevectors.itertuples(): plt.plot(row.vectime[1:], diff(row.vectime), 'o') plt.xlim(0,100) plt.show() We now know enough NumPy to be able to write a function that computes running average (a.k.a. \"mean filter\"). Let's try it out in a plot immediately. def running_avg ( x ): return np . cumsum ( x ) / np . arange ( 1 , x . size + 1 ) # example plot : for row in somevectors . itertuples (): plt . plot ( row . vectime , running_avg ( row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () For certain quantities such as queue length or on-off status, weighted average (with time intervals used as weights) makes more sense. Here is a function that computes running time-average: def running_timeavg ( t , x ) : dt = t [ 1 : ] - t [:- 1 ] return np . cumsum ( x [:- 1 ] * dt ) / t [ 1 : ] # example plot : for row in somevectors . itertuples () : plt . plot ( row . vectime [ 1 : ], running_timeavg ( row . vectime , row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Computing the integral of the vector as a step function is very similar to the running_timeavg() function. (Note: Computing integral in other ways is part of NumPy and SciPy, if you ever need it. For example, np.trapz(y,x) computes integral using the trapezoidal rule.) def integrate_steps ( t , x ) : dt = t [ 1 : ] - t [:- 1 ] return np . cumsum ( x [:- 1 ] * dt ) # example plot : for row in somevectors . itertuples () : plt . plot ( row . vectime [ 1 : ], integrate_steps ( row . vectime , row . vecvalue )) plt . show () As the last example in this section, here is a function that computes moving window average. It relies on the clever trick of subtracting the cumulative sum of the original vector from its shifted version to get the sum of values in every N -sized window. def winavg ( x , N ) : xpad = np . concatenate (( np . zeros ( N ), x )) # pad with zeroes s = np . cumsum ( xpad ) ss = s [ N : ] - s [:- N ] ss [ N - 1 : ] /= N ss [ : N - 1 ] /= np . arange ( 1 , min ( N - 1 , ss . size ) + 1 ) return ss # example : for row in somevectors . itertuples () : plt . plot ( row . vectime , winavg ( row . vecvalue , 10 )) plt . xlim ( 0 , 200 ) plt . show () You can find further hints for smoothing the plot of an output vector in the signal processing chapter of the SciPy Cookbook (see References). Resources \u00b6 The primary and authentic source of information on Pandas, Matplotlib and other libraries is their official documentation. I do not link them here because they are trivial to find via Google. Instead, here is a random collection of other resources that I found useful while writing this tutorial (not counting all the StackOverflow pages I visited.) Pandas tutorial from Greg Reda: http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/ On reshaping data frames: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping Matplotlib tutorial of Nicolas P. Rougier: https://www.labri.fr/perso/nrougier/teaching/matplotlib/ Creating boxplots with Matplotlib, from Bharat Bhole: http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/ SciPy Cookbook on signal smoothing: http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html Visual Guide on Pandas (video): https://www.youtube.com/watch?v=9d5-Ti6onew Python Pandas Cookbook (videos): https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3 Acknowledgements \u00b6 The author, Andras Varga would like to thank the participants of the 2016 OMNeT++ Summit for the valuable feedback, and especially Dr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.","title":"Index src"},{"location":"tutorials/pandas/index-src/#1-when-to-use-python","text":"The Analysis Tool in the OMNeT++ IDE is best suited for casual exploration of simulation results. If you are doing sophisticated result analysis, you will notice after a while that you have outgrown the IDE. The need for customized charts, the necessity of multi-step computations to produce chart input, or the sheer volume of raw simulation results might all be causes to make you look for something else. If you are an R or Matlab expert, you'll probably reach for those tools, but for everyone else, Python with the right libraries is pretty much the best choice. Python has a big momentum for data science, and in addition to having excellent libraries for data analysis and visualization, it is also a great general-purpose programming language. Python is used for diverse problems ranging from building desktop GUIs to machine learning and AI, so the knowledge you gain by learning it will be convertible to other areas. This tutorial will walk you through the initial steps of using Python for analysing simulation results, and shows how to do some of the most common tasks. The tutorial assumes that you have a working knowledge of OMNeT++ with regard to result recording, and basic familiarity with Python.","title":"1. When to use Python?"},{"location":"tutorials/pandas/index-src/#2-setting-up","text":"Before we can start, you need to install the necessary software. First, make sure you have Python, either version 2.x or 3.x (they are slightly incompatible.) If you have both versions available on your system, we recommend version 3.x. You also need OMNeT++ version 5.2 or later. We will heavily rely on three Python packages: NumPy , Pandas , and Matplotlib . There are also optional packages that will be useful for certain tasks: SciPy , PivotTable.js . We also recommend that you install IPython and Jupyter , because they let you work much more comfortably than the bare Python shell. On most systems, these packages can be installed with pip , the Python package manager (if you go for Python 3, replace pip with pip3 in the commands below): sudo pip install ipython jupyter sudo pip install numpy pandas matplotlib sudo pip install scipy pivottablejs As packages continually evolve, there might be incompatibilities between versions. We used the following versions when writing this tutorial: Pandas 0.20.2, NumPy 1.12.1, SciPy 0.19.1, Matplotlib 1.5.1, PivotTable.js 0.8.0. An easy way to determine which versions you have installed is using the pip list command. (Note that the last one is the version of the Python interface library, the PivotTable.js main Javascript library uses different version numbers, e.g. 2.7.0.)","title":"2. Setting up"},{"location":"tutorials/pandas/index-src/#3-getting-your-simulation-results-into-python","text":"OMNeT++ result files have their own file format which is not directly digestible by Python. There are a number of ways to get your data inside Python: Export from the IDE. The Analysis Tool can export data in a number of formats, the ones that are useful here are CSV and Python-flavoured JSON. In this tutorial we'll use the CSV export, and read the result into Pandas using its read_csv() function. Export using scavetool. Exporting from the IDE may become tedious after a while, because you have to go through the GUI every time your simulations are re-run. Luckily, you can automate the exporting with OMNeT++'s scavetool program. scavetool exposes the same export functionality as the IDE, and also allows filtering of the data. Read the OMNeT++ result files directly from Python. Development of a Python package to read these files into Pandas data frames is underway, but given that these files are line-oriented text files with a straightforward and well-documented structure, writing your own custom reader is also a perfectly feasible option. SQLite. Since version 5.1, OMNeT++ has the ability to record simulation results int SQLite3 database files, which can be opened directly from Python using the sqlite package. This lets you use SQL queries to select the input data for your charts or computations, which is kind of cool! You can even use GUIs like SQLiteBrowser to browse the database and craft your SELECT statements. Note: if you configure OMNeT++ for SQLite3 output, you'll still get .vec and .sca files as before, only their format will change from textual to SQLite's binary format. When querying the contents of the files, one issue to deal with is that SQLite does not allow cross-database queries, so you either need to configure OMNeT++ to record everything into one file (i.e. each run should append instead of creating a new file), or use scavetool's export functionality to merge the files into one. Custom result recording. There is also the option to instrument the simulation (via C++ code) or OMNeT++ (via custom result recorders) to produce files that Python can directly digest, e.g. CSV. However, in the light of the above options, it is rarely necessary to go this far. With large-scale simulation studies, it can easily happen that the full set of simulation results do not fit into the memory at once. There are also multiple approaches to deal with this problem: If you don't need all simulation results for the analysis, you can configure OMNeT++ to record only a subset of them. Fine-grained control is available. Perform filtering and aggregation steps before analysis. The IDE and scavetool are both capable of filtering the results before export. When the above approaches are not enough, it can help to move part of the result processing (typically, filtering and aggregation) into the simulation model as dedicated result collection modules. However, this solution requires significantly more work than the previous two, so use with care. In this tutorial, we'll work with the contents of the samples/resultfiles directory distributed with OMNeT++. The directory contains result files produced by the Aloha and Routing sample simulations, both of which are parameter studies. We'll start by looking at the Aloha results. As the first step, we use OMNeT++'s scavetool to convert Aloha's scalar files to CSV. Run the following commands in the terminal (replace ~/omnetpp with the location of your OMNeT++ installation): cd ~/omnetpp/samples/resultfiles/aloha scavetool x *.sca -o aloha.csv In the scavetool command line, x means export, and the export format is inferred from the output file's extension. (Note that scavetool supports two different CSV output formats. We need CSV Records , or CSV-R for short, which is the default for the .csv extension.) Let us spend a minute on what the export has created. The CSV file has a fixed number of columns named run , type , module , name , value , etc. Each result item, i.e. scalar, statistic, histogram and vector, produces one row of output in the CSV. Other items such as run attributes, iteration variables of the parameter study and result attributes also generate their own rows. The content of the type column determines what type of information a given row contains. The type column also determines which other columns are in use. For example, the binedges and binvalues columns are only filled in for histogram items. The colums are: run : Identifies the simulation run type : Row type, one of the following: scalar , vector , statistics , histogram , runattr , itervar , param , attr module : Hierarchical name (a.k.a. full path) of the module that recorded the result item name : Name of the result item (scalar, statistic, histogram or vector) attrname : Name of the run attribute or result item attribute (in the latter case, the module and name columns identify the result item the attribute belongs to) attrvalue : Value of run and result item attributes, iteration variables, saved ini param settings ( runattr , attr , itervar , param ) value : Output scalar value count , sumweights , mean , min , max , stddev : Fields of the statistics or histogram binedges , binvalues : Histogram bin edges and bin values, as space-separated lists. len(binedges)==len(binvalues)+1 vectime , vecvalue : Output vector time and value arrays, as space-separated lists When the export is done, you can start Jupyter server with the following command: jupyter notebook Open a web browser with the displayed URL to access the Jupyter GUI. Once there, choose New -> Python3 in the top right corner to open a blank notebook. The notebook allows you to enter Python commands or sequences of commands, run them, and view the output. Note that Enter simply inserts a newline; hit Ctrl+Enter to execute the commands in the current cell, or Alt+Enter to execute them and also insert a new cell below. If you cannot use Jupyter for some reason, a terminal-based Python shell ( python or ipython ) will also allow you to follow the tutorial. On the Python prompt, enter the following lines to make the functionality of Pandas, NumpPy and Matplotlib available in the session. The last, %matplotlib line is only needed for Jupyter. (It is a \"magic command\" that arranges plots to be displayed within the notebook.) import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline We utilize the read_csv() function to import the contents of the CSV file into a data frame. The data frame is the central concept of Pandas. We will continue to work with this data frame throughout the whole tutorial. aloha = pd.read_csv('aloha.csv')","title":"3. Getting your simulation results into Python"},{"location":"tutorials/pandas/index-src/#4-exploring-the-data-frame","text":"You can view the contents of the data frame by simply entering the name of the variable ( aloha ). Alternatively, you can use the head() method of the data frame to view just the first few lines. aloha.head() You can see that the structure of the data frame, i.e. rows and columns, directly corresponds to the contents of the CSV file. Column names have been taken from the first line of the CSV file. Missing values are represented with NaNs (not-a-number). The complementary tail() method shows the last few lines. There is also an iloc method that we use at places in this tutorial to show rows from the middle of the data frame. It accepts a range: aloha.iloc[20:30] selects 10 lines from line 20, aloha.iloc[:5] is like head() , and aloha.iloc[-5:] is like tail() . aloha.iloc[1200:1205] Hint: If you are in the terminal and you find that the data frame printout does not make use of the whole width of the terminal, you can increase the display width for better readability with the following commands: pd.set_option('display.width', 180) pd.set_option('display.max_colwidth', 100) If you have not looked at any Pandas tutorial yet, now is a very good time to read one. (See References at the bottom of this page for hints.) Until you finish, here are some basics for your short-term survival. You can refer to a column as a whole with the array index syntax: aloha['run'] . Alternatively, the more convenient member access syntax ( aloha.run ) can also be used, with restrictions. (E.g. the column name must be valid as a Python identifier, and should not collide with existing methods of the data frame. Names that are known to cause trouble include name , min , max , mean ). aloha.run.head() # .head() is for limiting the output to 5 lines here Selecting multiple columns is also possible, one just needs to use a list of column names as index. The result will be another data frame. (The double brackets in the command are due to the fact that both the array indexing and the list syntax use square brackets.) tmp = aloha[['run', 'attrname', 'attrvalue']] tmp.head() The describe() method can be used to get an idea about the contents of a column. When applied to a non-numeric column, it prints the number of non-null elements in it ( count ), the number of unique values ( unique ), the most frequently occurring value ( top ) and its multiplicity ( freq ), and the inferred data type (more about that later.) aloha.module.describe() You can get a list of the unique values using the unique() method. For example, the following command lists the names of modules that have recorded any statistics: aloha.module.unique() When you apply describe() to a numeric column, you get a statistical summary with things like mean, standard deviation, minimum, maximum, and various quantiles. aloha.value.describe() Applying describe() to the whole data frame creates a similar report about all numeric columns. aloha.describe() Let's spend a minute on data types and column data types. Every column has a data type (abbreviated dtype ) that determines what type of values it may contain. Column dtypes can be printed with dtypes : aloha.dtypes The two most commonly used dtypes are float64 and object . A float64 column contains floating-point numbers, and missing values are represented with NaNs. An object column may contain basically anything -- usually strings, but we'll also have NumPy arrays ( np.ndarray ) as elements in this tutorial. Numeric values and booleans may also occur in an object column. Missing values in an object column are usually represented with None , but Pandas also interprets the floating-point NaN like that. Some degree of confusion arises from fact that some Pandas functions check the column's dtype, while others are already happy if the contained elements are of the required type. To clarify: applying describe() to a column prints a type inferred from the individual elements, not the column dtype. The column dtype type can be changed with the astype() method; we'll see an example for using it later in this tutorial. The column dtype can be accessed as the dtype property of a column, for example aloha.stddev.dtype yields dtype('float64') . There are also convenience functions such as is_numeric_dtype() and is_string_dtype() for checking column dtype. (They need to be imported from the pandas.api.types package though.) Another vital thing to know, especially due of the existence of the type column in the OMNeT++ CSV format, is how to filter rows. Perhaps surprisingly, the array index syntax can be used here as well. For example, the following expression selects the rows that contain iteration variables: aloha[aloha.type == 'itervar'] . With a healthy degree of sloppiness, here's how it works: aloha.type yields the values in the type column as an array-like data structure; aloha.type=='itervar' performs element-wise comparison and produces an array of booleans containing True where the condition holds and False where not; and indexing a data frame with an array of booleans returns the rows that correspond to True values in the array. Conditions can be combined with AND/OR using the \" & \" and \" | \" operators, but you need parentheses because of operator precedence. The following command selects the rows that contain scalars with a certain name and owner module: tmp = aloha[(aloha.type=='scalar') & (aloha.module=='Aloha.server') & (aloha.name=='channelUtilization:last')] tmp.head() You'll also need to know how to add a new column to the data frame. Now that is a bit controversial topic, because at the time of writing, there is a \"convenient\" syntax and an \"official\" syntax for it. The \"convenient\" syntax is a simple assignment, for example: aloha['qname'] = aloha.module + \".\" + aloha.name aloha[aloha.type=='scalar'].head() # print excerpt It looks nice and natural, but it is not entirely correct. It often results in a warning: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame... . The message essentially says that the operation (here, adding the new column) might have been applied to a temporary object instead of the original data frame, and thus might have been ineffective. Luckily, that is not the case most of the time (the operation does take effect). Nevertheless, for production code, i.e. scripts, the \"official\" solution, the assign() method of the data frame is recommended, like this: aloha = aloha.assign(qname = aloha.module + \".\" + aloha.name) aloha[aloha.type=='scalar'].head() For completeness, one can remove a column from a data frame using either the del operator or the drop() method of the data frame. Here we show the former (also to remove the column we added above, as we won't need it for now): del aloha['qname']","title":"4. Exploring the data frame"},{"location":"tutorials/pandas/index-src/#5-revisiting-csv-loading","text":"The way we have read the CSV file has one small deficiency: all data in the attrvalue column are represented as strings, event though many of them are really numbers, for example the values of the iaMean and numHosts iteration variables. You can verify that by printing the unique values ( aloha.attrvalue.unique() -- it will print all values with quotes), or using the type() operator on an element: type( aloha[aloha.type=='scalar'].iloc[0].value ) The reason is that read_csv() infers data types of columns from the data it finds in them. Since the attrvalue column is shared by run attributes, result item attributes, iteration variables and some other types of rows, there are many non-numeric strings in it, and read_csv() decides that it is a string column. A similar issue arises with the binedges , binvalues , vectime , vecvalue columns. These columns contain lists of numbers separated by spaces, so they are read into strings as well. However, we would like to store them as NumPy arrays ( ndarray ) inside the data frame, because that's the form we can use in plots or as computation input. Luckily, read_csv() allows us to specify conversion functions for each column. So, armed with the following two short functions: def parse_if_number ( s ): try : return float ( s ) except : return True if s == \"true\" else False if s == \"false\" else s if s else None def parse_ndarray ( s ): return np . fromstring ( s , sep = ' ' ) if s else None we can read the CSV file again, this time with the correct conversions: aloha = pd.read_csv('aloha.csv', converters = { 'attrvalue': parse_if_number, 'binedges': parse_ndarray, 'binvalues': parse_ndarray, 'vectime': parse_ndarray, 'vecvalue': parse_ndarray}) You can verify the result e.g. by printing the unique values again.","title":"5. Revisiting CSV loading"},{"location":"tutorials/pandas/index-src/#6-load-time-filtering","text":"If the CSV file is large, you may want to skip certain columns or rows when reading it into memory. (File size is about the only valid reason for using load-time filtering, because you can also filter out or drop rows/columns from the data frame when it is already loaded.) To filter out columns, you need to specify in the usecols parameter the list of columns to keep: tmp = pd.read_csv('aloha.csv', usecols=['run', 'type', 'module', 'name', 'value']) There is no such direct support for filtering out rows based on their content, but we can implement it using the iterator API that reads the CSV file in chunks. We can filter each chunk before storing and finally concatenating them into a single data frame: iter = pd.read_csv('aloha.csv', iterator=True, chunksize=100) chunks = [ chunk[chunk['type']!='histogram'] for chunk in iter ] # discards type=='histogram' lines tmp = pd.concat(chunks)","title":"6. Load-time filtering"},{"location":"tutorials/pandas/index-src/#7-plotting-scalars","text":"Scalars can serve as input for many different kinds of plots. Here we'll show how one can create a \"throughput versus offered load\" type plot. We will plot the channel utilization in the Aloha model in the function of the packet generation frequency. Channel utilization is also affected by the number of hosts in the network -- we want results belonging to the same number of hosts to form iso lines. Packet generation frequency and the number of hosts are present in the results as iteration variables named iaMean and numHosts ; channel utilization values are the channelUtilization : last scalars saved by the Aloha.server module. The data contains the results from two simulation runs for each (iaMean, numHosts) pair done with different seeds; we want to average them for the plot. The first few steps are fairly straightforward. We only need the scalars and the iteration variables from the data frame, so we filter out the rest. Then we create a qname column from other columns to hold the names of our variables: the names of scalars are in the module and name columns (we want to join them with a dot), and the names of iteration variables are in the attrname column. Since attrname is not filled in for scalar rows, we can take attrname as qname first, then fill in the holes with module.name . We use the combine_first() method for that: a.combine_first(b) fills the holes in a using the corresponding values from b . The similar issue arises with values: values of output scalars are in the value column, while that of iteration variables are in the attrvalue column. Since attrvalue is unfilled for scalar rows, we can again utilize combine_first() to merge two. There is one more catch: we need to change the dtype of the attrvalue to float64 , otherwise the resulting value column also becomes object dtype. (Luckily, all our iteration variables are numeric, so the dtype conversion is possible. In other simulations that contain non-numeric itervars, one needs to filter those out, force them into numeric values somehow, or find some other trick to make things work.) scalars = aloha[(aloha.type=='scalar') | (aloha.type=='itervar')] # filter rows scalars = scalars.assign(qname = scalars.attrname.combine_first(scalars.module + '.' + scalars.name)) # add qname column scalars.value = scalars.value.combine_first(scalars.attrvalue.astype('float64')) # merge value columns scalars[['run', 'type', 'qname', 'value', 'module', 'name', 'attrname']].iloc[80:90] # print an excerpt of the result To work further, it would be very convenient if we had a format where each simulation run corresponds to one row, and all variables produced by that run had their own columns. We can call it the wide format, and it can be produced using the pivot() method: scalars_wide = scalars.pivot('run', columns='qname', values='value') scalars_wide.head() We are interested in only three columns for our plot: scalars_wide[['numHosts', 'iaMean', 'Aloha.server.channelUtilization:last']].head() Since we have our x and y data in separate columns now, we can utilize the scatter plot feature of the data frame for plotting it: # set the default image resolution and size plt.rcParams['figure.figsize'] = [8.0, 3.0] plt.rcParams['figure.dpi'] = 144 # create a scatter plot scalars_wide.plot.scatter('iaMean', 'Aloha.server.channelUtilization:last') plt.show() NOTE: Although plt.show() is not needed in Jupyter ( %matplotlib inline turns on immediate display), we'll continue to include it in further code fragments, so that they work without change when you use another Python shell. The resulting chart looks quite good as the first attempt. However, it has some shortcomings: Dots are not connected. The dots that have the same numHosts value should be connected with iso lines. As the result of having two simulation runs for each (iaMean,numHosts) pair, the dots appear in pairs. We'd like to see their averages instead. Unfortunately, scatter plot can only take us this far, we need to look for another way. What we really need as chart input is a table where rows correspond to different iaMean values, columns correspond to different numHosts values, and cells contain channel utilization values (the average of the repetitions). Such table can be produced from the \"wide format\" with another pivoting operation. We use pivot_table() , a cousin of the pivot() method we've seen above. The difference between them is that pivot() is a reshaping operation (it just rearranges elements), while pivot_table() is more of a spreadsheet-style pivot table creation operation, and primarily intended for numerical data. pivot_table() accepts an aggregation function with the default being mean , which is quite convenient for us now (we want to average channel utilization over repetitions.) aloha_pivot = scalars_wide.pivot_table(index='iaMean', columns='numHosts', values='Aloha.server.channelUtilization:last') # note: aggregation function = mean (that's the default) aloha_pivot.head() Note that rows correspond to various iaMean values ( iaMean serves as index); there is one column for each value of numHosts ; and that data in the table are the averages of the channel utilizations produced by the simulations performed with the respective iaMean and numHosts values. For the plot, every column should generate a separate line (with the x values coming from the index column, iaMean ) labelled with the column name. The basic Matplotlib interface cannot create such plot in one step. However, the Pandas data frame itself has a plotting interface which knows how to interpret the data, and produces the correct plot without much convincing: aloha_pivot.plot.line() plt.ylabel('channel utilization') plt.show()","title":"7. Plotting scalars"},{"location":"tutorials/pandas/index-src/#8-interactive-pivot-tables","text":"Getting the pivot table right is not always easy, so having a GUI where one can drag columns around and immediately see the result is definitely a blessing. Pivottable.js presents such a GUI inside a browser, and although the bulk of the code is Javascript, it has a Python frond-end that integrates nicely with Jupyter. Let's try it! import pivottablejs as pj pj . pivot_ui ( scalars_wide ) An interactive panel containing the pivot table will appear. Here is how you can reproduce the above \"Channel utilization vs iaMean\" plot in it: Drag numHosts to the \"rows\" area of the pivot table. The table itself is the area on the left that initially only displays \"Totals | 42\", and the \"rows\" area is the empty rectangle directly of left it. The table should show have two columns ( numHosts and Totals ) and five rows in total after dragging. Drag iaMean to the \"columns\" area (above the table). Columns for each value of iaMean should appear in the table. Near the top-left corner of the table, select Average from the combo box that originally displays Count , and select ChannelUtilization : last from the combo box that appears below it. In the top-left corner of the panel, select Line Chart from the combo box that originally displays Table . If you can't get to see it, the following command will programmatically configure the pivot table in the appropriate way: pj.pivot_ui(scalars_wide, rows=['numHosts'], cols=['iaMean'], vals=['Aloha.server.channelUtilization:last'], aggregatorName='Average', rendererName='Line Chart') If you want experiment with Excel's or LibreOffice's built-in pivot table functionality, the data frame's to_clipboard() and to_csv() methods will help you transfer the data. For example, you can issue the scalars_wide.to_clipboard() command to put the data on the clipboard, then paste it into the spreadsheet. Alternatively, type print(scalars_wide.to_csv()) to print the data in CSV format that you can select and then copy/paste. Or, use scalars_wide.to_csv(\"scalars.csv\") to save the data into a file which you can import.","title":"8. Interactive pivot tables"},{"location":"tutorials/pandas/index-src/#9-plotting-histograms","text":"In this section we explore how to plot histograms recorded by the simulation. Histograms are in rows that have \"histogram\" in the type column. Histogram bin edges and bin values (counts) are in the binedges and binvalues columns as NumPy array objects ( ndarray ). Let us begin by selecting the histograms into a new data frame for convenience. histograms = aloha[aloha.type=='histogram'] len(histograms) We have 84 histograms. It makes no sense to plot so many histograms on one chart, so let's just take one on them, and examine its content. hist = histograms.iloc[0] # the first histogram hist.binedges, hist.binvalues The easiest way to plot the histogram from these two arrays is to look at it as a step function, and create a line plot with the appropriate drawing style. The only caveat is that we need to add an extra 0 element to draw the right side of the last histogram bin. plt.plot(hist.binedges, np.append(hist.binvalues, 0), drawstyle='steps-post') # or maybe steps-mid, for integers plt.show() Another way to plot a recorded histogram is Matplotlib's hist() method, although that is a bit tricky. Instead of taking histogram data, hist() insists on computing the histogram itself from an array of values -- but we only have the histogram, and not the data it was originally computed from. Fortunately, hist() can accept a bin edges array, and another array as weights for the values. Thus, we can trick it into doing what we want by passing in our binedges array twice, once as bin edges and once as values, and specifying binvalues as weights. plt . hist ( bins = hist . binedges , x = hist . binedges [:- 1 ], weights = hist . binvalues ) plt . show () hist() has some interesting options. For example, we can change the plotting style to be similar to a line plot by setting histtype='step' . To plot the normalized version of the histogram, specify density=True . To draw the cumulative density function, also specify cumulative=True . The following plot shows the effect of some of these options. plt . hist ( bins = hist . binedges , x = hist . binedges [:- 1 ], weights = hist . binvalues , histtype='step' , density = True ) plt . show () To plot several histograms, we can iterate over the histograms and draw them one by one on the same plot. The following code does that, and also adds a legend and adjusts the bounds of the x axis. somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5] for row in somehistograms.itertuples(): plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post') plt.legend(somehistograms.module + \".\" + somehistograms.name) plt.xlim(0, 0.5) plt.show() Note, however, that the legend contains the same string for all histograms, which is not very meaningful. We could improve that by including some characteristics of the simulation that generated them, i.e. the number of hosts ( numHosts iteration variable) and frame interarrival times ( iaTime iteration variable). We'll see in the next section how that can be achieved.","title":"9. Plotting histograms"},{"location":"tutorials/pandas/index-src/#10-adding-iteration-variables-as-columns","text":"In this step, we add the iteration variables associated with the simulation run to the data frame as columns. There are several reasons why this is a good idea: they are very useful for generating the legends for plots of e.g. histograms and vectors (e.g. \"collision multiplicity histogram for numHosts=20 and iaMean=2s\"), and often needed as chart input as well. First, we select the iteration variables vars as a smaller data frame. itervars_df = aloha.loc[aloha.type=='itervar', ['run', 'attrname', 'attrvalue']] itervars_df.head() We reshape the result by using the pivot() method. The following statement will convert unique values in the attrname column into separate columns: iaMean and numHosts . The new data frame will be indexed with the run id. itervarspivot_df = itervars_df.pivot(index='run', columns='attrname', values='attrvalue') itervarspivot_df.head() Now, we only need to add the new columns back into the original dataframe, using merge() . This operation is not quite unlike an SQL join of two tables on the run column. aloha2 = aloha.merge(itervarspivot_df, left_on='run', right_index=True, how='outer') aloha2.head() For plot legends, it is also useful to have a single iterationvars column with string values like numHosts=10, iaMean=2 . This is easier than the above: we can just select the rows containing the run attribute named iterationvars (it contains exactly the string we need), take only the run and attrvalue columns, rename the attrvalue column to iterationvars , and then merge back the result into the original data frame in a way we did above. The selection and renaming step can be done as follows. (Note: we need .astype ( str ) in the condition so that rows where attrname is not filled in do not cause trouble.) itervarscol_df = aloha.loc[(aloha.type=='runattr') & (aloha.attrname.astype(str)=='iterationvars'), ['run', 'attrvalue']] itervarscol_df = itervarscol_df.rename(columns={'attrvalue': 'iterationvars'}) itervarscol_df.head() In the merging step, we join the two tables (I mean, data frames) on the run column: aloha3 = aloha2.merge(itervarscol_df, left_on='run', right_on='run', how='outer') aloha3.head() To see the result of our work, let's try plotting the same histograms again, this time with a proper legend: histograms = aloha3[aloha3.type=='histogram'] somehistograms = histograms[histograms.name == 'collisionLength:histogram'][:5] for row in somehistograms.itertuples(): plt.plot(row.binedges, np.append(row.binvalues, 0), drawstyle='steps-post') plt.title('collisionLength:histogram') plt.legend(somehistograms.iterationvars) plt.xlim(0, 0.5) plt.show()","title":"10. Adding iteration variables as columns"},{"location":"tutorials/pandas/index-src/#11-plotting-vectors","text":"This section deals with basic plotting of output vectors. Output vectors are basically time series data, but values have timestamps instead of being evenly spaced. Vectors are in rows that have \"vector\" in the type column. The values and their timestamps are in the vecvalue and vectime columns as NumPy array objects ( ndarray ). We'll use a different data set for exploring output vector plotting, one from the routing example simulation. There are pre-recorded result files in the samples/resultfiles/routing directory; change into it in the terminal, and issue the following command to convert them to CSV: scavetool x *.sca *.vec -o routing.csv Then we read the the CSV file into a data frame in the same way we saw with the aloha dataset: routing = pd.read_csv('routing.csv', converters = { 'attrvalue': parse_if_number, 'binedges': parse_ndarray, 'binvalues': parse_ndarray, 'vectime': parse_ndarray, 'vecvalue': parse_ndarray}) Let us begin by selecting the vectors into a new data frame for convenience. vectors = routing[routing.type=='vector'] len(vectors) Our data frame contains results from one run. To get some idea what vectors we have, let's print the list unique vector names and module names: vectors.name.unique(), vectors.module.unique() A vector can be plotted on a line chart by simply passing the vectime and vecvalue arrays to plt.plot() : vec = vectors[vectors.name == 'qlen:vector'].iloc[4] # take some vector plt.plot(vec.vectime, vec.vecvalue, drawstyle='steps-post') plt.xlim(0,100) plt.show() When several vectors need to be placed on the same plot, one can simply use a for loop. somevectors = vectors[vectors.name == 'qlen:vector'][:5] for row in somevectors.itertuples(): plt.plot(row.vectime, row.vecvalue, drawstyle='steps-post') plt.title(somevectors.name.values[0]) plt.legend(somevectors.module) plt.show()","title":"11. Plotting vectors"},{"location":"tutorials/pandas/index-src/#12-vector-filtering","text":"Plotting vectors \"as is\" is often not practical, as the result will be a crowded plot that's difficult to draw conclusions from. To remedy that, one can apply some kind of filtering before plotting, or plot a derived quantity such as the integral, sum or running average instead of the original. Such things can easily be achieved with the help of NumPy. Vector time and value are already stored in the data frame as NumPy arrays ( ndarray ), so we can apply NumPy functions to them. For example, let's try np.cumsum() which computes cumulative sum: x = np.array([8, 2, 1, 5, 7]) np.cumsum(x) for row in somevectors.itertuples(): plt.plot(row.vectime, np.cumsum(row.vecvalue)) plt.show() Plotting cumulative sum against time might be useful e.g. for an output vector where the simulation emits the packet length for each packet that has arrived at its destination. There, the sum would represent \"total bytes received\". Plotting the count against time for the same output vector would represent \"number of packets received\". For such a plot, we can utilize np.arange(1,n) which simply returns the numbers 1, 2, .., n-1 as an array: for row in somevectors.itertuples(): plt.plot(row.vectime, np.arange(1, row.vecvalue.size+1), '.-', drawstyle='steps-post') plt.xlim(0,5); plt.ylim(0,20) plt.show() Note that we changed the plotting style to \"steps-post\", so that for any t time the plot accurately represents the number of values whose timestamp is less than or equal to t . As another warm-up exercise, let's plot the time interval that elapses between adjacent values; that is, for each element we want to plot the time difference between the that element and the previous one. This can be achieved by computing t [ 1 : ] - t [:- 1 ] , which is the elementwise subtraction of the t array and its shifted version. Array indexing starts at 0, so t[1:] means \"drop the first element\". Negative indices count from the end of the array, so t [:- 1 ] means \"without the last element\". The latter is necessary because the sizes of the two arrays must match. or convenience, we encapsulate the formula into a Python function: def diff ( t ) : return t [ 1 : ] - t [:- 1 ] # example t = np . array ([ 0.1 , 1.5 , 1.6 , 2.0 , 3.1 ]) diff ( t ) We can now plot it. Note that as diff() makes the array one element shorter, we need to write row.vectime[1:] to drop the first element (it has no preceding element, so diff() cannot be computed for it.) Also, we use dots for plotting instead of lines, as it makes more sense here. for row in somevectors.itertuples(): plt.plot(row.vectime[1:], diff(row.vectime), 'o') plt.xlim(0,100) plt.show() We now know enough NumPy to be able to write a function that computes running average (a.k.a. \"mean filter\"). Let's try it out in a plot immediately. def running_avg ( x ): return np . cumsum ( x ) / np . arange ( 1 , x . size + 1 ) # example plot : for row in somevectors . itertuples (): plt . plot ( row . vectime , running_avg ( row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () For certain quantities such as queue length or on-off status, weighted average (with time intervals used as weights) makes more sense. Here is a function that computes running time-average: def running_timeavg ( t , x ) : dt = t [ 1 : ] - t [:- 1 ] return np . cumsum ( x [:- 1 ] * dt ) / t [ 1 : ] # example plot : for row in somevectors . itertuples () : plt . plot ( row . vectime [ 1 : ], running_timeavg ( row . vectime , row . vecvalue )) plt . xlim ( 0 , 100 ) plt . show () Computing the integral of the vector as a step function is very similar to the running_timeavg() function. (Note: Computing integral in other ways is part of NumPy and SciPy, if you ever need it. For example, np.trapz(y,x) computes integral using the trapezoidal rule.) def integrate_steps ( t , x ) : dt = t [ 1 : ] - t [:- 1 ] return np . cumsum ( x [:- 1 ] * dt ) # example plot : for row in somevectors . itertuples () : plt . plot ( row . vectime [ 1 : ], integrate_steps ( row . vectime , row . vecvalue )) plt . show () As the last example in this section, here is a function that computes moving window average. It relies on the clever trick of subtracting the cumulative sum of the original vector from its shifted version to get the sum of values in every N -sized window. def winavg ( x , N ) : xpad = np . concatenate (( np . zeros ( N ), x )) # pad with zeroes s = np . cumsum ( xpad ) ss = s [ N : ] - s [:- N ] ss [ N - 1 : ] /= N ss [ : N - 1 ] /= np . arange ( 1 , min ( N - 1 , ss . size ) + 1 ) return ss # example : for row in somevectors . itertuples () : plt . plot ( row . vectime , winavg ( row . vecvalue , 10 )) plt . xlim ( 0 , 200 ) plt . show () You can find further hints for smoothing the plot of an output vector in the signal processing chapter of the SciPy Cookbook (see References).","title":"12. Vector Filtering"},{"location":"tutorials/pandas/index-src/#resources","text":"The primary and authentic source of information on Pandas, Matplotlib and other libraries is their official documentation. I do not link them here because they are trivial to find via Google. Instead, here is a random collection of other resources that I found useful while writing this tutorial (not counting all the StackOverflow pages I visited.) Pandas tutorial from Greg Reda: http://www.gregreda.com/2013/10/26/working-with-pandas-dataframes/ On reshaping data frames: https://pandas.pydata.org/pandas-docs/stable/reshaping.html#reshaping Matplotlib tutorial of Nicolas P. Rougier: https://www.labri.fr/perso/nrougier/teaching/matplotlib/ Creating boxplots with Matplotlib, from Bharat Bhole: http://blog.bharatbhole.com/creating-boxplots-with-matplotlib/ SciPy Cookbook on signal smoothing: http://scipy-cookbook.readthedocs.io/items/SignalSmooth.html Visual Guide on Pandas (video): https://www.youtube.com/watch?v=9d5-Ti6onew Python Pandas Cookbook (videos): https://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3","title":"Resources"},{"location":"tutorials/pandas/index-src/#acknowledgements","text":"The author, Andras Varga would like to thank the participants of the 2016 OMNeT++ Summit for the valuable feedback, and especially Dr Kyeong Soo (Joseph) Kim for bringing my attention to Pandas and Jupyter.","title":"Acknowledgements"},{"location":"tutorials/swarm/","text":"Introduction \u00b6 This tutorial will show you how to utilize AWS, Amazon's cloud computing service, for running INET simulation campaigns. Following registration and minimal configuration on the AWS management interface, you'll be able to use a command-line tool for running simulations on AWS much like you'd run simulations locally. Our tool transparently takes care of submitting the simulation jobs to AWS, orchestrates their execution, and downloads the result files into the same local results/ folder where the locally running simulation would create them. Your simulation project needs to be published on GitHub, because AWS cloud nodes retrieve the source code of the simulation by checking it out from GitHub. The tool currently has some hardcoded assumptions about the project, so it only works with INET and INET forks. Work is underway to make the tool generic enough to run any simulation. Running the simulation on a cloud service incurs some overhead (retrieving the source of the simulation on cloud nodes, building it, distributing the binaries to all processors, and finally downloading the results to your own computer), so your simulation campaign needs to be large enough to benefit from cloud computing: it needs to consist of several simulation runs, and each run should be longer than at least a couple seconds (real time). Although AWS offers a Free Tier for trial purposes, the requirements for running INET simulations unfortunately exceeds the resource constraints of the Free Tier. However, AWS usage is very affordable (expect prices of around 1 USD for one hour of uptime), so it is still well worth it if you have simulation campaigns that take too long to complete on your locally available computing resources. (We are not affiliated with Amazon.) This solution utilizes Docker Swarm, and a couple of other services and technologies. In addition to scripts that manage the tasks closely associated with running simulations remotely, we also provide a command-line tool to make the management of the Swarm on AWS easier.","title":"Motivation"},{"location":"tutorials/swarm/#introduction","text":"This tutorial will show you how to utilize AWS, Amazon's cloud computing service, for running INET simulation campaigns. Following registration and minimal configuration on the AWS management interface, you'll be able to use a command-line tool for running simulations on AWS much like you'd run simulations locally. Our tool transparently takes care of submitting the simulation jobs to AWS, orchestrates their execution, and downloads the result files into the same local results/ folder where the locally running simulation would create them. Your simulation project needs to be published on GitHub, because AWS cloud nodes retrieve the source code of the simulation by checking it out from GitHub. The tool currently has some hardcoded assumptions about the project, so it only works with INET and INET forks. Work is underway to make the tool generic enough to run any simulation. Running the simulation on a cloud service incurs some overhead (retrieving the source of the simulation on cloud nodes, building it, distributing the binaries to all processors, and finally downloading the results to your own computer), so your simulation campaign needs to be large enough to benefit from cloud computing: it needs to consist of several simulation runs, and each run should be longer than at least a couple seconds (real time). Although AWS offers a Free Tier for trial purposes, the requirements for running INET simulations unfortunately exceeds the resource constraints of the Free Tier. However, AWS usage is very affordable (expect prices of around 1 USD for one hour of uptime), so it is still well worth it if you have simulation campaigns that take too long to complete on your locally available computing resources. (We are not affiliated with Amazon.) This solution utilizes Docker Swarm, and a couple of other services and technologies. In addition to scripts that manage the tasks closely associated with running simulations remotely, we also provide a command-line tool to make the management of the Swarm on AWS easier.","title":"Introduction"},{"location":"tutorials/swarm/page1/","text":"In this page we show you how to deploy and use the tools to run simulations on AWS. Caution The solution and the toolset presented here are currently experimental, and we are collecting feedback about it. Please let us know if you are interested, and if you try it, we request you to report bugs, errors or any difficulty you experience in using it. We'd also be happy to hear about successful usage. Use the bugtracker link at the bottom of this page for feedback. Installation \u00b6 Install Docker, python3 and pip3 on your local computer. We recommend that you use Linux. A working OMNeT++ installation, and a checked-out INET (fork) git repository is also necessary. Deployment on AWS \u00b6 Creating an AWS Account \u00b6 To access any web service AWS offers, you must first create an AWS account at http://aws.amazon.com . If you already have one, just log in to the AWS Management Console with it; otherwise, follow the instructions here: AWS registration . During registration, you will have to enter your credit card number. Amazon will charge it with a small amount (1 USD) to verify it. Creating the Access Policy \u00b6 On the AWS Management Console, navigate to the IAM Service configuration page, and switch to Policies . Or just click this link , it will take you there. Click the Create policy button. Switch over to the JSON tab. Paste the contents of this file into the entry field, replacing its entire contents. This policy is the superset of the officially published one on the Docker for AWS website . It had to be slightly altered to make it fit into the default size limit, so it grants slightly more privileges than necessary. It also adds the ec2 : CreateKeyPair and the cloudwatch : PutMetricAlarm permissions. These are necessary to automate the connection to the swarm, and the shutting down of the machines after they have been idle for a while. Click Review policy . Enter a Name for the new policy, for example \"inet-docker-swarm-policy\", then click Create policy . Creating the User \u00b6 Switch over to Users and click Add user . Enter a User name , for example \"inet-swarm-cli\", and tick the checkbox next to Programmatic access . Leave the other checkbox unchecked. Click Next: Permissions . Select Attach existing policies directly , search for the name of the policy we just created (by typing in a part of it, like \"inet\") , then check the checkboy next to it, and click Next: Review at the bottom. If everything looks alright, click Create user . As the final step of user creation, save the Access key ID and the Secret access key , somewhere safe. It's a good idea that you do this by clicking Download .csv . This will let you download this information into a simple text file, so you won't make any mistakes while copy-pasting them. Also, read the notice in the green area, particularly this part: \"This is the last time these credentials will be available to download. However, you can create new credentials at any time.\" This means that if you don't save the key ID and the secret key now, you will have to delete this user and create a new one. Important Keep these strings private, since these grant access to your account, without the need for your password or other credentials. (Of course, only until you delete this user, or revoke its permissions.) Treat them with similar caution as you do your passwords. Configuring CLI Access \u00b6 To let your computer manage and use your AWS account, we have to configure it with the credentials of the user you just created for it. First we need to install the AWS CLI utility using pip : $ pip3 install --user --upgrade awscli Then start the configuration: $ aws configure If at first you get aws : command not found , or The program 'aws' is currently not installed , try running ~/.local/bin/aws configure instead. When asked, enter the Access Key ID , then the Secret Access Key . They will be recorded into an INI file, which is by default at ~/.aws/credentials . For Default region , choose the one closest to you geographically. You can find the list of region codes and their locations here . In my case, the Frankfurt datacenter was closest, so I entered eu-central-1 . This setting is recorded in the ~/.aws/config file. You can find more info about Regions and Availability Zones here , here , and here . You can leave Default output format empty. Important Once all this information is entered correctly, any software you run on your computer has access to your AWS account, as permitted by the policy attached to the configured user. Remove (rename) the credentials file mentioned above to (temporarily) disable access. The proper way to completely and permanently revoke this access is to delete the IAM User we just created. From this point on in this tutorial, we won't need the AWS Management Console to initiate any actions. However, if you wish, you can use it to observe and check for yourself what the aws_swarm_tool.py script does. Installing Local Client Scripts \u00b6 Having set up your AWS account and access to it, we can proceed by installing the client-side tools for submitting simulations. First, install the required Python libraries: $ pip3 install --user --upgrade boto3 psutil pymongo redis rq Save the following files into ~/.local/bin : aws_swarm_tool.py docker-compose.yml inet_runall_swarm.py Make the script files executable, and put ~/.local/bin into the PATH if it does not already contain it: $ cd ~/.local/bin ; chmod +x aws_swarm_tool.py inet_runall_swarm.py $ export PATH = $PATH : $HOME /.local/bin Run the following command to set up the virtual machines and necessary infrastructure on AWS, and perform local configuration for accessing it: $ aws_swarm_tool.py init Usage \u00b6 Running Simulations \u00b6 Change into the directory under INET (or your INET fork) that contains your simulation. IMPORTANT: Your INET installation should be a checked-out copy of a GitHub repository with all changes pushed up to GitHub, because our tool only sends the Git URL of your project and the hash of the currently checked-out commit to AWS, not the full source code. Additionally, OMNeT++ should be ready to use, with its tools (like opp_run or opp_run_dbg ) accessible as commands. $ cd examples/inet/ber Enter the command for running the simulations, using our inet_runall_swarm.py program instead of ./run or inet : $ inet_runall_swarm.py -c ber-flavour-experiment The inet_runall_swarm.py tool will expand the list of simulation runs to be executed, submit them to the job queue, and wait for the jobs to finish. The results will be downloaded automatically into the results folder. You can monitor progress at http://localhost:9181/ which displays the content of the job queue. Stopping, Restarting and Deleting the Swarm \u00b6 Once you are done, you can stop the machines: $ aws_swarm_tool.py halt They will also shut down automatically after an hour of being idle. Important AWS usage is billed by uptime, i.e. you are charged for the time the Swarm is up and running, regardless whether you are actually using it for simulations or not. Our tool configures AWS to halt the Swarm after one hour of idle time, but for extra safety, double-check that the Swarm has been stopped after you have finished working with it. WE ARE NOT RESPONSIBLE FOR EXTRA COSTS CAUSED BY LEAVING AWS NODES RUNNING. Read on for instructions. The halt command only shuts down the virtual machine instances, but leaves all other resources intact, this way resuming the usage is faster than the initial deployment (with init ). These additional resources being there inactively do not cost you anything. To use the Swarm again, you have to start it up again: $ aws_swarm_tool.py resume To completely delete the entire Swarm: $ aws_swarm_tool.py delete Checking the Status Manually \u00b6 Important When you do not intend to work with AWS for an extended period of time, you may want to double-check manually that the stack on AWS has been stopped indeed, and you are no longer being billed for it. To check the status, select the EC2 service on the AWS Management Console, and check the Running Instances . If you see instances named inet-Node or inet-Manager , in the running state, the Swarm is active. To delete the Swarm manually (if the command-line tool does not work for some reason), go to the CloudFormation service page on the AWS Management Console. Tick the checkbox next to the inet Stack in the table, then click Actions > Delete Stack . This should remove all INET-related resources. Feedback and Discussion \u00b6 We are looking for any kind of feedback or suggestion you might have regarding this tutorial here: https://github.com/omnetpp/omnetpp-tutorials/issues/3","title":"Installation and Usage"},{"location":"tutorials/swarm/page1/#installation","text":"Install Docker, python3 and pip3 on your local computer. We recommend that you use Linux. A working OMNeT++ installation, and a checked-out INET (fork) git repository is also necessary.","title":"Installation"},{"location":"tutorials/swarm/page1/#deployment-on-aws","text":"","title":"Deployment on AWS"},{"location":"tutorials/swarm/page1/#creating-an-aws-account","text":"To access any web service AWS offers, you must first create an AWS account at http://aws.amazon.com . If you already have one, just log in to the AWS Management Console with it; otherwise, follow the instructions here: AWS registration . During registration, you will have to enter your credit card number. Amazon will charge it with a small amount (1 USD) to verify it.","title":"Creating an AWS Account"},{"location":"tutorials/swarm/page1/#creating-the-access-policy","text":"On the AWS Management Console, navigate to the IAM Service configuration page, and switch to Policies . Or just click this link , it will take you there. Click the Create policy button. Switch over to the JSON tab. Paste the contents of this file into the entry field, replacing its entire contents. This policy is the superset of the officially published one on the Docker for AWS website . It had to be slightly altered to make it fit into the default size limit, so it grants slightly more privileges than necessary. It also adds the ec2 : CreateKeyPair and the cloudwatch : PutMetricAlarm permissions. These are necessary to automate the connection to the swarm, and the shutting down of the machines after they have been idle for a while. Click Review policy . Enter a Name for the new policy, for example \"inet-docker-swarm-policy\", then click Create policy .","title":"Creating the Access Policy"},{"location":"tutorials/swarm/page1/#creating-the-user","text":"Switch over to Users and click Add user . Enter a User name , for example \"inet-swarm-cli\", and tick the checkbox next to Programmatic access . Leave the other checkbox unchecked. Click Next: Permissions . Select Attach existing policies directly , search for the name of the policy we just created (by typing in a part of it, like \"inet\") , then check the checkboy next to it, and click Next: Review at the bottom. If everything looks alright, click Create user . As the final step of user creation, save the Access key ID and the Secret access key , somewhere safe. It's a good idea that you do this by clicking Download .csv . This will let you download this information into a simple text file, so you won't make any mistakes while copy-pasting them. Also, read the notice in the green area, particularly this part: \"This is the last time these credentials will be available to download. However, you can create new credentials at any time.\" This means that if you don't save the key ID and the secret key now, you will have to delete this user and create a new one. Important Keep these strings private, since these grant access to your account, without the need for your password or other credentials. (Of course, only until you delete this user, or revoke its permissions.) Treat them with similar caution as you do your passwords.","title":"Creating the User"},{"location":"tutorials/swarm/page1/#configuring-cli-access","text":"To let your computer manage and use your AWS account, we have to configure it with the credentials of the user you just created for it. First we need to install the AWS CLI utility using pip : $ pip3 install --user --upgrade awscli Then start the configuration: $ aws configure If at first you get aws : command not found , or The program 'aws' is currently not installed , try running ~/.local/bin/aws configure instead. When asked, enter the Access Key ID , then the Secret Access Key . They will be recorded into an INI file, which is by default at ~/.aws/credentials . For Default region , choose the one closest to you geographically. You can find the list of region codes and their locations here . In my case, the Frankfurt datacenter was closest, so I entered eu-central-1 . This setting is recorded in the ~/.aws/config file. You can find more info about Regions and Availability Zones here , here , and here . You can leave Default output format empty. Important Once all this information is entered correctly, any software you run on your computer has access to your AWS account, as permitted by the policy attached to the configured user. Remove (rename) the credentials file mentioned above to (temporarily) disable access. The proper way to completely and permanently revoke this access is to delete the IAM User we just created. From this point on in this tutorial, we won't need the AWS Management Console to initiate any actions. However, if you wish, you can use it to observe and check for yourself what the aws_swarm_tool.py script does.","title":"Configuring CLI Access"},{"location":"tutorials/swarm/page1/#installing-local-client-scripts","text":"Having set up your AWS account and access to it, we can proceed by installing the client-side tools for submitting simulations. First, install the required Python libraries: $ pip3 install --user --upgrade boto3 psutil pymongo redis rq Save the following files into ~/.local/bin : aws_swarm_tool.py docker-compose.yml inet_runall_swarm.py Make the script files executable, and put ~/.local/bin into the PATH if it does not already contain it: $ cd ~/.local/bin ; chmod +x aws_swarm_tool.py inet_runall_swarm.py $ export PATH = $PATH : $HOME /.local/bin Run the following command to set up the virtual machines and necessary infrastructure on AWS, and perform local configuration for accessing it: $ aws_swarm_tool.py init","title":"Installing Local Client Scripts"},{"location":"tutorials/swarm/page1/#usage","text":"","title":"Usage"},{"location":"tutorials/swarm/page1/#running-simulations","text":"Change into the directory under INET (or your INET fork) that contains your simulation. IMPORTANT: Your INET installation should be a checked-out copy of a GitHub repository with all changes pushed up to GitHub, because our tool only sends the Git URL of your project and the hash of the currently checked-out commit to AWS, not the full source code. Additionally, OMNeT++ should be ready to use, with its tools (like opp_run or opp_run_dbg ) accessible as commands. $ cd examples/inet/ber Enter the command for running the simulations, using our inet_runall_swarm.py program instead of ./run or inet : $ inet_runall_swarm.py -c ber-flavour-experiment The inet_runall_swarm.py tool will expand the list of simulation runs to be executed, submit them to the job queue, and wait for the jobs to finish. The results will be downloaded automatically into the results folder. You can monitor progress at http://localhost:9181/ which displays the content of the job queue.","title":"Running Simulations"},{"location":"tutorials/swarm/page1/#stopping-restarting-and-deleting-the-swarm","text":"Once you are done, you can stop the machines: $ aws_swarm_tool.py halt They will also shut down automatically after an hour of being idle. Important AWS usage is billed by uptime, i.e. you are charged for the time the Swarm is up and running, regardless whether you are actually using it for simulations or not. Our tool configures AWS to halt the Swarm after one hour of idle time, but for extra safety, double-check that the Swarm has been stopped after you have finished working with it. WE ARE NOT RESPONSIBLE FOR EXTRA COSTS CAUSED BY LEAVING AWS NODES RUNNING. Read on for instructions. The halt command only shuts down the virtual machine instances, but leaves all other resources intact, this way resuming the usage is faster than the initial deployment (with init ). These additional resources being there inactively do not cost you anything. To use the Swarm again, you have to start it up again: $ aws_swarm_tool.py resume To completely delete the entire Swarm: $ aws_swarm_tool.py delete","title":"Stopping, Restarting and Deleting the Swarm"},{"location":"tutorials/swarm/page1/#checking-the-status-manually","text":"Important When you do not intend to work with AWS for an extended period of time, you may want to double-check manually that the stack on AWS has been stopped indeed, and you are no longer being billed for it. To check the status, select the EC2 service on the AWS Management Console, and check the Running Instances . If you see instances named inet-Node or inet-Manager , in the running state, the Swarm is active. To delete the Swarm manually (if the command-line tool does not work for some reason), go to the CloudFormation service page on the AWS Management Console. Tick the checkbox next to the inet Stack in the table, then click Actions > Delete Stack . This should remove all INET-related resources.","title":"Checking the Status Manually"},{"location":"tutorials/swarm/page1/#feedback-and-discussion","text":"We are looking for any kind of feedback or suggestion you might have regarding this tutorial here: https://github.com/omnetpp/omnetpp-tutorials/issues/3","title":"Feedback and Discussion"},{"location":"tutorials/swarm/page2/","text":"This is a distributed application, built on top of Docker Swarm (the new, integrated \"Swarm Mode\", not the legacy docker-swarm utility). It is composed of seven different Docker services . Services \u00b6 The application is made up of several Docker services: redis, mongo, builder, runner, visualizer, dashboard, distcc. Let's discuss what each of them are there for, and what they do. Redis \u00b6 This service runs the official Redis image on the Manager. It is needed by RQ (Redis Queue) that we use for job queueing. Mongo \u00b6 Mongo is a database that we use for temporary storage of binaries and result files. Builder \u00b6 This is one of the two services running an RQ worker. It starts a single container on the manager, and listens on the build queue for jobs. It uses the distcc servers from the distcc service through the buildnet network to distribute the compilation tasks across all nodes. Once a build is done, it submits the binaries (actually, the libINET.so file) to the Mongo service, so the runner containers can access it later. Runner \u00b6 The other RQ worker, running as many containers in each host, as their respecrtive number of cpu cores. Except the manager, because that needs some extra juice running the other services, like redis and mongo. it is done by requesting a large number of containers (100), but reserving 95% of a core for each container, so they automatically \"expand\" to \"fill\" the available number of (remaining) CPUs, like a liquid. It gets the built libINET.so from the MongoDB server, and also submits the simulation results there. Visualizer \u00b6 This service starts a single container on the manager, using the official docker-swarm-visualizer image. In that container, a web server runs that lets you quickly inspect the state of the swarm, including its nodes, services and containers, just using your web browser. It listens on port 8080 , so once your swarm application is up and running (and you are connected to the swarm if it is on AWS), you can check it out at [ http://localhost:8080/ ]. Dashboard \u00b6 Similarly to the visualizer , this is an auxiliary service, running a single container on the manager, with a web server in it. This one lets you see, and manage in a limited way, the RQ queues, workers, and jobs. See: [ http://localhost:9181/ ]. distcc \u00b6 This service starts exactly one container on all nodes (workers and the manager alike). They all run a distcc server, listening for incoming requests for compilation (completely independent from RQ). They are only attached to the buildnet network, and have deterministic IP addresses. When the builder container starts a build in a build job, it will try to connect to the distcc containers, and will use them to distribute the compilation tasks to all nodes. Networks \u00b6 The stack also contains two virtual networks. Each service is attached to one or both of these networks. The networks are: interlink buildnet Both of them use the overlay driver, meaning that these are entirely virtual networks, not interfering with the underlying real one between the nodes. Interlink \u00b6 This is the main network, all services except distcc are attached to it. Buildnet \u00b6 The buildnet network connects the containers of the distcc service with the builder service. It operates on a fixed subnet, The builder This was only necessary to give the distcc containers deterministic and known IP addresses. On interlink they didn't always get the same addresses, they were randomly interleaved with the containers of all the other services. This would not be necessary at all if multicast traffic worked on overlay networks between nodes, because then we could just use the built-in zeroconf service discovery capabilities of distcc (the software itself). However, until this issue is resolved, we have to resort to this solution. Operation \u00b6 aws_swarm_tool init: Deploying the official CloudFormation template supplied by Docker, called Docker for AWS. Using the default settings, the script creates 1 manager and 3 workers, each of them as a c4.4xlarge type Instance. It also creates an alarm and an AutoScaling policy that makes sure that all machines are shut down after 1 hour of inactivity (precisely, if the maximum CPU utilization of the manager machine was below 10 percent for 4 consecutive 15 minute periods). This is to reduce the chances that they are forgotten about, and left running indefinitely, generating unexpected expenditure. To be able to connect to the Swarm we are about to create, we must first create and SSH keypair. The aws_swarm_tool.py can do this for us. Connecting to the Swarm is essentially opening an SSH connection to the manager machine, and forwarding a handful of ports through that tunnel from the local machine to the swarm. There is no need to do it manually, the aws_swarm_tool.py script has a command for it: $ aws_swarm_tool.py connect In addition to bringing up the SSH connection, the script also saves the process ID (PID) of the SSH client process into a file (so called PID-file) in a temporary directory (most likely /tmp ), called What is Docker Swarm? \u00b6 With version 1.12.0, Docker introduced Swarm Mode . It makes it possible to connect multiple computers (called hosts or nodes) on a network, into a cluster - called swarm. This new feature enables someting called \"container orchestration\". It makes the development, deployment, and maintenance of distributed, multi-container applications easier. You can read more about it here . One great advantage of this is that wherever a Docker Swarm is configured, any application can be run, let it be on local machines, or any cloud computing platform.","title":"How it Works"},{"location":"tutorials/swarm/page2/#services","text":"The application is made up of several Docker services: redis, mongo, builder, runner, visualizer, dashboard, distcc. Let's discuss what each of them are there for, and what they do.","title":"Services"},{"location":"tutorials/swarm/page2/#redis","text":"This service runs the official Redis image on the Manager. It is needed by RQ (Redis Queue) that we use for job queueing.","title":"Redis"},{"location":"tutorials/swarm/page2/#mongo","text":"Mongo is a database that we use for temporary storage of binaries and result files.","title":"Mongo"},{"location":"tutorials/swarm/page2/#builder","text":"This is one of the two services running an RQ worker. It starts a single container on the manager, and listens on the build queue for jobs. It uses the distcc servers from the distcc service through the buildnet network to distribute the compilation tasks across all nodes. Once a build is done, it submits the binaries (actually, the libINET.so file) to the Mongo service, so the runner containers can access it later.","title":"Builder"},{"location":"tutorials/swarm/page2/#runner","text":"The other RQ worker, running as many containers in each host, as their respecrtive number of cpu cores. Except the manager, because that needs some extra juice running the other services, like redis and mongo. it is done by requesting a large number of containers (100), but reserving 95% of a core for each container, so they automatically \"expand\" to \"fill\" the available number of (remaining) CPUs, like a liquid. It gets the built libINET.so from the MongoDB server, and also submits the simulation results there.","title":"Runner"},{"location":"tutorials/swarm/page2/#visualizer","text":"This service starts a single container on the manager, using the official docker-swarm-visualizer image. In that container, a web server runs that lets you quickly inspect the state of the swarm, including its nodes, services and containers, just using your web browser. It listens on port 8080 , so once your swarm application is up and running (and you are connected to the swarm if it is on AWS), you can check it out at [ http://localhost:8080/ ].","title":"Visualizer"},{"location":"tutorials/swarm/page2/#dashboard","text":"Similarly to the visualizer , this is an auxiliary service, running a single container on the manager, with a web server in it. This one lets you see, and manage in a limited way, the RQ queues, workers, and jobs. See: [ http://localhost:9181/ ].","title":"Dashboard"},{"location":"tutorials/swarm/page2/#distcc","text":"This service starts exactly one container on all nodes (workers and the manager alike). They all run a distcc server, listening for incoming requests for compilation (completely independent from RQ). They are only attached to the buildnet network, and have deterministic IP addresses. When the builder container starts a build in a build job, it will try to connect to the distcc containers, and will use them to distribute the compilation tasks to all nodes.","title":"distcc"},{"location":"tutorials/swarm/page2/#networks","text":"The stack also contains two virtual networks. Each service is attached to one or both of these networks. The networks are: interlink buildnet Both of them use the overlay driver, meaning that these are entirely virtual networks, not interfering with the underlying real one between the nodes.","title":"Networks"},{"location":"tutorials/swarm/page2/#interlink","text":"This is the main network, all services except distcc are attached to it.","title":"Interlink"},{"location":"tutorials/swarm/page2/#buildnet","text":"The buildnet network connects the containers of the distcc service with the builder service. It operates on a fixed subnet, The builder This was only necessary to give the distcc containers deterministic and known IP addresses. On interlink they didn't always get the same addresses, they were randomly interleaved with the containers of all the other services. This would not be necessary at all if multicast traffic worked on overlay networks between nodes, because then we could just use the built-in zeroconf service discovery capabilities of distcc (the software itself). However, until this issue is resolved, we have to resort to this solution.","title":"Buildnet"},{"location":"tutorials/swarm/page2/#operation","text":"aws_swarm_tool init: Deploying the official CloudFormation template supplied by Docker, called Docker for AWS. Using the default settings, the script creates 1 manager and 3 workers, each of them as a c4.4xlarge type Instance. It also creates an alarm and an AutoScaling policy that makes sure that all machines are shut down after 1 hour of inactivity (precisely, if the maximum CPU utilization of the manager machine was below 10 percent for 4 consecutive 15 minute periods). This is to reduce the chances that they are forgotten about, and left running indefinitely, generating unexpected expenditure. To be able to connect to the Swarm we are about to create, we must first create and SSH keypair. The aws_swarm_tool.py can do this for us. Connecting to the Swarm is essentially opening an SSH connection to the manager machine, and forwarding a handful of ports through that tunnel from the local machine to the swarm. There is no need to do it manually, the aws_swarm_tool.py script has a command for it: $ aws_swarm_tool.py connect In addition to bringing up the SSH connection, the script also saves the process ID (PID) of the SSH client process into a file (so called PID-file) in a temporary directory (most likely /tmp ), called","title":"Operation"},{"location":"tutorials/swarm/page2/#what-is-docker-swarm","text":"With version 1.12.0, Docker introduced Swarm Mode . It makes it possible to connect multiple computers (called hosts or nodes) on a network, into a cluster - called swarm. This new feature enables someting called \"container orchestration\". It makes the development, deployment, and maintenance of distributed, multi-container applications easier. You can read more about it here . One great advantage of this is that wherever a Docker Swarm is configured, any application can be run, let it be on local machines, or any cloud computing platform.","title":"What is Docker Swarm?"},{"location":"tutorials/tictoc/","text":"Introduction \u00b6 This tutorial guides you through building and working with an example simulation model, showing you along the way some of the commonly used OMNeT++ features. The tutorial is based on the Tictoc example simulation, which you can find in the samples/tictoc directory of your OMNeT++ installation, so you can try out immediately how the examples work. However, you'll find the tutorial much more useful if you actually carry out the steps described here. We assume that you have a good C++ knowledge, and you are in general familiar with C/C++ development (editing source files, compiling, debugging etc.) To make the examples easier to follow, all source code in here is cross-linked to the OMNeT++ API documentation. This document and the TicToc model are an expanded version of the original TicToc tutorial from Ahmet Sekercioglu (Monash University).","title":"Introduction"},{"location":"tutorials/tictoc/#introduction","text":"This tutorial guides you through building and working with an example simulation model, showing you along the way some of the commonly used OMNeT++ features. The tutorial is based on the Tictoc example simulation, which you can find in the samples/tictoc directory of your OMNeT++ installation, so you can try out immediately how the examples work. However, you'll find the tutorial much more useful if you actually carry out the steps described here. We assume that you have a good C++ knowledge, and you are in general familiar with C/C++ development (editing source files, compiling, debugging etc.) To make the examples easier to follow, all source code in here is cross-linked to the OMNeT++ API documentation. This document and the TicToc model are an expanded version of the original TicToc tutorial from Ahmet Sekercioglu (Monash University).","title":"Introduction"},{"location":"tutorials/tictoc/conclusion/","text":"Closing words \u00b6 Congratulations! \u00b6 You have successfully completed this tutorial! You have gained a good overview and the basic skills to work with OMNeT++, from writing simulations to analyzing results. To go to the next level, we recommend you to read the Simulation Manual and skim through the User Guide . Comments and suggestions regarding this tutorial will be very much appreciated.","title":"Closing words"},{"location":"tutorials/tictoc/conclusion/#closing-words","text":"","title":"Closing words"},{"location":"tutorials/tictoc/conclusion/#congratulations","text":"You have successfully completed this tutorial! You have gained a good overview and the basic skills to work with OMNeT++, from writing simulations to analyzing results. To go to the next level, we recommend you to read the Simulation Manual and skim through the User Guide . Comments and suggestions regarding this tutorial will be very much appreciated.","title":"Congratulations!"},{"location":"tutorials/tictoc/part1/","text":"Part 1 - Getting Started \u00b6 1.1 The model \u00b6 For a start, let us begin with a \"network\" that consists of two nodes. The nodes will do something simple: one of the nodes will create a packet, and the two nodes will keep passing the same packet back and forth. We'll call the nodes tic and toc . Later we'll gradually improve this model, introducing OMNeT++ features at each step. Here are the steps you take to implement your first simulation from scratch. 1.2 Setting up the project \u00b6 Start the OMNeT++ IDE by typing omnetpp in your terminal. (We assume that you already have a working OMNeT++ installation. If not, please install the latest version, consulting the Installation Guide as needed.) Once in the IDE, choose New -> OMNeT++ Project from the menu. A wizard dialog will appear. Enter tictoc as project name, choose Empty project when asked about the initial content of the project, then click Finish . An empty project will be created, as you can see in the Project Explorer . (Note: Some OMNeT++ versions will generate a package.ned file into the project. We don't need it now: delete the file by selecting it and hitting Delete.) The project will hold all files that belong to our simulation. In our example, the project consists of a single directory. For larger simulations, the project's contents are usually sorted into src/ and simulations/ folders, and possibly subfolders underneath them. Note Using the IDE is entirely optional. Almost all functionality of OMNeT++ (except for some very graphics-intensive and interactive features like sequence chart browsing and result plotting) is available on the command line. Model source files can be edited with any text editor, and OMNeT++ provides command-line tools for special tasks such as makefile creation, message file to C++ translation, result file querying and data export, and so on. To proceed without the IDE, simply create a directory and create the following NED, C++ and ini files in it with your favorite text editor. 1.3 Adding the NED file \u00b6 OMNeT++ uses NED files to define components and to assemble them into larger units like networks. We start implementing our model by adding a NED file. To add the file to the project, right-click the project directory in the Project Explorer panel on the left, and choose New -> Network Description File (NED) from the menu. Enter tictoc1.ned when prompted for the file name. Once created, the file can be edited in the Editor area of the OMNeT++ IDE. The OMNeT++ IDE's NED editor has two modes, Design and Source ; one can switch between them using the tabs at the bottom of the editor. In Design mode, the topology can be edited graphically, using the mouse and the palette on the right. In Source mode, the NED source code can be directly edited as text. Changes done in one mode will be immediately reflected in the other, so you can freely switch between modes during editing, and do each change in whichever mode it is more convenient. (Since NED files are plain text files, you can even use an external text editor to edit them, although you'll miss syntax highlighting, content assist, cross-references and other IDE features.) Switch into Source mode, and enter the following: When you're done, switch back to Design mode. You should see something like this: The first block in the file declares Txc1 as a simple module type. Simple modules are atomic on NED level. They are also active components, and their behavior is implemented in C++. The declaration also says that Txc1 has an input gate named in , and an output gate named out . The second block declares Tictoc1 as a network. Tictoc1 is assembled from two submodules, tic and toc , both instances of the module type Txc1 . tic 's output gate is connected to toc 's input gate, and vice versa. There will be a 100ms propagation delay both ways. Note You can find a detailed description of the NED language in the OMNeT++ Simulation Manual . (The manual can also be found in the doc directory of your OMNeT++ installation.) 1.4 Adding the C++ files \u00b6 We now need to implement the functionality of the Txc1 simple module in C++. Create a file named txc1.cc by choosing New -> Source File from the project's context menu (or File -> New -> File from the IDE's main menu), and enter the following content: The Txc1 simple module type is represented by the C++ class Txc1 . The Txc1 class needs to subclass from OMNeT++'s cSimpleModule class, and needs to be registered in OMNeT++ with the Define_Module() macro. Note It is a common mistake to forget the Define_Module() line. If it is missing, you'll get an error message similar to this one: \"Error: Class 'Txc1' not found -- perhaps its code was not linked in, or the class wasn't registered with Register_Class(), or in the case of modules and channels, with Define_Module()/Define_Channel()\" . We redefine two methods from cSimpleModule : initialize() and handleMessage() . They are invoked from the simulation kernel: the first one only once, and the second one whenever a message arrives at the module. In initialize() we create a message object ( cMessage ), and send it out on gate out . Since this gate is connected to the other module's input gate, the simulation kernel will deliver this message to the other module in the argument to handleMessage() -- after a 100ms propagation delay assigned to the link in the NED file. The other module just sends it back (another 100ms delay), so it will result in a continuous ping-pong. Messages (packets, frames, jobs, etc) and events (timers, timeouts) are all represented by cMessage objects (or its subclasses) in OMNeT++. After you send or schedule them, they will be held by the simulation kernel in the \"scheduled events\" or \"future events\" list until their time comes and they are delivered to the modules via handleMessage() . Note that there is no stopping condition built into this simulation: it would continue forever. You will be able to stop it from the GUI. (You could also specify a simulation time limit or CPU time limit in the configuration file, but we don't do that in the tutorial.) 1.5 Adding omnetpp.ini \u00b6 To be able to run the simulation, we need to create an omnetpp.ini file. omnetpp.ini tells the simulation program which network you want to simulate (as NED files may contain several networks), you can pass parameters to the model, explicitly specify seeds for the random number generators, etc. Create an omnetpp.ini file using the File -> New -> Initialization file (INI) menu item. The new file will open in an Inifile Editor . As the NED Editor, the Inifile Editor also has two modes, Form and Source , which edit the same content. The former is more suitable for configuring the simulation kernel, and the latter for entering module parameters. For now, just switch to Source mode and enter the following: [General] network = Tictoc1 You can verify the result in Form mode: tictoc2 and further steps will all share a common file. We are now done with creating the first model, and ready to compile and run it. Sources: , ,","title":"Getting Started"},{"location":"tutorials/tictoc/part1/#part-1-getting-started","text":"","title":"Part 1 - Getting Started"},{"location":"tutorials/tictoc/part1/#11-the-model","text":"For a start, let us begin with a \"network\" that consists of two nodes. The nodes will do something simple: one of the nodes will create a packet, and the two nodes will keep passing the same packet back and forth. We'll call the nodes tic and toc . Later we'll gradually improve this model, introducing OMNeT++ features at each step. Here are the steps you take to implement your first simulation from scratch.","title":"1.1 The model"},{"location":"tutorials/tictoc/part1/#12-setting-up-the-project","text":"Start the OMNeT++ IDE by typing omnetpp in your terminal. (We assume that you already have a working OMNeT++ installation. If not, please install the latest version, consulting the Installation Guide as needed.) Once in the IDE, choose New -> OMNeT++ Project from the menu. A wizard dialog will appear. Enter tictoc as project name, choose Empty project when asked about the initial content of the project, then click Finish . An empty project will be created, as you can see in the Project Explorer . (Note: Some OMNeT++ versions will generate a package.ned file into the project. We don't need it now: delete the file by selecting it and hitting Delete.) The project will hold all files that belong to our simulation. In our example, the project consists of a single directory. For larger simulations, the project's contents are usually sorted into src/ and simulations/ folders, and possibly subfolders underneath them. Note Using the IDE is entirely optional. Almost all functionality of OMNeT++ (except for some very graphics-intensive and interactive features like sequence chart browsing and result plotting) is available on the command line. Model source files can be edited with any text editor, and OMNeT++ provides command-line tools for special tasks such as makefile creation, message file to C++ translation, result file querying and data export, and so on. To proceed without the IDE, simply create a directory and create the following NED, C++ and ini files in it with your favorite text editor.","title":"1.2 Setting up the project"},{"location":"tutorials/tictoc/part1/#13-adding-the-ned-file","text":"OMNeT++ uses NED files to define components and to assemble them into larger units like networks. We start implementing our model by adding a NED file. To add the file to the project, right-click the project directory in the Project Explorer panel on the left, and choose New -> Network Description File (NED) from the menu. Enter tictoc1.ned when prompted for the file name. Once created, the file can be edited in the Editor area of the OMNeT++ IDE. The OMNeT++ IDE's NED editor has two modes, Design and Source ; one can switch between them using the tabs at the bottom of the editor. In Design mode, the topology can be edited graphically, using the mouse and the palette on the right. In Source mode, the NED source code can be directly edited as text. Changes done in one mode will be immediately reflected in the other, so you can freely switch between modes during editing, and do each change in whichever mode it is more convenient. (Since NED files are plain text files, you can even use an external text editor to edit them, although you'll miss syntax highlighting, content assist, cross-references and other IDE features.) Switch into Source mode, and enter the following: When you're done, switch back to Design mode. You should see something like this: The first block in the file declares Txc1 as a simple module type. Simple modules are atomic on NED level. They are also active components, and their behavior is implemented in C++. The declaration also says that Txc1 has an input gate named in , and an output gate named out . The second block declares Tictoc1 as a network. Tictoc1 is assembled from two submodules, tic and toc , both instances of the module type Txc1 . tic 's output gate is connected to toc 's input gate, and vice versa. There will be a 100ms propagation delay both ways. Note You can find a detailed description of the NED language in the OMNeT++ Simulation Manual . (The manual can also be found in the doc directory of your OMNeT++ installation.)","title":"1.3 Adding the NED file"},{"location":"tutorials/tictoc/part1/#14-adding-the-c-files","text":"We now need to implement the functionality of the Txc1 simple module in C++. Create a file named txc1.cc by choosing New -> Source File from the project's context menu (or File -> New -> File from the IDE's main menu), and enter the following content: The Txc1 simple module type is represented by the C++ class Txc1 . The Txc1 class needs to subclass from OMNeT++'s cSimpleModule class, and needs to be registered in OMNeT++ with the Define_Module() macro. Note It is a common mistake to forget the Define_Module() line. If it is missing, you'll get an error message similar to this one: \"Error: Class 'Txc1' not found -- perhaps its code was not linked in, or the class wasn't registered with Register_Class(), or in the case of modules and channels, with Define_Module()/Define_Channel()\" . We redefine two methods from cSimpleModule : initialize() and handleMessage() . They are invoked from the simulation kernel: the first one only once, and the second one whenever a message arrives at the module. In initialize() we create a message object ( cMessage ), and send it out on gate out . Since this gate is connected to the other module's input gate, the simulation kernel will deliver this message to the other module in the argument to handleMessage() -- after a 100ms propagation delay assigned to the link in the NED file. The other module just sends it back (another 100ms delay), so it will result in a continuous ping-pong. Messages (packets, frames, jobs, etc) and events (timers, timeouts) are all represented by cMessage objects (or its subclasses) in OMNeT++. After you send or schedule them, they will be held by the simulation kernel in the \"scheduled events\" or \"future events\" list until their time comes and they are delivered to the modules via handleMessage() . Note that there is no stopping condition built into this simulation: it would continue forever. You will be able to stop it from the GUI. (You could also specify a simulation time limit or CPU time limit in the configuration file, but we don't do that in the tutorial.)","title":"1.4 Adding the C++ files"},{"location":"tutorials/tictoc/part1/#15-adding-omnetppini","text":"To be able to run the simulation, we need to create an omnetpp.ini file. omnetpp.ini tells the simulation program which network you want to simulate (as NED files may contain several networks), you can pass parameters to the model, explicitly specify seeds for the random number generators, etc. Create an omnetpp.ini file using the File -> New -> Initialization file (INI) menu item. The new file will open in an Inifile Editor . As the NED Editor, the Inifile Editor also has two modes, Form and Source , which edit the same content. The former is more suitable for configuring the simulation kernel, and the latter for entering module parameters. For now, just switch to Source mode and enter the following: [General] network = Tictoc1 You can verify the result in Form mode: tictoc2 and further steps will all share a common file. We are now done with creating the first model, and ready to compile and run it. Sources: , ,","title":"1.5  Adding omnetpp.ini"},{"location":"tutorials/tictoc/part2/","text":"Part 2 - Running the Simulation \u00b6 2.1 Launching the simulation program \u00b6 Once you complete the above steps, you can launch the simulation by selecting %omnetpp.ini (in either the editor area or the Project Explorer ), and pressing the Run button. The IDE will build your project automatically. If there are compilation errors, you need to rectify those until you get an error-free compilation and linking. You can manually trigger a build by hitting choosing Project -> Build All from the menu, or hitting Ctrl+B . Note If you want to build the simulation executable on the command-line, create a Makefile using the opp_makemake command, then enter make to build the project. It will produce an executable that can be run by entering ./tictoc . 2.2 Running the simulation \u00b6 After successfully building and launching your simulation, you should see a new GUI window appear, similar to the one in the screenshot below. The window belongs to Qtenv , the main OMNeT++ simulation runtime GUI. You should also see the network containing tic and toc displayed graphically in the main area. Press the Run button on the toolbar to start the simulation. What you should see is that tic and toc are exchanging messages with each other. The main window toolbar displays the current simulation time. This is virtual time, it has nothing to do with the actual (or wall-clock) time that the program takes to execute. Actually, how many seconds you can simulate in one real-world second depends highly on the speed of your hardware and even more on the nature and complexity of the simulation model itself. Note that it takes zero simulation time for a node to process the message. The only thing that makes the simulation time pass in this model is the propagation delay on the connections. You can play with slowing down the animation or making it faster with the slider at the top of the graphics window. You can stop the simulation by hitting F8 (equivalent to the STOP button on the toolbar), single-step through it (F4), run it with (F5) or without (F6) animation. F7 (express mode) completely turns off tracing features for maximum speed. Note the event/sec and simsec/sec gauges on the status bar of the main window (only visible when the simulation is running in fast or express mode). Exercise Explore the GUI by running the simulation several times. Try Run , Run Until , Rebuild Network , and other functions. You can exit the simulation program by clicking its Close icon or choosing File -> Exit . 2.3 Debugging \u00b6 The simulation is just a C++ program, and as such, it often needs to be debugged while it is being developed. In this section we'll look at the basics of debugging to help you acquire this vital task. The simulation can be started in debug mode by clicking the Debug button on the IDE's main toolbar. This will cause the simulation program to be launched under a debugger (usually gdb ). The IDE will also switch into \"Debug perspective\", i.e. rearrange its various panes and views to a layout which is better suited to debugging. You can end the debugging session with the Terminate button (a red square) on the toolbar. Runtime errors \u00b6 Debugging is most often needed to track down runtime errors. Let's try it! First, deliberately introduce an error into the program. In , duplicate the send() line inside handleMessage() , so that the code looks like this: void Txc1::handleMessage(cMessage *msg) { //... send(msg, \"out\"); // send out the message send(msg, \"out\"); // THIS SHOULD CAUSE AN ERROR } When you launch the simulation in normal mode ( Run button) and try to run it, you'll get an error message like this: Now, run the simulation in Debug mode. Due to a debug-on-errors option being enabled by default, the simulation program will stop in the debugger. You can locate the error by examining the stack trace (the list of nested function calls) in the Debug view: You can see that it was OMNeT++'s breakIntoDebuggerIfRequested() method that activated the debugger. From then on, you need to search for a function that looks familiar, i.e. for one that is part of the model. In our case, that is the \"Txc1::handleMessage() at txc1.cc:54\" line. Selecting that line will show you the corresponding source code in the editor area, and lets you examine the values of variables in the Variables view. This information will help you determine the cause of the error and fix it. Crashes \u00b6 Tracking down crashes i.e. segfaults is similar, let's try that as well. Undo the previous source code edit (remove the duplicate send() line), and introduce another error. Let's pretend we forgot to create the message before sending it, and change the following lines in initialize() cMessage *msg = new cMessage(\"tictocMsg\"); send(msg, \"out\"); to simply cMessage *msg; // no initialization! send(msg, \"out\"); When you run the simulation, it will crash. (You will get an error message similar to \"Simulation terminated with exit code: 139\"). If you launch the simulation again, this time in Debug mode, the crash will bring you into the debugger. Once there, you'll be able to locate the error in the Debug view and examine variables, which will help you identify and fix the bug. Breakpoints \u00b6 You can also manually place breakpoints into the code. Breakpoints will stop execution, and let you examine variables, execute the code line-by-line, or resume execution (until the next breakpoint). A breakpoint can be placed at a specific line in the source code by double-clicking on the left gutter in the editor, or choosing Toggle Breakpoint from the context menu. The list of active (and inactive) breakpoints can be examined in the Breakpoints view. Exercise Experiment with breakpoints! Place a breakpoint at the beginning of the handleMessage() method function, and run the simulation. Use appropriate buttons on the toolbar to single-step, continue execution until next time the breakpoint is hit, and so on. Debug next event \u00b6 If you did the previous exercise, you must have noticed that the breakpoint was triggered at each and every event in the Txc1 simple module. In real life it often occurs that an error only surfaces at, say, the 357 th event in that module, so ideally that's when you'd want to start debugging. It is not very convenient to have to hit Resume 356 times just to get to the place of the error. A possible solution is to add a condition or an ignore -count to the breakpoint (see Breakpoint Properties in its context menu). However, there is a potentially more convenient solution. In Qtenv , use Run Until to get to the event to be debugged. Then, choose Simulation -> Debug Next Event from the menu. This will trigger a breakpoint in the debugger at the beginning of handleMessage() of the next event, and you can start debugging that event. 2.4 The Debug/Run dialog \u00b6 Let us return to launching simulations once more. When you launch the simulation program with the Run or Debug buttons on the IDE toolbar, settings associated with the launch are saved in a launch configuration . Launch configurations can be viewed in the Run/Debug Configurations dialog which can be opened e.g. by clicking the little down arrow next to the Run ( Debug ) toolbar button to open a menu, and choosing Run (Debug) Configurations... in it. In the same menu, you can also click the name of a launch configuration (e.g. tictoc ) while holding down the Ctrl key to open the dialog with the corresponding configuration. The dialog allows you activate various settings for the launch. 2.5 Visualizing on a Sequence Chart \u00b6 The OMNeT++ simulation kernel can record the message exchanges during the simulation into an event log file . To enable recording the event log, check the Record eventlog checkbox in the launch configuration dialog. Alternatively, you can specify record-eventlog = true in omnetpp.ini, or even, use the Record button in the Qtenv graphical runtime environment after launching, The log file can be analyzed later with the Sequence Chart tool in the IDE. The results directory in the project folder contains the .elog file. Double-clicking on it in the OMNeT++ IDE opens the Sequence Chart tool, and the event log tab at the bottom of the window. Note The resulting log file can be quite large, so enable this feature only if you really need it. The following figure has been created with the Sequence Chart tool, and shows how the message is routed between the different nodes in the network. In this instance the chart is very simple, but when you have a complex model, sequence charts can be very valuable in debugging, exploring or documenting the model's behaviour. Sources: , ,","title":"Running the Simulation"},{"location":"tutorials/tictoc/part2/#part-2-running-the-simulation","text":"","title":"Part 2 - Running the Simulation"},{"location":"tutorials/tictoc/part2/#21-launching-the-simulation-program","text":"Once you complete the above steps, you can launch the simulation by selecting %omnetpp.ini (in either the editor area or the Project Explorer ), and pressing the Run button. The IDE will build your project automatically. If there are compilation errors, you need to rectify those until you get an error-free compilation and linking. You can manually trigger a build by hitting choosing Project -> Build All from the menu, or hitting Ctrl+B . Note If you want to build the simulation executable on the command-line, create a Makefile using the opp_makemake command, then enter make to build the project. It will produce an executable that can be run by entering ./tictoc .","title":"2.1 Launching the simulation program"},{"location":"tutorials/tictoc/part2/#22-running-the-simulation","text":"After successfully building and launching your simulation, you should see a new GUI window appear, similar to the one in the screenshot below. The window belongs to Qtenv , the main OMNeT++ simulation runtime GUI. You should also see the network containing tic and toc displayed graphically in the main area. Press the Run button on the toolbar to start the simulation. What you should see is that tic and toc are exchanging messages with each other. The main window toolbar displays the current simulation time. This is virtual time, it has nothing to do with the actual (or wall-clock) time that the program takes to execute. Actually, how many seconds you can simulate in one real-world second depends highly on the speed of your hardware and even more on the nature and complexity of the simulation model itself. Note that it takes zero simulation time for a node to process the message. The only thing that makes the simulation time pass in this model is the propagation delay on the connections. You can play with slowing down the animation or making it faster with the slider at the top of the graphics window. You can stop the simulation by hitting F8 (equivalent to the STOP button on the toolbar), single-step through it (F4), run it with (F5) or without (F6) animation. F7 (express mode) completely turns off tracing features for maximum speed. Note the event/sec and simsec/sec gauges on the status bar of the main window (only visible when the simulation is running in fast or express mode). Exercise Explore the GUI by running the simulation several times. Try Run , Run Until , Rebuild Network , and other functions. You can exit the simulation program by clicking its Close icon or choosing File -> Exit .","title":"2.2 Running the simulation"},{"location":"tutorials/tictoc/part2/#23-debugging","text":"The simulation is just a C++ program, and as such, it often needs to be debugged while it is being developed. In this section we'll look at the basics of debugging to help you acquire this vital task. The simulation can be started in debug mode by clicking the Debug button on the IDE's main toolbar. This will cause the simulation program to be launched under a debugger (usually gdb ). The IDE will also switch into \"Debug perspective\", i.e. rearrange its various panes and views to a layout which is better suited to debugging. You can end the debugging session with the Terminate button (a red square) on the toolbar.","title":"2.3 Debugging"},{"location":"tutorials/tictoc/part2/#runtime-errors","text":"Debugging is most often needed to track down runtime errors. Let's try it! First, deliberately introduce an error into the program. In , duplicate the send() line inside handleMessage() , so that the code looks like this: void Txc1::handleMessage(cMessage *msg) { //... send(msg, \"out\"); // send out the message send(msg, \"out\"); // THIS SHOULD CAUSE AN ERROR } When you launch the simulation in normal mode ( Run button) and try to run it, you'll get an error message like this: Now, run the simulation in Debug mode. Due to a debug-on-errors option being enabled by default, the simulation program will stop in the debugger. You can locate the error by examining the stack trace (the list of nested function calls) in the Debug view: You can see that it was OMNeT++'s breakIntoDebuggerIfRequested() method that activated the debugger. From then on, you need to search for a function that looks familiar, i.e. for one that is part of the model. In our case, that is the \"Txc1::handleMessage() at txc1.cc:54\" line. Selecting that line will show you the corresponding source code in the editor area, and lets you examine the values of variables in the Variables view. This information will help you determine the cause of the error and fix it.","title":"Runtime errors"},{"location":"tutorials/tictoc/part2/#crashes","text":"Tracking down crashes i.e. segfaults is similar, let's try that as well. Undo the previous source code edit (remove the duplicate send() line), and introduce another error. Let's pretend we forgot to create the message before sending it, and change the following lines in initialize() cMessage *msg = new cMessage(\"tictocMsg\"); send(msg, \"out\"); to simply cMessage *msg; // no initialization! send(msg, \"out\"); When you run the simulation, it will crash. (You will get an error message similar to \"Simulation terminated with exit code: 139\"). If you launch the simulation again, this time in Debug mode, the crash will bring you into the debugger. Once there, you'll be able to locate the error in the Debug view and examine variables, which will help you identify and fix the bug.","title":"Crashes"},{"location":"tutorials/tictoc/part2/#breakpoints","text":"You can also manually place breakpoints into the code. Breakpoints will stop execution, and let you examine variables, execute the code line-by-line, or resume execution (until the next breakpoint). A breakpoint can be placed at a specific line in the source code by double-clicking on the left gutter in the editor, or choosing Toggle Breakpoint from the context menu. The list of active (and inactive) breakpoints can be examined in the Breakpoints view. Exercise Experiment with breakpoints! Place a breakpoint at the beginning of the handleMessage() method function, and run the simulation. Use appropriate buttons on the toolbar to single-step, continue execution until next time the breakpoint is hit, and so on.","title":"Breakpoints"},{"location":"tutorials/tictoc/part2/#debug-next-event","text":"If you did the previous exercise, you must have noticed that the breakpoint was triggered at each and every event in the Txc1 simple module. In real life it often occurs that an error only surfaces at, say, the 357 th event in that module, so ideally that's when you'd want to start debugging. It is not very convenient to have to hit Resume 356 times just to get to the place of the error. A possible solution is to add a condition or an ignore -count to the breakpoint (see Breakpoint Properties in its context menu). However, there is a potentially more convenient solution. In Qtenv , use Run Until to get to the event to be debugged. Then, choose Simulation -> Debug Next Event from the menu. This will trigger a breakpoint in the debugger at the beginning of handleMessage() of the next event, and you can start debugging that event.","title":"Debug next event"},{"location":"tutorials/tictoc/part2/#24-the-debugrun-dialog","text":"Let us return to launching simulations once more. When you launch the simulation program with the Run or Debug buttons on the IDE toolbar, settings associated with the launch are saved in a launch configuration . Launch configurations can be viewed in the Run/Debug Configurations dialog which can be opened e.g. by clicking the little down arrow next to the Run ( Debug ) toolbar button to open a menu, and choosing Run (Debug) Configurations... in it. In the same menu, you can also click the name of a launch configuration (e.g. tictoc ) while holding down the Ctrl key to open the dialog with the corresponding configuration. The dialog allows you activate various settings for the launch.","title":"2.4 The Debug/Run dialog"},{"location":"tutorials/tictoc/part2/#25-visualizing-on-a-sequence-chart","text":"The OMNeT++ simulation kernel can record the message exchanges during the simulation into an event log file . To enable recording the event log, check the Record eventlog checkbox in the launch configuration dialog. Alternatively, you can specify record-eventlog = true in omnetpp.ini, or even, use the Record button in the Qtenv graphical runtime environment after launching, The log file can be analyzed later with the Sequence Chart tool in the IDE. The results directory in the project folder contains the .elog file. Double-clicking on it in the OMNeT++ IDE opens the Sequence Chart tool, and the event log tab at the bottom of the window. Note The resulting log file can be quite large, so enable this feature only if you really need it. The following figure has been created with the Sequence Chart tool, and shows how the message is routed between the different nodes in the network. In this instance the chart is very simple, but when you have a complex model, sequence charts can be very valuable in debugging, exploring or documenting the model's behaviour. Sources: , ,","title":"2.5 Visualizing on a Sequence Chart"},{"location":"tutorials/tictoc/part3/","text":"Part 3 - Enhancing the 2-node TicToc \u00b6 3.1 Adding icons \u00b6 Here we make the model look a bit prettier in the GUI. We assign the block/routing icon (the file images/block/routing.png ), and paint it cyan for tic and yellow for toc . This is achieved by adding display strings to the NED file. The i= tag in the display string specifies the icon. You can see the result here: 3.2 Adding logging \u00b6 We also modify the C++ code. We add log statements to Txc1 so that it prints what it is doing. OMNeT++ provides a sophisticated logging facility with log levels, log channels, filtering, etc. that are useful for large and complex models, but in this model we'll use its simplest form EV : and When you run the simulation in the OMNeT++ runtime environment, the following output will appear in the log window: You can also open separate output windows for tic and toc by right-clicking on their icons and choosing Component log from the menu. This feature will be useful when you have a large model (\"fast scrolling logs syndrome\") and you're interested only in the log messages of specific module. Sources: , , 3.3 Adding state variables \u00b6 In this step we add a counter to the module, and delete the message after ten exchanges. We add the counter as a class member: We set the variable to 10 in initialize() and decrement in handleMessage() , that is, on every message arrival. After it reaches zero, the simulation will run out of events and terminate. Note the line in the source: this makes it possible to see the counter value in the graphical runtime environment. If you click on tic 's icon, the inspector window in the bottom left corner of the main window will display details about tic . Make sure that Children mode is selected from the toolbar at the top. The inspector now displays the counter variable. As you continue running the simulation, you can follow as the counter keeps decrementing until it reaches zero. Sources: , , 3.4 Adding parameters \u00b6 In this step you'll learn how to add input parameters to the simulation: we'll turn the \"magic number\" 10 into a parameter and add a boolean parameter to decide whether the module should send out the first message in its initialization code (whether this is a tic or a toc module). Module parameters have to be declared in the NED file. The data type can be numeric, string, bool, or xml (the latter is for easy access to XML config files), among others. We also have to modify the C++ code to read the parameter in initialize() , and assign it to the counter. We can use the second parameter to decide whether to send initial message: Now, we can assign the parameters in the NED file or from omnetpp.ini . Assignments in the NED file take precedence. You can define default values for parameters if you use the default(...) syntax in the NED file. In this case you can either set the value of the parameter in omnetpp.ini or use the default value provided by the NED file. Here, we assign one parameter in the NED file: and the other in omnetpp.ini : Note that because omnetpp.ini supports wildcards, and parameters assigned from NED files take precedence over the ones in omnetpp.ini, we could have used Tictoc4.t*c.limit=5 or Tictoc4.*.limit=5 or even **.limit=5 with the same effect. (The difference between * and ** is that * will not match a dot and ** will.) In the graphical runtime environment, you can inspect module parameters either in the object tree on the left-hand side of the main window, or in the Parameters page of the module inspector (information is shown in the bottom left corner of the main window after clicking on a module). The module with the smaller limit will delete the message and thereby conclude the simulation. Sources: , , 3.5 Using NED inheritance \u00b6 If we take a closer look at the NED file we will realize that tic and toc differs only in their parameter values and their display string. We can create a new simple module type by inheriting from an other one and specifying or overriding some of its parameters. In our case we will derive two simple module types ( Tic and Toc ). Later we can use these types when defining the submodules in the network. Deriving from an existing simple module is easy. Here is the base module: And here is the derived module. We just simply specify the parameter values and add some display properties. The Toc module looks similar, but with different parameter values. Note The C++ implementation is inherited from the base simple module ( Txc5 ). Once we created the new simple modules, we can use them as submodule types in our network: As you can see, the network definition is much shorter and simpler now. Inheritance allows you to use common types in your network and avoid redundant definitions and parameter settings. 3.6 Modeling processing delay \u00b6 In the previous models, tic and toc immediately sent back the received message. Here we'll add some timing: tic and toc will hold the message for 1 simulated second before sending it back. In OMNeT++ such timing is achieved by the module sending a message to itself. Such messages are called self-messages (but only because of the way they are used, otherwise they are ordinary message objects). We added two cMessage * variables, event and tictocMsg to the class, to remember the message we use for timing and message whose processing delay we are simulating. We \"send\" the self-messages with the scheduleAt() function, specifying when it should be delivered back to the module. In handleMessage() now we have to differentiate whether a new message has arrived via the input gate or the self-message came back (timer expired). Here we are using but we could have written if (msg->isSelfMessage()) as well. We have left out the counter, to keep the source code small. While running the simulation you will see the following log output: Sources: , , 3.7 Random numbers and parameters \u00b6 In this step we'll introduce random numbers. We change the delay from 1s to a random value which can be set from the NED file or from omnetpp.ini. Module parameters are able to return random variables; however, to make use of this feature we have to read the parameter in handleMessage() every time we use it. In addition, we'll \"lose\" (delete) the packet with a small (hardcoded) probability. We'll assign the parameters in omnetpp.ini: You can try that no matter how many times you re-run the simulation (or restart it, Simulate -> Rebuild network menu item), you'll get exactly the same results. This is because OMNeT++ uses a deterministic algorithm (by default the Mersenne Twister RNG) to generate random numbers, and initializes it to the same seed. This is important for reproducible simulations. You can experiment with different seeds if you add the following lines to omnetpp.ini: [General] seed-0-mt = 532569 # or any other 32-bit value From the syntax you have probably guessed that OMNeT++ supports more than one RNGs. That's right, however, all models in this tutorial use RNG 0. Exercise Try other distributions as well. Sources: , , 3.8 Timeout, cancelling timers \u00b6 In order to get one step closer to modelling networking protocols, let us transform our model into a stop-and-wait simulation. This time we'll have separate classes for tic and toc . The basic scenario is similar to the previous ones: tic and toc will be tossing a message to one another. However, toc will \"lose\" the message with some nonzero probability, and in that case tic will have to resend it. Here's toc 's code: Thanks to the bubble() call in the code, toc will display a callout whenever it drops the message. So, tic will start a timer whenever it sends the message. When the timer expires, we'll assume the message was lost and send another one. If toc 's reply arrives, the timer has to be cancelled. The timer will be (what else?) a self-message. Cancelling the timer will be done with the cancelEvent() call. Note that this does not prevent us from being able to reuse the same timeout message over and over. You can read Tic's full source in Sources: , , 3.9 Retransmitting the same message \u00b6 In this step we refine the previous model. There we just created another packet if we needed to retransmit. This is OK because the packet didn't contain much, but in real life it's usually more practical to keep a copy of the original packet so that we can re-send it without the need to build it again. Keeping a pointer to the sent message - so we can send it again - might seem easier, but when the message is destroyed at the other node the pointer becomes invalid. What we do here is keep the original packet and send only copies of it. We delete the original when toc 's acknowledgement arrives. To make it easier to visually verify the model, we'll include a message sequence number in the message names. In order to avoid handleMessage() growing too large, we'll put the corresponding code into two new functions, generateNewMessage() and sendCopyOf() and call them from handleMessage() . The functions: Sources: , ,","title":"Enhancing the 2-node TicToc"},{"location":"tutorials/tictoc/part3/#part-3-enhancing-the-2-node-tictoc","text":"","title":"Part 3 - Enhancing the 2-node TicToc"},{"location":"tutorials/tictoc/part3/#31-adding-icons","text":"Here we make the model look a bit prettier in the GUI. We assign the block/routing icon (the file images/block/routing.png ), and paint it cyan for tic and yellow for toc . This is achieved by adding display strings to the NED file. The i= tag in the display string specifies the icon. You can see the result here:","title":"3.1 Adding icons"},{"location":"tutorials/tictoc/part3/#32-adding-logging","text":"We also modify the C++ code. We add log statements to Txc1 so that it prints what it is doing. OMNeT++ provides a sophisticated logging facility with log levels, log channels, filtering, etc. that are useful for large and complex models, but in this model we'll use its simplest form EV : and When you run the simulation in the OMNeT++ runtime environment, the following output will appear in the log window: You can also open separate output windows for tic and toc by right-clicking on their icons and choosing Component log from the menu. This feature will be useful when you have a large model (\"fast scrolling logs syndrome\") and you're interested only in the log messages of specific module. Sources: , ,","title":"3.2 Adding logging"},{"location":"tutorials/tictoc/part3/#33-adding-state-variables","text":"In this step we add a counter to the module, and delete the message after ten exchanges. We add the counter as a class member: We set the variable to 10 in initialize() and decrement in handleMessage() , that is, on every message arrival. After it reaches zero, the simulation will run out of events and terminate. Note the line in the source: this makes it possible to see the counter value in the graphical runtime environment. If you click on tic 's icon, the inspector window in the bottom left corner of the main window will display details about tic . Make sure that Children mode is selected from the toolbar at the top. The inspector now displays the counter variable. As you continue running the simulation, you can follow as the counter keeps decrementing until it reaches zero. Sources: , ,","title":"3.3 Adding state variables"},{"location":"tutorials/tictoc/part3/#34-adding-parameters","text":"In this step you'll learn how to add input parameters to the simulation: we'll turn the \"magic number\" 10 into a parameter and add a boolean parameter to decide whether the module should send out the first message in its initialization code (whether this is a tic or a toc module). Module parameters have to be declared in the NED file. The data type can be numeric, string, bool, or xml (the latter is for easy access to XML config files), among others. We also have to modify the C++ code to read the parameter in initialize() , and assign it to the counter. We can use the second parameter to decide whether to send initial message: Now, we can assign the parameters in the NED file or from omnetpp.ini . Assignments in the NED file take precedence. You can define default values for parameters if you use the default(...) syntax in the NED file. In this case you can either set the value of the parameter in omnetpp.ini or use the default value provided by the NED file. Here, we assign one parameter in the NED file: and the other in omnetpp.ini : Note that because omnetpp.ini supports wildcards, and parameters assigned from NED files take precedence over the ones in omnetpp.ini, we could have used Tictoc4.t*c.limit=5 or Tictoc4.*.limit=5 or even **.limit=5 with the same effect. (The difference between * and ** is that * will not match a dot and ** will.) In the graphical runtime environment, you can inspect module parameters either in the object tree on the left-hand side of the main window, or in the Parameters page of the module inspector (information is shown in the bottom left corner of the main window after clicking on a module). The module with the smaller limit will delete the message and thereby conclude the simulation. Sources: , ,","title":"3.4 Adding parameters"},{"location":"tutorials/tictoc/part3/#35-using-ned-inheritance","text":"If we take a closer look at the NED file we will realize that tic and toc differs only in their parameter values and their display string. We can create a new simple module type by inheriting from an other one and specifying or overriding some of its parameters. In our case we will derive two simple module types ( Tic and Toc ). Later we can use these types when defining the submodules in the network. Deriving from an existing simple module is easy. Here is the base module: And here is the derived module. We just simply specify the parameter values and add some display properties. The Toc module looks similar, but with different parameter values. Note The C++ implementation is inherited from the base simple module ( Txc5 ). Once we created the new simple modules, we can use them as submodule types in our network: As you can see, the network definition is much shorter and simpler now. Inheritance allows you to use common types in your network and avoid redundant definitions and parameter settings.","title":"3.5 Using NED inheritance"},{"location":"tutorials/tictoc/part3/#36-modeling-processing-delay","text":"In the previous models, tic and toc immediately sent back the received message. Here we'll add some timing: tic and toc will hold the message for 1 simulated second before sending it back. In OMNeT++ such timing is achieved by the module sending a message to itself. Such messages are called self-messages (but only because of the way they are used, otherwise they are ordinary message objects). We added two cMessage * variables, event and tictocMsg to the class, to remember the message we use for timing and message whose processing delay we are simulating. We \"send\" the self-messages with the scheduleAt() function, specifying when it should be delivered back to the module. In handleMessage() now we have to differentiate whether a new message has arrived via the input gate or the self-message came back (timer expired). Here we are using but we could have written if (msg->isSelfMessage()) as well. We have left out the counter, to keep the source code small. While running the simulation you will see the following log output: Sources: , ,","title":"3.6 Modeling processing delay"},{"location":"tutorials/tictoc/part3/#37-random-numbers-and-parameters","text":"In this step we'll introduce random numbers. We change the delay from 1s to a random value which can be set from the NED file or from omnetpp.ini. Module parameters are able to return random variables; however, to make use of this feature we have to read the parameter in handleMessage() every time we use it. In addition, we'll \"lose\" (delete) the packet with a small (hardcoded) probability. We'll assign the parameters in omnetpp.ini: You can try that no matter how many times you re-run the simulation (or restart it, Simulate -> Rebuild network menu item), you'll get exactly the same results. This is because OMNeT++ uses a deterministic algorithm (by default the Mersenne Twister RNG) to generate random numbers, and initializes it to the same seed. This is important for reproducible simulations. You can experiment with different seeds if you add the following lines to omnetpp.ini: [General] seed-0-mt = 532569 # or any other 32-bit value From the syntax you have probably guessed that OMNeT++ supports more than one RNGs. That's right, however, all models in this tutorial use RNG 0. Exercise Try other distributions as well. Sources: , ,","title":"3.7 Random numbers and parameters"},{"location":"tutorials/tictoc/part3/#38-timeout-cancelling-timers","text":"In order to get one step closer to modelling networking protocols, let us transform our model into a stop-and-wait simulation. This time we'll have separate classes for tic and toc . The basic scenario is similar to the previous ones: tic and toc will be tossing a message to one another. However, toc will \"lose\" the message with some nonzero probability, and in that case tic will have to resend it. Here's toc 's code: Thanks to the bubble() call in the code, toc will display a callout whenever it drops the message. So, tic will start a timer whenever it sends the message. When the timer expires, we'll assume the message was lost and send another one. If toc 's reply arrives, the timer has to be cancelled. The timer will be (what else?) a self-message. Cancelling the timer will be done with the cancelEvent() call. Note that this does not prevent us from being able to reuse the same timeout message over and over. You can read Tic's full source in Sources: , ,","title":"3.8 Timeout, cancelling timers"},{"location":"tutorials/tictoc/part3/#39-retransmitting-the-same-message","text":"In this step we refine the previous model. There we just created another packet if we needed to retransmit. This is OK because the packet didn't contain much, but in real life it's usually more practical to keep a copy of the original packet so that we can re-send it without the need to build it again. Keeping a pointer to the sent message - so we can send it again - might seem easier, but when the message is destroyed at the other node the pointer becomes invalid. What we do here is keep the original packet and send only copies of it. We delete the original when toc 's acknowledgement arrives. To make it easier to visually verify the model, we'll include a message sequence number in the message names. In order to avoid handleMessage() growing too large, we'll put the corresponding code into two new functions, generateNewMessage() and sendCopyOf() and call them from handleMessage() . The functions: Sources: , ,","title":"3.9 Retransmitting the same message"},{"location":"tutorials/tictoc/part4/","text":"Part 4 - Turning it Into a Real Network \u00b6 4.1 More than two nodes \u00b6 Now we'll make a big step: create several tic modules and connect them into a network. For now, we'll keep it simple what they do: one of the nodes generates a message, and the others keep tossing it around in random directions until it arrives at a predetermined destination node. The NED file will need a few changes. First of all, the Txc module will need to have multiple input and output gates: The [ ] turns the gates into gate vectors. The size of the vector (the number of gates) will be determined where we use Txc to build the network. Here we created 6 modules as a module vector, and connected them. The resulting topology looks like this: In this version, tic[0] will generate the message to be sent around. This is done in initialize() , with the help of the getIndex() function which returns the index of the module in the vector. The meat of the code is the forwardMessage() function which we invoke from handleMessage() whenever a message arrives at the node. It draws a random gate number, and sends out message on that gate. When the message arrives at tic[3] , its handleMessage() will delete the message. See the full code in Exercise You'll notice that this simple \"routing\" is not very efficient: often the packet keeps bouncing between two nodes for a while before it is sent to a different direction. This can be improved somewhat if nodes don't send the packet back to the sender. Implement this. Hints: cMessage::getArrivalGate() , cGate::getIndex() . Note that if the message didn't arrive via a gate but was a self-message, then getArrivalGate() returns NULL . Sources: , , 4.2 Channels and inner type definitions \u00b6 Our new network definition is getting quite complex and long, especially the connections section. Let's try to simplify it. The first thing we notice is that the connections always use the same delay parameter. It is possible to create types for the connections (they are called channels) similarly to simple modules. We should create a channel type which specifies the delay parameter and we will use that type for all connections in the network. As you have noticed we have defined the new channel type inside the network definition by adding a types section. This type definition is only visible inside the network. It is called as a local or inner type. You can use simple modules as inner types too, if you wish. Note We have created the channel by specializing the built-in DelayChannel . (built-in channels can be found inside the ned package. Thats why we used the full type name ned.DelayChannel ) after the extends keyword. Now let's check how the connections section changed. As you see we just specify the channel name inside the connection definition. This allows to easily change the delay parameter for the whole network. Sources: , , 4.3 Using two-way connections \u00b6 If we check the connections section a little more, we will realize that each node pair is connected with two connections. One for each direction. OMNeT++ 4 supports two way connections, so let's use them. First of all, we have to define two-way (or so called inout ) gates instead of the separate input and output gates we used previously. The new connections section would look like this: We have modified the gate names so we have to make some modifications to the C++ code. Note The special $i and $o suffix after the gate name allows us to use the connection's two direction separately. Sources: , , 4.4 Defining our message class \u00b6 In this step the destination address is no longer hardcoded tic[3] -- we draw a random destination, and we'll add the destination address to the message. The best way is to subclass cMessage and add destination as a data member. Hand-coding the message class is usually tedious because it contains a lot of boilerplate code, so we let OMNeT++ generate the class for us. The message class specification is in tictoc13.msg : Note See Section 6 of the OMNeT++ manual for more details on messages. The makefile is set up so that the message compiler, opp_msgc is invoked and it generates tictoc13_m.h and tictoc13_m.cc from the message declaration (The file names are generated from the tictoc13.msg file name, not the message type name). They will contain a generated TicTocMsg13 class subclassed from [ cMessage ]; the class will have getter and setter methods for every field. We'll include tictoc13_m.h into our C++ code, and we can use TicTocMsg13 as any other class. For example, we use the following lines in generateMessage() to create the message and fill its fields. Then, handleMessage() begins like this: In the argument to handleMessage(), we get the message as a cMessage* pointer. However, we can only access its fields defined in TicTocMsg13 if we cast msg to TicTocMsg13* . Plain C-style cast ( (TicTocMsg13 *)msg ) is not safe because if the message is not a TicTocMsg13 after all the program will just crash, causing an error which is difficult to explore. C++ offers a solution which is called dynamic_cast . Here we use check_and_cast<>() which is provided by OMNeT++: it tries to cast the pointer via dynamic_cast , and if it fails it stops the simulation with an error message, similar to the following: In the next line, we check if the destination address is the same as the node's address. The getIndex() member function returns the index of the module in the submodule vector (remember, in the NED file we declarared it as tic[6]: Txc13 , so our nodes have addresses 0..5). To make the model execute longer, after a message arrives to its destination the destination node will generate another message with a random destination address, and so forth. Read the full code: When you run the model, it'll look like this: You can click on the messages to see their content in the inspector window. Double-clicking will open the inspector in a new window. (You'll either have to temporarily stop the simulation for that, or to be very fast in handling the mouse). The inspector window displays lots of useful information; the message fields can be seen on the Contents page. Sources: , , , Exercise In this model, there is only one message underway at any given moment: nodes only generate a message when another message arrives at them. We did it this way to make it easier to follow the simulation. Change the module class so that instead, it generates messages periodically. The interval between messages should be a module parameter, returning exponentially distributed random numbers.","title":"Turning it Into a Real Network"},{"location":"tutorials/tictoc/part4/#part-4-turning-it-into-a-real-network","text":"","title":"Part 4 - Turning it Into a Real Network"},{"location":"tutorials/tictoc/part4/#41-more-than-two-nodes","text":"Now we'll make a big step: create several tic modules and connect them into a network. For now, we'll keep it simple what they do: one of the nodes generates a message, and the others keep tossing it around in random directions until it arrives at a predetermined destination node. The NED file will need a few changes. First of all, the Txc module will need to have multiple input and output gates: The [ ] turns the gates into gate vectors. The size of the vector (the number of gates) will be determined where we use Txc to build the network. Here we created 6 modules as a module vector, and connected them. The resulting topology looks like this: In this version, tic[0] will generate the message to be sent around. This is done in initialize() , with the help of the getIndex() function which returns the index of the module in the vector. The meat of the code is the forwardMessage() function which we invoke from handleMessage() whenever a message arrives at the node. It draws a random gate number, and sends out message on that gate. When the message arrives at tic[3] , its handleMessage() will delete the message. See the full code in Exercise You'll notice that this simple \"routing\" is not very efficient: often the packet keeps bouncing between two nodes for a while before it is sent to a different direction. This can be improved somewhat if nodes don't send the packet back to the sender. Implement this. Hints: cMessage::getArrivalGate() , cGate::getIndex() . Note that if the message didn't arrive via a gate but was a self-message, then getArrivalGate() returns NULL . Sources: , ,","title":"4.1 More than two nodes"},{"location":"tutorials/tictoc/part4/#42-channels-and-inner-type-definitions","text":"Our new network definition is getting quite complex and long, especially the connections section. Let's try to simplify it. The first thing we notice is that the connections always use the same delay parameter. It is possible to create types for the connections (they are called channels) similarly to simple modules. We should create a channel type which specifies the delay parameter and we will use that type for all connections in the network. As you have noticed we have defined the new channel type inside the network definition by adding a types section. This type definition is only visible inside the network. It is called as a local or inner type. You can use simple modules as inner types too, if you wish. Note We have created the channel by specializing the built-in DelayChannel . (built-in channels can be found inside the ned package. Thats why we used the full type name ned.DelayChannel ) after the extends keyword. Now let's check how the connections section changed. As you see we just specify the channel name inside the connection definition. This allows to easily change the delay parameter for the whole network. Sources: , ,","title":"4.2 Channels and inner type definitions"},{"location":"tutorials/tictoc/part4/#43-using-two-way-connections","text":"If we check the connections section a little more, we will realize that each node pair is connected with two connections. One for each direction. OMNeT++ 4 supports two way connections, so let's use them. First of all, we have to define two-way (or so called inout ) gates instead of the separate input and output gates we used previously. The new connections section would look like this: We have modified the gate names so we have to make some modifications to the C++ code. Note The special $i and $o suffix after the gate name allows us to use the connection's two direction separately. Sources: , ,","title":"4.3 Using two-way connections"},{"location":"tutorials/tictoc/part4/#44-defining-our-message-class","text":"In this step the destination address is no longer hardcoded tic[3] -- we draw a random destination, and we'll add the destination address to the message. The best way is to subclass cMessage and add destination as a data member. Hand-coding the message class is usually tedious because it contains a lot of boilerplate code, so we let OMNeT++ generate the class for us. The message class specification is in tictoc13.msg : Note See Section 6 of the OMNeT++ manual for more details on messages. The makefile is set up so that the message compiler, opp_msgc is invoked and it generates tictoc13_m.h and tictoc13_m.cc from the message declaration (The file names are generated from the tictoc13.msg file name, not the message type name). They will contain a generated TicTocMsg13 class subclassed from [ cMessage ]; the class will have getter and setter methods for every field. We'll include tictoc13_m.h into our C++ code, and we can use TicTocMsg13 as any other class. For example, we use the following lines in generateMessage() to create the message and fill its fields. Then, handleMessage() begins like this: In the argument to handleMessage(), we get the message as a cMessage* pointer. However, we can only access its fields defined in TicTocMsg13 if we cast msg to TicTocMsg13* . Plain C-style cast ( (TicTocMsg13 *)msg ) is not safe because if the message is not a TicTocMsg13 after all the program will just crash, causing an error which is difficult to explore. C++ offers a solution which is called dynamic_cast . Here we use check_and_cast<>() which is provided by OMNeT++: it tries to cast the pointer via dynamic_cast , and if it fails it stops the simulation with an error message, similar to the following: In the next line, we check if the destination address is the same as the node's address. The getIndex() member function returns the index of the module in the submodule vector (remember, in the NED file we declarared it as tic[6]: Txc13 , so our nodes have addresses 0..5). To make the model execute longer, after a message arrives to its destination the destination node will generate another message with a random destination address, and so forth. Read the full code: When you run the model, it'll look like this: You can click on the messages to see their content in the inspector window. Double-clicking will open the inspector in a new window. (You'll either have to temporarily stop the simulation for that, or to be very fast in handling the mouse). The inspector window displays lots of useful information; the message fields can be seen on the Contents page. Sources: , , , Exercise In this model, there is only one message underway at any given moment: nodes only generate a message when another message arrives at them. We did it this way to make it easier to follow the simulation. Change the module class so that instead, it generates messages periodically. The interval between messages should be a module parameter, returning exponentially distributed random numbers.","title":"4.4 Defining our message class"},{"location":"tutorials/tictoc/part5/","text":"Part 5 - Adding Statistics Collection \u00b6 5.1 Displaying the number of packets sent/received \u00b6 To get an overview at runtime how many messages each node sent or received, we've added two counters to the module class: numSent and numReceived. They are set to zero and WATCH 'ed in the initialize() method. Now we can use the Find/inspect objects dialog ( Inspect menu; it is also on the toolbar) to learn how many packets were sent or received by the various nodes. It's true that in this concrete simulation model the numbers will be roughly the same, so you can only learn from them that intuniform() works properly. But in real-life simulations it can be very useful that you can quickly get an overview about the state of various nodes in the model. It can be also arranged that this info appears above the module icons. The t= display string tag specifies the text; we only need to modify the displays string during runtime. The following code does the job: And the result looks like this: Sources: , , , 5.2 Adding statistics collection \u00b6 The previous simulation model does something interesting enough so that we can collect some statistics. For example, you may be interested in the average hop count a message has to travel before reaching its destination. We'll record in the hop count of every message upon arrival into an output vector (a sequence of (time,value) pairs, sort of a time series). We also calculate mean, standard deviation, minimum, maximum values per node, and write them into a file at the end of the simulation. Then we'll use tools from the OMNeT++ IDE to analyse the output files. For that, we add an output vector object (which will record the data into Tictoc15-#0.vec ) and a histogram object (which also calculates mean, etc) to the class. When a message arrives at the destination node, we update the statistics. The following code has been added to handleMessage() : The hopCountVector.record() call writes the data into Tictoc15-#0.vec . With a large simulation model or long execution time, the Tictoc15-#0.vec file may grow very large. To handle this situation, you can specifically disable/enable vector in omnetpp.ini, and you can also specify a simulation time interval in which you're interested (data recorded outside this interval will be discarded.) When you begin a new simulation, the existing Tictoc15-#0.vec/sca files get deleted. Scalar data (collected by the histogram object in this simulation) have to be recorded manually, in the finish() function. finish() is invoked on successful completion of the simulation, i.e. not when it's stopped with an error. The recordScalar() calls in the code below write into the Tictoc15-#0.sca file. The files are stored in the results/ subdirectory. You can also view the data during simulation. To do that, right click on a module, and choose Open Details . In the module inspector's Contents page you'll find the hopCountStats and hopCountVector objects. To open their inspectors, right click on cHistogram hopCountStats or cOutVector HopCount , and click Open Graphical View . The inspector: They will be initially empty -- run the simulation in Fast (or even Express ) mode to get enough data to be displayed. After a while you'll get something like this: When you think enough data has been collected, you can stop the simulation and then we'll analyse the result files ( Tictoc15-#0.vec and Tictoc15-#0.sca ) off-line. You'll need to choose Simulate -> Call finish() from the menu (or click the corresponding toolbar button) before exiting -- this will cause the finish() functions to run and data to be written into Tictoc15-#0.sca . Sources: , , , 5.3 Statistic collection without modifying your model \u00b6 In the previous step we have added statistic collection to our model. While we can compute and save any value we wish, usually it is not known at the time of writing the model, what data the enduser will need. OMNeT++ provides an additional mechanism to record values and events. Any model can emit signals that can carry a value or an object. The model writer just have to decide what signals to emit, what data to attach to them and when to emit them. The enduser can attach 'listeners' to these signals that can process or record these data items. This way the model code does not have to contain any code that is specific to the statistics collection and the enduser can freely add additional statistics without even looking into the C++ code. We will re-write the statistic collection introduced in the last step to use signals. First of all, we can safely remove all statistic related variables from our module. There is no need for the cOutVector and cHistogram classes either. We will need only a single signal that carries the hopCount of the message at the time of message arrival at the destination. First we need to define our signal. The arrivalSignal is just an identifier that can be used later to easily refer to our signal. We must register all signals before using them. The best place to do this is the initialize() method of the module. Now we can emit our signal, when the message has arrived to the destination node. As we do not have to save or store anything manually, the finish() method can be deleted. We no longer need it. The last step is that we have to define the emitted signal also in the NED file. Declaring signals in the NED file allows you to have all information about your module in one place. You will see the parameters it takes, its input and output gates, and also the signals and statistics it provides. Now we can define also a statistic that should be collected by default. Our previous example has collected statistics (max, min, mean, count, etc.) about the hop count of the arriving messages, so let's collect the same data here, too. The source key specifies the signal we want our statistic to attach to. The record key can be used to tell what should be done with the received data. In our case we specify that each value must be saved in a vector file (vector) and also we need to calculate min,max,mean,count etc. (stats). (NOTE: stats is just a shorthand for min, max, mean, sum, count, etc.) With this step we have finished our model. Now we have just realized that we would like to see a histogram of the hopCount on the tic[1] module. On the other hand we are short on disk storage and we are not interested having the vector data for the first three module tic 0,1,2. No problem. We can add our histogram and remove the unneeded vector recording without even touching the C++ or NED files. Just open the INI file and modify the statistic recording: We can configure a wide range of statistics without even looking into the C++ code, provided that the original model emits the necessary signals for us. Sources: , , , 5.4 Adding figures \u00b6 OMNeT++ can display figures on the canvas, such as text, geometric shapes or images. These figures can be static, or change dynamically according to what happens in the simulation. In this case, we will display a static descriptive text, and a dynamic text showing the hop count of the last message that arrived at its destination. We create figures in , with the @figure property. This creates two text figures named description and lasthopcount , and sets their positions on the canvas (we place them in the top right corner). The font attribute sets the figure text's font. It has three parameters: typeface, size, style . Any one of them can be omitted to leave the parameter at default. Here we set the description figure's font to bold. By default the text in lasthopcount is static, but we'll change it when a message arrives. This is done in , in the handleMessage() function. The figure is represented by the cTextFigure C++ class. There are several figure types, all of them are subclassed from the cFigure base class. We insert the code responsible for updating the figure text after we retreive the hopcount variable. We want to draw the figures on the network's canvas. The getParentModule() function returns the parent of the node, ie. the network. Then the getCanvas() function returns the network's canvas, and getFigure() gets the figure by name. Then, we update the figure's text with the setText() function. Tip For more information on figures and the canvas, see The Canvas section of the OMNeT++ manual When you run the simulation, the figure displays 'last hopCount: N/A' before the arrival of the first message. Then, it is updated whenever a message arrives at its destination. Tip If the figure text and nodes overlap, press 're-layout'. In the last few steps, we have collected and displayed statistics. In the next part, we'll see and analyze them in the IDE. Sources: , , ,","title":"Adding Statistics Collection"},{"location":"tutorials/tictoc/part5/#part-5-adding-statistics-collection","text":"","title":"Part 5 - Adding Statistics Collection"},{"location":"tutorials/tictoc/part5/#51-displaying-the-number-of-packets-sentreceived","text":"To get an overview at runtime how many messages each node sent or received, we've added two counters to the module class: numSent and numReceived. They are set to zero and WATCH 'ed in the initialize() method. Now we can use the Find/inspect objects dialog ( Inspect menu; it is also on the toolbar) to learn how many packets were sent or received by the various nodes. It's true that in this concrete simulation model the numbers will be roughly the same, so you can only learn from them that intuniform() works properly. But in real-life simulations it can be very useful that you can quickly get an overview about the state of various nodes in the model. It can be also arranged that this info appears above the module icons. The t= display string tag specifies the text; we only need to modify the displays string during runtime. The following code does the job: And the result looks like this: Sources: , , ,","title":"5.1 Displaying the number of packets sent/received"},{"location":"tutorials/tictoc/part5/#52-adding-statistics-collection","text":"The previous simulation model does something interesting enough so that we can collect some statistics. For example, you may be interested in the average hop count a message has to travel before reaching its destination. We'll record in the hop count of every message upon arrival into an output vector (a sequence of (time,value) pairs, sort of a time series). We also calculate mean, standard deviation, minimum, maximum values per node, and write them into a file at the end of the simulation. Then we'll use tools from the OMNeT++ IDE to analyse the output files. For that, we add an output vector object (which will record the data into Tictoc15-#0.vec ) and a histogram object (which also calculates mean, etc) to the class. When a message arrives at the destination node, we update the statistics. The following code has been added to handleMessage() : The hopCountVector.record() call writes the data into Tictoc15-#0.vec . With a large simulation model or long execution time, the Tictoc15-#0.vec file may grow very large. To handle this situation, you can specifically disable/enable vector in omnetpp.ini, and you can also specify a simulation time interval in which you're interested (data recorded outside this interval will be discarded.) When you begin a new simulation, the existing Tictoc15-#0.vec/sca files get deleted. Scalar data (collected by the histogram object in this simulation) have to be recorded manually, in the finish() function. finish() is invoked on successful completion of the simulation, i.e. not when it's stopped with an error. The recordScalar() calls in the code below write into the Tictoc15-#0.sca file. The files are stored in the results/ subdirectory. You can also view the data during simulation. To do that, right click on a module, and choose Open Details . In the module inspector's Contents page you'll find the hopCountStats and hopCountVector objects. To open their inspectors, right click on cHistogram hopCountStats or cOutVector HopCount , and click Open Graphical View . The inspector: They will be initially empty -- run the simulation in Fast (or even Express ) mode to get enough data to be displayed. After a while you'll get something like this: When you think enough data has been collected, you can stop the simulation and then we'll analyse the result files ( Tictoc15-#0.vec and Tictoc15-#0.sca ) off-line. You'll need to choose Simulate -> Call finish() from the menu (or click the corresponding toolbar button) before exiting -- this will cause the finish() functions to run and data to be written into Tictoc15-#0.sca . Sources: , , ,","title":"5.2 Adding statistics collection"},{"location":"tutorials/tictoc/part5/#53-statistic-collection-without-modifying-your-model","text":"In the previous step we have added statistic collection to our model. While we can compute and save any value we wish, usually it is not known at the time of writing the model, what data the enduser will need. OMNeT++ provides an additional mechanism to record values and events. Any model can emit signals that can carry a value or an object. The model writer just have to decide what signals to emit, what data to attach to them and when to emit them. The enduser can attach 'listeners' to these signals that can process or record these data items. This way the model code does not have to contain any code that is specific to the statistics collection and the enduser can freely add additional statistics without even looking into the C++ code. We will re-write the statistic collection introduced in the last step to use signals. First of all, we can safely remove all statistic related variables from our module. There is no need for the cOutVector and cHistogram classes either. We will need only a single signal that carries the hopCount of the message at the time of message arrival at the destination. First we need to define our signal. The arrivalSignal is just an identifier that can be used later to easily refer to our signal. We must register all signals before using them. The best place to do this is the initialize() method of the module. Now we can emit our signal, when the message has arrived to the destination node. As we do not have to save or store anything manually, the finish() method can be deleted. We no longer need it. The last step is that we have to define the emitted signal also in the NED file. Declaring signals in the NED file allows you to have all information about your module in one place. You will see the parameters it takes, its input and output gates, and also the signals and statistics it provides. Now we can define also a statistic that should be collected by default. Our previous example has collected statistics (max, min, mean, count, etc.) about the hop count of the arriving messages, so let's collect the same data here, too. The source key specifies the signal we want our statistic to attach to. The record key can be used to tell what should be done with the received data. In our case we specify that each value must be saved in a vector file (vector) and also we need to calculate min,max,mean,count etc. (stats). (NOTE: stats is just a shorthand for min, max, mean, sum, count, etc.) With this step we have finished our model. Now we have just realized that we would like to see a histogram of the hopCount on the tic[1] module. On the other hand we are short on disk storage and we are not interested having the vector data for the first three module tic 0,1,2. No problem. We can add our histogram and remove the unneeded vector recording without even touching the C++ or NED files. Just open the INI file and modify the statistic recording: We can configure a wide range of statistics without even looking into the C++ code, provided that the original model emits the necessary signals for us. Sources: , , ,","title":"5.3 Statistic collection without modifying your model"},{"location":"tutorials/tictoc/part5/#54-adding-figures","text":"OMNeT++ can display figures on the canvas, such as text, geometric shapes or images. These figures can be static, or change dynamically according to what happens in the simulation. In this case, we will display a static descriptive text, and a dynamic text showing the hop count of the last message that arrived at its destination. We create figures in , with the @figure property. This creates two text figures named description and lasthopcount , and sets their positions on the canvas (we place them in the top right corner). The font attribute sets the figure text's font. It has three parameters: typeface, size, style . Any one of them can be omitted to leave the parameter at default. Here we set the description figure's font to bold. By default the text in lasthopcount is static, but we'll change it when a message arrives. This is done in , in the handleMessage() function. The figure is represented by the cTextFigure C++ class. There are several figure types, all of them are subclassed from the cFigure base class. We insert the code responsible for updating the figure text after we retreive the hopcount variable. We want to draw the figures on the network's canvas. The getParentModule() function returns the parent of the node, ie. the network. Then the getCanvas() function returns the network's canvas, and getFigure() gets the figure by name. Then, we update the figure's text with the setText() function. Tip For more information on figures and the canvas, see The Canvas section of the OMNeT++ manual When you run the simulation, the figure displays 'last hopCount: N/A' before the arrival of the first message. Then, it is updated whenever a message arrives at its destination. Tip If the figure text and nodes overlap, press 're-layout'. In the last few steps, we have collected and displayed statistics. In the next part, we'll see and analyze them in the IDE. Sources: , , ,","title":"5.4 Adding figures"},{"location":"tutorials/tictoc/part6/","text":"Part 6 - Visualizing the Results With the IDE \u00b6 6.1 Visualizing output scalars and vectors \u00b6 The OMNeT++ IDE can help you analyze your results. It supports filtering, processing, and displaying scalar, vector, and histogram data. The following diagrams have been created with the Result Analysis tool of the IDE. The results directory in the project folder contains .vec and .sca files, which are the files that store the results in vector and scalar form, respectively. Vectors record data values as a function of time, while scalars typically record aggregate values at the end of the simulation. To open the Result Analysis tool, double click on either the .vec or the .sca files in the OMNeT++ IDE. Both files will be loaded by the Result Analysis tool. You can find the Browse Data tab at the bottom of the Result Analysis tool panel. Here you can browse results by type by switching the various tabs at the top of the tool panel, ie. Scalars, Vectors, Statistics, or Histograms. By default, all results of a result type are displayed. You can filter them by the module filter to view all or some of the individual modules, or the statistic name filter to display different types of statistics, ie. mean, max, min, standard deviation, etc. You can select some or all of the individual results by highlighting them. If you select multiple results, they will be plotted on one chart. Right click and select one of the Plot using ... menu items to display the figures. Tip For further information about the charting and processing capabilities, please refer to the OMNeT++ Users Guide (you can find it in the doc directory of the OMNeT++ installation). Our last model records the hopCount of a message each time the message reaches its destination. To plot these vectors for all nodes, select the 6 lines in the browse data tab, then right click and select Plot using Line chart . We can change various options about how the data on the chart is displayed. Right click on the chart background, and select Configure Chart . In the appearing dialog, on the Lines tab, set Line style to none , and Marker to . (dot) . The chart looks like the following: If we apply a mean operation we can see how the hopCount in the different nodes converge to an average. Right-click the chart, and select Apply... -> Mean . Again, right-click on the chart background, and select Configure Chart . In the Lines tab, set Line style to solid , and Marker to none . The mean is displayed on the following chart. The lines are easier to see this way because they are thinner. Scalar data can be plotted on bar charts. The next chart displays the mean and the maximum of the hopCount of the messages for each destination node, based on the scalar data recorded at the end of the simulation. In the Browse data tab, select Scalars , and make sure that the Show Statistics/Vector Fields as Scalars button is enabled on the top-right toolbar. Now select hop count:max and hop count:mean for all 6 nodes. You can use filtering and sorting to find the scalars easier, as shown below. This is how the chart looks like: To create a histogram that shows hopCount 's distribution, select Histograms on the Browse Data tab. Select all rows, then right click, and choose Plot using Histogram Chart .","title":"Visualizing the Results"},{"location":"tutorials/tictoc/part6/#part-6-visualizing-the-results-with-the-ide","text":"","title":"Part 6 - Visualizing the Results With the IDE"},{"location":"tutorials/tictoc/part6/#61-visualizing-output-scalars-and-vectors","text":"The OMNeT++ IDE can help you analyze your results. It supports filtering, processing, and displaying scalar, vector, and histogram data. The following diagrams have been created with the Result Analysis tool of the IDE. The results directory in the project folder contains .vec and .sca files, which are the files that store the results in vector and scalar form, respectively. Vectors record data values as a function of time, while scalars typically record aggregate values at the end of the simulation. To open the Result Analysis tool, double click on either the .vec or the .sca files in the OMNeT++ IDE. Both files will be loaded by the Result Analysis tool. You can find the Browse Data tab at the bottom of the Result Analysis tool panel. Here you can browse results by type by switching the various tabs at the top of the tool panel, ie. Scalars, Vectors, Statistics, or Histograms. By default, all results of a result type are displayed. You can filter them by the module filter to view all or some of the individual modules, or the statistic name filter to display different types of statistics, ie. mean, max, min, standard deviation, etc. You can select some or all of the individual results by highlighting them. If you select multiple results, they will be plotted on one chart. Right click and select one of the Plot using ... menu items to display the figures. Tip For further information about the charting and processing capabilities, please refer to the OMNeT++ Users Guide (you can find it in the doc directory of the OMNeT++ installation). Our last model records the hopCount of a message each time the message reaches its destination. To plot these vectors for all nodes, select the 6 lines in the browse data tab, then right click and select Plot using Line chart . We can change various options about how the data on the chart is displayed. Right click on the chart background, and select Configure Chart . In the appearing dialog, on the Lines tab, set Line style to none , and Marker to . (dot) . The chart looks like the following: If we apply a mean operation we can see how the hopCount in the different nodes converge to an average. Right-click the chart, and select Apply... -> Mean . Again, right-click on the chart background, and select Configure Chart . In the Lines tab, set Line style to solid , and Marker to none . The mean is displayed on the following chart. The lines are easier to see this way because they are thinner. Scalar data can be plotted on bar charts. The next chart displays the mean and the maximum of the hopCount of the messages for each destination node, based on the scalar data recorded at the end of the simulation. In the Browse data tab, select Scalars , and make sure that the Show Statistics/Vector Fields as Scalars button is enabled on the top-right toolbar. Now select hop count:max and hop count:mean for all 6 nodes. You can use filtering and sorting to find the scalars easier, as shown below. This is how the chart looks like: To create a histogram that shows hopCount 's distribution, select Histograms on the Browse Data tab. Select all rows, then right click, and choose Plot using Histogram Chart .","title":"6.1 Visualizing output scalars and vectors"},{"location":"tutorials/tictoc/part7/","text":"Part 7 - Parameter Studies \u00b6 7.1 The goal \u00b6 We want to run the simulation with a different number of nodes, and see how the behavior of the network changes. With OMNeT++ you can do parameter studies, which are multiple simulation runs with different parameter values. We'll make the number of central nodes (the \"handle\" in the dumbbell shape) a parameter, and use the same random routing protocol as before. We're interested in how the average hop count depends on the number of nodes. 7.2 Making the network topology parametric \u00b6 To parameterize the network, the number of nodes is given as a NED parameter, numCentralNodes . This parameter specifies how many nodes are in the central section of the network, but doesn't cover the two nodes at each side. The total number of nodes including the four nodes on the sides is numCentralNodes+4 . The default of the numCentralNodes parameter is 2, this corresponds to the network in the previous step. Now, we must specify that the variable number of nodes should be connected into the dumbbell shape. First, the two nodes on one side is connected to the third one. Then the the last two nodes on the other side is connected to the third last. The nodes in the center of the dumbbell can be connected with a for loop. Starting from the third, each *i*th node is connected to the *i+1*th. Here is how the network looks like with numCentralNodes = 4 : To run the simulation with multiple different values of numCentralNodes , we specify the variable N in the ini file: 7.3 Setting up a parameter study \u00b6 We specify that N should go from 2 to 100, in steps of 2. This produces about 50 simulation runs. Each can be explored in the graphical user interface, but simulation batches are often run from the command line interface using the Cmdenv runtime environment. Tip You can find more information on variables and parameter studies in the Parameter Studies section of the OMNeT++ manual. To increase the accuracy of the simulation we may need to run the same simulation several times using different random numbers. These runs are called Repetitions and are specified in omnetpp.ini : This means that each simulation run will be executed four times, each time with a different seed for the RNGs. This produces more samples, which can be averaged. With more repetitions, the results will increasingly converge to the expected values. 7.4 Running the parameter study \u00b6 Now, we can run the simulations. In the dropdown menu of the Run icon, select Run Configurations . In the Run Configurations dialog, select the config name, make sure Cmdenv is selected as the user interface. If you have a multicore CPU, you can specify how many simulations to run concurrently. Note Alternatively, you can run the simulation batches from the command line with opp_runall tool with the following command: opp_runall -j4 ./tictoc -u Cmdenv -c TicToc18 The -j parameter specifies the number of CPU cores, the -u parameter the user interface, and -c the config to run. 7.5 Analyzing the results \u00b6 Now, we can visualize and analyze the data we've collected from the simulation runs. We'll display the average hop count for messages that reach their destinations vs N , the number of central nodes. Additionally, we will display the average number of packets that reached their destinations vs N . The analysis file Tictoc18.anf contains the dataset we will use for the visualization. These two average scalars are not recorded during the simulation, we will have to compute them from the available data. The hop count is recorded at each node when a message arrives, so the mean of hop count will be available as a statistic. But this is recorded per node, and we're interested in the average of the mean hop count for all nodes. The Scatter Chart can average multiple values before plotting them. Tip Refer to the chapter \"Analysing the Results\" in the User Guide for more information on charts. You can find it in the doc/ directory of your OMNeT++ installation. To make the chart average the hop count over all nodes instead of plotting them as separate lines, set the axes in the configuration dialog as shown: Here is the average hop count vs N : The average hop count increases as the network gets larger, as packets travel more to reach their destination. The increase is polynomial. We're also interested in the average number of packets that arrive at their destination. The count of the arrived packets is available at each node. Below is the average number of packets that arrived vs N : Notice that the Y axis is logarithmic. The average number of packets that arrive decreases rapidly as N increases, and the network gets larger.","title":"Parameter Studies"},{"location":"tutorials/tictoc/part7/#part-7-parameter-studies","text":"","title":"Part 7 - Parameter Studies"},{"location":"tutorials/tictoc/part7/#71-the-goal","text":"We want to run the simulation with a different number of nodes, and see how the behavior of the network changes. With OMNeT++ you can do parameter studies, which are multiple simulation runs with different parameter values. We'll make the number of central nodes (the \"handle\" in the dumbbell shape) a parameter, and use the same random routing protocol as before. We're interested in how the average hop count depends on the number of nodes.","title":"7.1 The goal"},{"location":"tutorials/tictoc/part7/#72-making-the-network-topology-parametric","text":"To parameterize the network, the number of nodes is given as a NED parameter, numCentralNodes . This parameter specifies how many nodes are in the central section of the network, but doesn't cover the two nodes at each side. The total number of nodes including the four nodes on the sides is numCentralNodes+4 . The default of the numCentralNodes parameter is 2, this corresponds to the network in the previous step. Now, we must specify that the variable number of nodes should be connected into the dumbbell shape. First, the two nodes on one side is connected to the third one. Then the the last two nodes on the other side is connected to the third last. The nodes in the center of the dumbbell can be connected with a for loop. Starting from the third, each *i*th node is connected to the *i+1*th. Here is how the network looks like with numCentralNodes = 4 : To run the simulation with multiple different values of numCentralNodes , we specify the variable N in the ini file:","title":"7.2 Making the network topology parametric"},{"location":"tutorials/tictoc/part7/#73-setting-up-a-parameter-study","text":"We specify that N should go from 2 to 100, in steps of 2. This produces about 50 simulation runs. Each can be explored in the graphical user interface, but simulation batches are often run from the command line interface using the Cmdenv runtime environment. Tip You can find more information on variables and parameter studies in the Parameter Studies section of the OMNeT++ manual. To increase the accuracy of the simulation we may need to run the same simulation several times using different random numbers. These runs are called Repetitions and are specified in omnetpp.ini : This means that each simulation run will be executed four times, each time with a different seed for the RNGs. This produces more samples, which can be averaged. With more repetitions, the results will increasingly converge to the expected values.","title":"7.3 Setting up a parameter study"},{"location":"tutorials/tictoc/part7/#74-running-the-parameter-study","text":"Now, we can run the simulations. In the dropdown menu of the Run icon, select Run Configurations . In the Run Configurations dialog, select the config name, make sure Cmdenv is selected as the user interface. If you have a multicore CPU, you can specify how many simulations to run concurrently. Note Alternatively, you can run the simulation batches from the command line with opp_runall tool with the following command: opp_runall -j4 ./tictoc -u Cmdenv -c TicToc18 The -j parameter specifies the number of CPU cores, the -u parameter the user interface, and -c the config to run.","title":"7.4 Running the parameter study"},{"location":"tutorials/tictoc/part7/#75-analyzing-the-results","text":"Now, we can visualize and analyze the data we've collected from the simulation runs. We'll display the average hop count for messages that reach their destinations vs N , the number of central nodes. Additionally, we will display the average number of packets that reached their destinations vs N . The analysis file Tictoc18.anf contains the dataset we will use for the visualization. These two average scalars are not recorded during the simulation, we will have to compute them from the available data. The hop count is recorded at each node when a message arrives, so the mean of hop count will be available as a statistic. But this is recorded per node, and we're interested in the average of the mean hop count for all nodes. The Scatter Chart can average multiple values before plotting them. Tip Refer to the chapter \"Analysing the Results\" in the User Guide for more information on charts. You can find it in the doc/ directory of your OMNeT++ installation. To make the chart average the hop count over all nodes instead of plotting them as separate lines, set the axes in the configuration dialog as shown: Here is the average hop count vs N : The average hop count increases as the network gets larger, as packets travel more to reach their destination. The increase is polynomial. We're also interested in the average number of packets that arrive at their destination. The count of the arrived packets is available at each node. Below is the average number of packets that arrived vs N : Notice that the Y axis is logarithmic. The average number of packets that arrive decreases rapidly as N increases, and the network gets larger.","title":"7.5 Analyzing the results"}]}